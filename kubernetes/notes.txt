######################################################################################
Cluster Implementations
######################################################################################

--------------------------------------------------------------------------------------
Installing minikube
--------------------------------------------------------------------------------------

https://phoenixnap.com/kb/install-minikube-on-ubuntu

--------------------------------------------------------------------------------------
Google Cloud Platform
--------------------------------------------------------------------------------------

Google Kubernetes Engine (GKE)


--------------------------------------------------------------------------------------
Multiple Node Cluster running on 4 ubuntu server VMs
--------------------------------------------------------------------------------------

Steps for creating:  creating-cluster.txt


######################################################################################
### Basic kubectl Commands
######################################################################################

- To view information about the Kubernetes cluster:

kubectl cluster-info   

Kubernetes master is running at https://192.168.56.4:6443
KubeDNS is running at https://192.168.56.4:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

---------------------------------------------------------------------------------------

- To view all the nodes belonging to the cluster:

$ kubectl get nodes

NAME      STATUS   ROLES    AGE     VERSION
m-node1   Ready    master   4d23h   v1.19.2
w-node1   Ready    <none>   4d22h   v1.19.2
w-node2   Ready    <none>   4d19h   v1.19.2
w-node3   Ready    <none>   4d19h   v1.19.2

- If your created your own Kubernetes cluster, the following command will display
  additional details about each Node that is part of the cluster.  The additional
  information includes the IP address of each Node.  


$ kubectl get nodes -o wide

NAME      STATUS   ROLES    AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
m-node1   Ready    master   4d23h   v1.19.2   192.168.56.4   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.11
w-node1   Ready    <none>   4d22h   v1.19.2   192.168.56.5   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.11
w-node2   Ready    <none>   4d19h   v1.19.2   192.168.56.6   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.11
w-node3   Ready    <none>   4d19h   v1.19.2   192.168.56.7   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.11

- The describe command can be used to find details of a specific resource.  The following will
  display the details of a specific node:

$ kubectl describe node m-node1

[minikube-specific]

- If using Minikube, there will be only one node.  This node acts as the Master and Worker node.
- To view more details about a an object, in this case a node, kubectl describe can be used

kubectl describe node minikube

[minikube-specific]


Name:               m-node1
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=m-node1
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/master=
                    role=ingress-controller
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 192.168.56.4/24
                    projectcalico.org/IPv4VXLANTunnelAddr: 192.168.242.192
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 26 Sep 2020 14:40:19 -0400
Taints:             node-role.kubernetes.io/master:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  m-node1
  AcquireTime:     <unset>
  RenewTime:       Thu, 01 Oct 2020 14:37:34 -0400
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Thu, 01 Oct 2020 14:01:52 -0400   Thu, 01 Oct 2020 14:01:52 -0400   CalicoIsUp                   Calico is running on this node
  MemoryPressure       False   Thu, 01 Oct 2020 14:36:23 -0400   Sat, 26 Sep 2020 14:40:15 -0400   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Thu, 01 Oct 2020 14:36:23 -0400   Sat, 26 Sep 2020 14:40:15 -0400   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Thu, 01 Oct 2020 14:36:23 -0400   Sat, 26 Sep 2020 14:40:15 -0400   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Thu, 01 Oct 2020 14:36:23 -0400   Sat, 26 Sep 2020 14:50:52 -0400   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  192.168.56.4
  Hostname:    m-node1
Capacity:
  cpu:                2
  ephemeral-storage:  9219412Ki
  hugepages-2Mi:      0
  memory:             2035284Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  8496610086
  hugepages-2Mi:      0
  memory:             1932884Ki
  pods:               110
System Info:
  Machine ID:                 ca47f5d681d74a76976b9aaf7237f866
  System UUID:                8fa853aa-b426-a649-9f91-4a3a8e346f8a
  Boot ID:                    78dd597a-ad39-489f-9a64-4a3e80bb5b00
  Kernel Version:             5.4.0-48-generic
  OS Image:                   Ubuntu 20.04.1 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://19.3.11
  Kubelet Version:            v1.19.2
  Kube-Proxy Version:         v1.19.2
PodCIDR:                      192.168.0.0/24
PodCIDRs:                     192.168.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
  ---------                   ----                                       ------------  ----------  ---------------  -------------  ---
  calico-system               calico-kube-controllers-6ddfb4769-lz6mq    0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d23h
  calico-system               calico-node-tdmj2                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d23h
  calico-system               calico-typha-6794d46fcc-nqk46              0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d23h
  kube-system                 coredns-f9fd979d6-64mkt                    100m (5%)     0 (0%)      70Mi (3%)        170Mi (9%)     4d23h
  kube-system                 coredns-f9fd979d6-p9hqt                    100m (5%)     0 (0%)      70Mi (3%)        170Mi (9%)     4d23h
  kube-system                 etcd-m-node1                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d23h
  kube-system                 kube-apiserver-m-node1                     250m (12%)    0 (0%)      0 (0%)           0 (0%)         4d23h
  kube-system                 kube-controller-manager-m-node1            200m (10%)    0 (0%)      0 (0%)           0 (0%)         4d23h
  kube-system                 kube-proxy-knzw8                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d23h
  kube-system                 kube-scheduler-m-node1                     100m (5%)     0 (0%)      0 (0%)           0 (0%)         4d23h
  tigera-operator             tigera-operator-cdc4b6478-gvsr5            0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d23h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (37%)  0 (0%)
  memory             140Mi (7%)  340Mi (18%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From                 Message
  ----    ------                   ----               ----                 -------
  Normal  Starting                 36m                kubelet, m-node1     Starting kubelet.
  Normal  NodeAllocatableEnforced  36m                kubelet, m-node1     Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  36m (x8 over 36m)  kubelet, m-node1     Node m-node1 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    36m (x8 over 36m)  kubelet, m-node1     Node m-node1 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     36m (x7 over 36m)  kubelet, m-node1     Node m-node1 status is now: NodeHasSufficientPID
  Normal  Starting                 36m                kube-proxy, m-node1  Starting kube-proxy.


- The following command can be used the SSH into one of the above nodes:

$ ssh w-node1@192.168.56.5


[minikube-specific]

- If using Minikube, you can ssh into the node as follows:
    - minikube ssh

[minikube-specific]


- Each node in the cluster runs the following:
     - Docker
     - Kubelet
     - the kube-proxy

- The kubectl command line sends HTTP REST requests to the Kubernetes API server running
  on the master node.

- If you ssh into one of the nodes you can see these running processes:

    ps uax

- You will see the above processes listed.

USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         640  1.3  4.6 1394160 95560 ?       Ssl  18:00   0:15 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
root         625  2.3  4.7 1922520 97256 ?       Ssl  18:00   0:28 /usr/bin/kubelet --node-ip=192.168.56.5 --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --k
root        1719  0.0  1.8 743152 37784 ?        Ssl  18:01   0:00 /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=w-node1

-----------------------------------------------------------------------------------------------------------------

- The following can be used to view all pods running across all namespaces:

$ kubectl get pods --all-namespaces -o wide


NAMESPACE         NAME                                        READY   STATUS      RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
calico-system     calico-kube-controllers-6ddfb4769-lz6mq     1/1     Running     14         4d23h   192.168.242.235   m-node1   <none>           <none>
calico-system     calico-node-2r5dx                           1/1     Running     11         4d19h   192.168.56.7      w-node3   <none>           <none>
calico-system     calico-node-hrmbs                           1/1     Running     12         4d20h   192.168.56.6      w-node2   <none>           <none>
calico-system     calico-node-rblrf                           1/1     Running     15         4d23h   192.168.56.5      w-node1   <none>           <none>
calico-system     calico-node-tdmj2                           1/1     Running     14         4d23h   192.168.56.4      m-node1   <none>           <none>
calico-system     calico-typha-6794d46fcc-78nlc               1/1     Running     21         4d20h   192.168.56.6      w-node2   <none>           <none>
calico-system     calico-typha-6794d46fcc-946qr               1/1     Running     25         4d23h   192.168.56.5      w-node1   <none>           <none>
calico-system     calico-typha-6794d46fcc-nqk46               1/1     Running     19         4d23h   192.168.56.4      m-node1   <none>           <none>
calico-system     calico-typha-6794d46fcc-sh485               1/1     Running     18         4d19h   192.168.56.7      w-node3   <none>           <none>
ingress-nginx     ingress-nginx-admission-create-q67mx        0/1     Completed   0          30h     192.168.129.228   w-node1   <none>           <none>
ingress-nginx     ingress-nginx-admission-patch-2hhw8         0/1     Completed   0          30h     192.168.144.91    w-node2   <none>           <none>
ingress-nginx     ingress-nginx-controller-84cb46fccd-gngdf   1/1     Running     3          30h     192.168.138.32    w-node3   <none>           <none>
kube-system       coredns-f9fd979d6-64mkt                     1/1     Running     14         5d      192.168.242.237   m-node1   <none>           <none>
kube-system       coredns-f9fd979d6-p9hqt                     1/1     Running     14         5d      192.168.242.236   m-node1   <none>           <none>
kube-system       etcd-m-node1                                1/1     Running     15         5d      192.168.56.4      m-node1   <none>           <none>
kube-system       kube-apiserver-m-node1                      1/1     Running     15         5d      192.168.56.4      m-node1   <none>           <none>
kube-system       kube-controller-manager-m-node1             1/1     Running     14         5d      192.168.56.4      m-node1   <none>           <none>
kube-system       kube-proxy-dgxd4                            1/1     Running     14         4d23h   192.168.56.5      w-node1   <none>           <none>
kube-system       kube-proxy-gs7d6                            1/1     Running     11         4d19h   192.168.56.7      w-node3   <none>           <none>
kube-system       kube-proxy-knzw8                            1/1     Running     14         5d      192.168.56.4      m-node1   <none>           <none>
kube-system       kube-proxy-mkmmv                            1/1     Running     12         4d20h   192.168.56.6      w-node2   <none>           <none>
kube-system       kube-scheduler-m-node1                      1/1     Running     15         5d      192.168.56.4      m-node1   <none>           <none>
tigera-operator   tigera-operator-cdc4b6478-gvsr5             1/1     Running     28         4d23h   192.168.56.4      m-node1   <none>           <none>


- Since wide output was requested, the node on which each Pod is running is also listed.  This provides a nice overview 
  the entire custer.  Since no pods have been created yet, all the Pods listed above are those required for running
  Kubernetes cluster.

- The following command can be used to continuously monitor all the pods:

$ watch kubectl get pods --all-namespaces -o wide

- When Pods are created by default, they are placed within the Default namespace if a specific namespace is not specified.
- Above, the namespaces listed container the following Pods:

  kube-system: This namespace contains all the pods that implement the based Kubernetes cluster.
  calico-system: This namespace contains all the Pods providing the Pod Networking implementation selected when building the cluster.
  ingress-nginx: Contains all of the pods implementing the Ingress Controller selected when setting up the cluster.


- Since a cluster can has several Pods running it is common to only list those within a given namespace.  First, the following
  command can be used to list all the defined namespaces:

  $ kubectl get namespaces

  NAME              STATUS   AGE
  calico-system     Active   5d
  default           Active   5d
  ingress-nginx     Active   31h
  kube-node-lease   Active   5d
  kube-public       Active   5d
  kube-system       Active   5d
  tigera-operator   Active   5d

 - Then filter on a namespace as follows:

  $ kubectl get pods -n calico-system

  NAME                                      READY   STATUS    RESTARTS   AGE
  calico-kube-controllers-6ddfb4769-lz6mq   1/1     Running   14         5d
  calico-node-2r5dx                         1/1     Running   11         4d20h
  calico-node-hrmbs                         1/1     Running   12         4d20h
  calico-node-rblrf                         1/1     Running   15         4d23h
  calico-node-tdmj2                         1/1     Running   14         5d
  calico-typha-6794d46fcc-78nlc             1/1     Running   21         4d20h
  calico-typha-6794d46fcc-946qr             1/1     Running   25         4d23h
  calico-typha-6794d46fcc-nqk46             1/1     Running   19         5d
  calico-typha-6794d46fcc-sh485             1/1     Running   18         4d20h

-------------------------------------------------------------------------------------------------

######################################################################################
Image for Tools to Inspect Cluster from Pod Container
######################################################################################

- The following Docker file defines a Ubuntu based image to which some some common
  command lines tools.

$ cat microservices/Kube-Tools/Dockerfile 

FROM ubuntu
RUN apt-get update
RUN apt-get install curl -y
RUN apt-get install dnsutils -y
RUN apt-get install iproute2 -y
RUN apt-get install iputils-ping -y
CMD [ "sleep", "60m" ]

- Build and push the image to Docker Hub by completing the following:

$ docker build -t blgreco72dev/kube-tools .

- Then list the images:

REPOSITORY                             TAG                 IMAGE ID            CREATED              SIZE
blgreco72dev/kube-tools                latest              3ce676040d97        About a minute ago   163MB


- Lastly push the image to your publish Docker Hub Repository:

$ docker push  blgreco72dev/kube-tools

######################################################################################
Building Example Microservice and Pushing to DockerHub 
######################################################################################

- This section will explain the simple .NET Core C# Microservice that will be used
  within the examples.

- Source Location:  Microservices/Kube.Service/src
- DockerFile:       Microservices/Kube.Service/DockerFile

[Service APIs]
---------------------------------------------------------------------------------------
curl http://localhost:6200/api/pod/host-name
  - Returns the host name associated with the Pod in which container is running.  This 
    will be used when scaling containers to identify the container to which the request 
    was routed.

curl http://localhost:6200/api/pod/environment-variables
  - Returns all the environment variables defined for the running Container.

curl http://localhost:6200/api/pod/app-settings
  - Returns a simple class of settings defined within the service's application settings file.

- The above APIs can be called locally before running the Microservice within Kubernetes.

$ cd src/Kube.Service.WebApi/
$ dotnet run


[Side Node]:
-------------------------------------------------------------------------------------------
On Ubuntu the following command can be used to determine the process attached to a port:

lsof -i :6200

This can be helpful the process remains running and the service can't be restarted 
due to the port being in use.  The output has the PID that can be killed if needed.

-------------------------------------------------------------------------------------------

- The following will build the Microservice and publish to a public DockerHub repository.
- The microservice has a Docker file that allows the source to be built within a Docker
  Container so having .NET Core installed on your local development computer is not required.

- Run the following to build the Docker image.  This will create a new image tagged as a 
  specific version and also update the latest version to be the same.

$ docker build -t blgreco72dev/kube-service:v1.0 . 
$ docker tag blgreco72dev/kube-service:v1.0 blgreco72dev/kube-service

- Once done list all the built images:

$ docker images

REPOSITORY                             TAG                 IMAGE ID            CREATED             SIZE
blgreco72dev/kube-service              latest              55656d4be676        17 seconds ago      211MB
blgreco72dev/kube-service              v1.0                55656d4be676        17 seconds ago      211MB

- Next, push both versions of the image to your Docker Hub repository:

$ docker push blgreco72dev/kube-service:v1.0
$ docker push blgreco72dev/kube-service

- The image should now appear on your public Docker Hub Repository.

- Delete the local image so it can be pulled from Docker Hub to validate they have been pushed:

$ docker image rm 556 -f
$ docker pull blgreco72dev/kube-service:v1.0
$ docker pull blgreco72dev/kube-service

- If the pull was successful, the following message should appear:
  Status: Downloaded newer image for blgreco72dev/kube-service:v1.0


######################################################################################
### Running Commands within a Pod's Container
######################################################################################

- It is often useful to run commands within a Container running within a Pod.

- If the Pod only has a single container (which is most often the case), the
  command will be executed within that container.  If the Pod has more than 
  one container, the -c switch must be specified to select the container by
  name.

- The following will run a Container within a Pod based on an ubuntu image
  containing the ping and curl command line executable.

$ kubectl run client --image blgreco72dev/kube-tools

- Then the following will start an interactive shell session within the client
  Pod's running Container:

$ kubectl exec -it client -- sh

- The above will be used to send http requests to specific Pods running
  within the cluster.

- When done, just type exit.


######################################################################################
### Running Containers in Kubernetes
######################################################################################

- It is a best practice to define an YAML or JSON file to describe the container to be executed.
- However, it can also be done at the command line for testing.

- The following will create a pod.  This is of very limited use since its execution is not being
  managed by a ReplicaSet.  The following image contains a simple application that will returned
  the host name in which it runs and other available information.

$ kubectl run kube-service --image=blgreco72dev/kube-service --port=6200

- The above command will run the container within a Kubernetes pod.  A pod can contain multiple containers
  but most often only one container is contained within a pod.  The command specifies the image to be executed
  within a Container of the Pod and the Port to expose on the Pod to which the running process is bound.

- The .NET Core Web Api REST service is listening on port 6200. This can be found within the Program.cs 
  of the solution.

- Each Pod has its own IP address and host name.
- You cannot list containers but the current pods can be listed.
- All Pods within a Cluster are all attached to a shared network and can communicate with each other 
  using the IP address assigned to each Pod.  This also hold true if the Pods are on different nodes.
  This is implemented using a software based Pod network installed when building the cluster.

$ kubectl get pods -o wide

NAME              READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
client            1/1     Running   3          3h26m   192.168.144.101   w-node2   <none>           <none>
kube-service      1/1     Running   0          19s     192.168.129.243   w-node1   <none>           <none>

- The above results show kube-service Pod running on w-node1.  The IP of the created pod is 192.168.129.243.

- This pod has been created on the Pod network and therefore cannot be accessed from your development host 
  operating system.

- However, the Pod can be accessed from another Pod running within the same Pod notwork.  This can be shows by
  running a shell within a Container of another running Pod and pinging the Pod's IP address:

kubectl exec -it client -- sh

# ping 192.168.129.243

PING 192.168.129.243 (192.168.129.243) 56(84) bytes of data.
64 bytes from 192.168.129.243: icmp_seq=1 ttl=62 time=0.664 ms
64 bytes from 192.168.129.243: icmp_seq=2 ttl=62 time=0.809 ms
64 bytes from 192.168.129.243: icmp_seq=3 ttl=62 time=0.428 ms
64 bytes from 192.168.129.243: icmp_seq=4 ttl=62 time=0.631 ms
64 bytes from 192.168.129.243: icmp_seq=5 ttl=62 time=0.518 ms

- Also, the host name of the Pod can be obtained by sending a HTTP request to the Pod on port 6200
  using the associated REST url:

# curl http://192.168.129.243:6200/api/pod/host-name
Kube-service

- Later when multiple Pods are created to run the service (using a ReplicaSet), an unique postfix is 
  added to the name of the pod for each running instance.  This will allow us to identify the Pod to
  which a specific request was routed.

-------------------------------------------------------------------------------------------------------------------

- The following command can be used to view details of a specific pod:

$ kubectl describe pod kube-service

Name:         kube-service
Namespace:    default
Priority:     0
Node:         w-node1/192.168.56.5
Start Time:   Thu, 01 Oct 2020 19:35:24 -0400
Labels:       run=kube-service
Annotations:  cni.projectcalico.org/podIP: 192.168.129.243/32
              cni.projectcalico.org/podIPs: 192.168.129.243/32
Status:       Running
IP:           192.168.129.243
IPs:
  IP:  192.168.129.243
Containers:
  kube-service:
    Container ID:   docker://317613cbb4ba0fde43f123d8f6b22e9196f480945f82afce3fd3fb68f5d96580
    Image:          blgreco72dev/kube-service
    Image ID:       docker-pullable://blgreco72dev/kube-service@sha256:99fe59f1227b5c328babcd6d2074c8fa16b4d8c119a725d0cc4574e5860e6986
    Port:           6200/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 01 Oct 2020 19:35:27 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-pf4lx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-pf4lx:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-pf4lx
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m33s  default-scheduler  Successfully assigned default/kube-service to w-node1
  Normal  Pulling    9m32s  kubelet, w-node1   Pulling image "blgreco72dev/kube-service"
  Normal  Pulled     9m31s  kubelet, w-node1   Successfully pulled image "blgreco72dev/kube-service" in 1.725457642s
  Normal  Created    9m31s  kubelet, w-node1   Created container kube-service
  Normal  Started    9m30s  kubelet, w-node1   Started container kube-service


- The details contain the following:
    - The container running within the Pod and the image from which it was created.
    - The port the container exposes (this is specified within the Docker file used to build the image.)
    - volumes
    - List of events recorded pertaining to the Pod.


######################################################################################
### Looking Inside the Pod
######################################################################################

- Kubernetes is built on top of a container implementation.  The most common one being Docker.
- When the minikube VM or any of the Ubuntu VMs created for the multi node cluster running in 
  Virtual Box, Docker was one of the first installed packages.

- The following will ssh into the VM running the kube-service Pod to view the allocated Docker 
  images and Containers.

- There is only one node in minikube, but for the multi node cluster, execute the following to 
  determine the node on which the kubia Pod is running:

$ kubectl get pods -o wide

NAME              READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
client            1/1     Running   3          3h40m   192.168.144.101   w-node2   <none>           <none>
kube-service      1/1     Running   0          14m     192.168.129.243   w-node1   <none>           <none>

- In the above multi-node cluster, the kube-service Pod is running on w-node1.
- Next, run the following to determine the IP address of w-node1:

kubectl get nodes -o wide

NAME      STATUS   ROLES    AGE    VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
m-node1   Ready    master   5d5h   v1.19.2   192.168.56.4   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.11
w-node1   Ready    <none>   5d4h   v1.19.2   192.168.56.5   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.11
w-node2   Ready    <none>   5d1h   v1.19.2   192.168.56.6   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.11
w-node3   Ready    <none>   5d1h   v1.19.2   192.168.56.7   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.11

- If minikube is being used, run the following to ssh into the simple node:

minikube ssh

- For the multi node cluster above, ssh into w-node1:

 $ ssh w-node1@192.168.56.5

-------------------------------------------------------------------------------------------------------------------------------------------------

- Once logged into the Node running the Pod containing the kube-container container, complete the following to inspect the Docker objects:

- List the images installed on the Node:

$ docker images

REPOSITORY                        TAG                 IMAGE ID            CREATED             SIZE
blgreco72dev/kube-service         latest              55656d4be676        13 hours ago        211MB
blgreco72dev/kube-service         <none>              b5a7281f4a98        13 hours ago        211MB
blgreco72dev/kube-service         v1.0                ae514209a271        18 hours ago        211MB
blgreco72dev/kube-service-1       latest              7ec71e420a81        5 days ago          211MB
blgreco72dev/ubuntu-tools         latest              fcd47ed02192        7 days ago          114MB
k8s.gcr.io/kube-proxy             v1.19.2             d373dd5a8593        2 weeks ago         118MB
nginx                             latest              7e4d58f0e5f3        3 weeks ago         133MB
calico/node                       v3.16.1             0f351f210d5e        3 weeks ago         164MB
calico/pod2daemon-flexvol         v3.16.1             4cbe1ed86c35        3 weeks ago         22.9MB
calico/cni                        v3.16.1             4ab373b1fac4        3 weeks ago         133MB
calico/typha                      v3.16.1             c5132b2bf06f        3 weeks ago         52.2MB
blgreco72dev/kubia                latest              1bbf21a8a605        4 weeks ago         660MB
nginx                             alpine              6f715d38cfe0        7 weeks ago         22.1MB
traefik                           v1.7                80ab7ee8304c        2 months ago        76.4MB
jettech/kube-webhook-certgen      v1.2.2              5693ebf5622a        3 months ago        49MB
traefik                           1.7.24              100389fa48aa        6 months ago        76.4MB
k8s.gcr.io/pause                  3.2                 80d28bedfe5d        7 months ago        683kB
gcr.io/google-samples/hello-app   1.0                 bc5c421ecd6c        2 years ago         9.86MB

- The above shows several of the Kubernetes component images used to create containers on the VM.
- As expected, the blgreco72dev/kube-service image exits.

Side Note
----------------------------------------------------------------------------------------------------
There are some images that are not longer being used.  For example, traefik was once installed but
has since been replaced by NGINX.  Also, blgreco72dev/ubuntu-tools is an old image name not being
used.  Need to research how Kubernetes manages old images.  There is a Docker prune command that
will delete non-used images that might be called by Kubernetes at some point....
----------------------------------------------------------------------------------------------------

- List the running containers:

$ docker container ls


CONTAINER ID        IMAGE                       COMMAND                  CREATED             STATUS              PORTS               NAMES
2a649791fd6e        blgreco72dev/kube-service   "dotnet Kube.Service…"   10 minutes ago      Up 10 minutes                           k8s_kube-service_kube-service_default_3f38386a-cc3d-42dc-9b6b-99a2a5cb2ddd_1
93f0c8077a29        k8s.gcr.io/pause:3.2        "/pause"                 10 minutes ago      Up 10 minutes                           k8s_POD_kube-service_default_3f38386a-cc3d-42dc-9b6b-99a2a5cb2ddd_1
3655bd3b0c36        0f351f210d5e                "start_runit"            10 minutes ago      Up 10 minutes                           k8s_calico-node_calico-node-rblrf_calico-system_b5a2b8a3-37be-4647-8259-e177f6d79f98_18
1bca3c80082e        blgreco72dev/kubia          "node app.js"            10 minutes ago      Up 10 minutes                           k8s_kubia-container_kubia-set-bsxv4_default_e3af4a09-c61f-43a8-a6ea-4f1eb7fd1018_4
afe94090131a        k8s.gcr.io/pause:3.2        "/pause"                 10 minutes ago      Up 10 minutes                           k8s_POD_kubia-set-bsxv4_default_e3af4a09-c61f-43a8-a6ea-4f1eb7fd1018_4
3c6556617662        c5132b2bf06f                "/sbin/tini -- calic…"   10 minutes ago      Up 10 minutes                           k8s_calico-typha_calico-typha-6794d46fcc-946qr_calico-system_777553aa-11cf-484c-a362-ea529bd8b0d7_27
a5da0eb8e2c7        d373dd5a8593                "/usr/local/bin/kube…"   10 minutes ago      Up 10 minutes                           k8s_kube-proxy_kube-proxy-dgxd4_kube-system_430660f7-28fa-49c7-8de8-6c331490eddd_16
56a01ae246f2        k8s.gcr.io/pause:3.2        "/pause"                 11 minutes ago      Up 11 minutes                           k8s_POD_calico-node-rblrf_calico-system_b5a2b8a3-37be-4647-8259-e177f6d79f98_16
845b36a36d89        k8s.gcr.io/pause:3.2        "/pause"                 11 minutes ago      Up 11 minutes                           k8s_POD_calico-typha-6794d46fcc-946qr_calico-system_777553aa-11cf-484c-a362-ea529bd8b0d7_16
a0060898261b        k8s.gcr.io/pause:3.2        "/pause"                 11 minutes ago      Up 11 minutes                           k8s_POD_kube-proxy-dgxd4_kube-system_430660f7-28fa-49c7-8de8-6c331490eddd_16


- The first listed container is the .NET Core service running the image that was submitted to Kubernetes for execution.
- Unlike the images, the list of running containers are all being used and there are no leftovers from prior deployments.
- Next, inspect the logs for the .NET Core service:

$ docker logs 2a649791fd6e

      Overriding address(es) 'http://+:80'. Binding to endpoints defined in UseKestrel() instead.
info: Microsoft.Hosting.Lifetime[0]
      Now listening on: http://[::]:6200
info: Microsoft.Hosting.Lifetime[0]
      Application started. Press Ctrl+C to shut down.
info: Microsoft.Hosting.Lifetime[0]
      Hosting environment: Production
info: Microsoft.Hosting.Lifetime[0]
      Content root path: /microservice

- The above is the same log that would have been seen if the .NET Core service was executed locally
  on a developer's computer.

-------------------------------------------------------------------------------------------------------------------------------------

- When a Pod is created by submitting a command to the Kubernetes REST API, the service takes multiple Steps
  to run a Pod for the specified Docker image.  At a high level, the Scheduler is called with the Pod specification.
  The scheduler determines the Node on which the Pod should be created.  The Kubelet process running on the node 
  receives the request to create the Pod.  The Kubelet process downloads the Docker image and creates the corresponding
  container.  

- As shown below, the Kubelet process is also responsible for monitoring the running containers and assures they 
  remain running at all times.

- While still logged into the VM running the kubia example container, attempt to stop the running container to see 
 if it is recreated.  Note the ID of the current running container above for the running .NET Core service.

docker container stop 2a649791fd6e

- Then list the containers again:

$ docker container ls

CONTAINER ID        IMAGE                       COMMAND                  CREATED             STATUS                  PORTS               NAMES
9f6c0346b607        blgreco72dev/kube-service   "dotnet Kube.Service…"   1 second ago        Up Less than a second                       k8s_kube-service_kube-service_default_3f38386a-cc3d-42dc-9b6b-99a2a5cb2ddd_2
93f0c8077a29        k8s.gcr.io/pause:3.2        "/pause"                 21 minutes ago      Up 21 minutes                               k8s_POD_kube-service_default_3f38386a-cc3d-42dc-9b6b-99a2a5cb2ddd_1


- Note that the stopped container was not just restarted by kubia but a new container created with a new container ID.
- If the new container is not listed, run the command to list the containers several times as it can take a moment for
  it to be recreated.


- Next, attempt to remove the running container to determine if it is recreated:

$ docker container rm -f 9f6c0346b607

Listing the containers again, shows that it was almost instantly recreated:

CONTAINER ID        IMAGE                       COMMAND                  CREATED             STATUS                  PORTS               NAMES
2c815c0b5531        blgreco72dev/kube-service   "dotnet Kube.Service…"   1 second ago        Up Less than a second                       k8s_kube-service_kube-service_default_3f38386a-cc3d-42dc-9b6b-99a2a5cb2ddd_2
93f0c8077a29        k8s.gcr.io/pause:3.2        "/pause"                 23 minutes ago      Up 23 minutes                               k8s_POD_kube-service_default_3f38386a-cc3d-42dc-9b6b-99a2a5cb2ddd_1

- As a last attempt, the image will be deleted and then the container removed.

docker container rm 2c815c0b5531 -f
docker image rm 55656d4be676 -f

- If you list the images and containers, the image will have been re-pulled down and
  a new container will have been created.

- Exit the node's shell:

$ exit

--------------------------------------------------------------------------------------------------------------------------------

- The following can be executed to view the events that took place for a Pod:

$ kubectl describe pod kube-service

Name:         kube-service
Namespace:    default
Priority:     0
Node:         w-node1/192.168.56.5
Start Time:   Thu, 01 Oct 2020 19:35:24 -0400
Labels:       run=kube-service
Annotations:  cni.projectcalico.org/podIP: 192.168.129.245/32
              cni.projectcalico.org/podIPs: 192.168.129.245/32
Status:       Running
IP:           192.168.129.245
IPs:
  IP:  192.168.129.245
Containers:
  kube-service:
    Container ID:   docker://b6a6ccc830b952afc4d56e69e9da3a36e99159292acfc324acc265a29f0c868e
    Image:          blgreco72dev/kube-service
    Image ID:       docker-pullable://blgreco72dev/kube-service@sha256:99fe59f1227b5c328babcd6d2074c8fa16b4d8c119a725d0cc4574e5860e6986
    Port:           6200/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 02 Oct 2020 08:20:14 -0400
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 02 Oct 2020 07:53:17 -0400
      Finished:     Fri, 02 Oct 2020 08:14:15 -0400
    Ready:          True
    Restart Count:  2
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-pf4lx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-pf4lx:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-pf4lx
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason          Age                From               Message
  ----    ------          ----               ----               -------
  Normal  Scheduled       12h                default-scheduler  Successfully assigned default/kube-service to w-node1
  Normal  Pulling         12h                kubelet, w-node1   Pulling image "blgreco72dev/kube-service"
  Normal  Pulled          12h                kubelet, w-node1   Successfully pulled image "blgreco72dev/kube-service" in 1.725457642s
  Normal  Created         12h                kubelet, w-node1   Created container kube-service
  Normal  Started         12h                kubelet, w-node1   Started container kube-service
  Normal  SandboxChanged  38m (x2 over 38m)  kubelet, w-node1   Pod sandbox changed, it will be killed and re-created.
  Normal  Pulled          38m                kubelet, w-node1   Successfully pulled image "blgreco72dev/kube-service" in 500.475173ms
  Normal  Pulled          17m                kubelet, w-node1   Successfully pulled image "blgreco72dev/kube-service" in 450.39594ms
  Normal  Pulled          14m                kubelet, w-node1   Successfully pulled image "blgreco72dev/kube-service" in 652.799978ms
  Normal  Pulling         11m (x4 over 38m)  kubelet, w-node1   Pulling image "blgreco72dev/kube-service"
  Normal  Started         11m (x4 over 38m)  kubelet, w-node1   Started container kube-service
  Normal  Created         11m (x4 over 38m)  kubelet, w-node1   Created container kube-service
  Normal  Pulled          11m                kubelet, w-node1   Successfully pulled image "blgreco72dev/kube-service" in 3.519829897s


- The following command will delete the pod.  Not that this will take a few seconds to complete:

$ kubectl delete pod kube-service

- The following command is helpful to see the pods running across all nodes and namespaces:

kubectl get pods -o wide --all-namespaces


######################################################################################
### Creating Resources using YAML
######################################################################################

- A Pod was created at the command line above as follows:

$ kubectl run kube-service --image=blgreco72dev/kube-service --port=6200

- This approach should not be used within production since the configuration 
  is not documented. Instead resources can be defined in YAML.

- If an existing resource exists within Kubernetes that was created at the
  command line, the YAML can be generated.  This is also useful for viewing
  a resources current configuration on the server.

- The following command can be used to obtain an YAML definition for any type of resource.
  Below shows requesting YAML for the created Pod created above.  The YAML can be very verbose 
  and contains properties with default values that would not typically be set.

$ kubectl get pod

NAME              READY   STATUS    RESTARTS   AGE
client            1/1     Running   4          16h

$ kubectl get pod client -o yaml


- The YAML file consists of the following sections:
    - Metadata
    - Specification
    - Status

- Make sure the kube-service Pod has been deleted since the following will create the 
  same Pod by defining an YAML file instead.

$ kubectl delete pod kube-service

- When defining a YAML file for a Pod, the Ports on which the container listens
  can be specified.  This is just for documentation and has no bearing on how
  the container or pod is created.

- The following shows an YAML definition for a Pod:


cat definitions/pod-definition.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: kube-service
spec:
  containers:
  - image: blgreco72dev/kube-service
    name: kube-srv
    ports:
    - containerPort: 6200
      protocol: TCP

- The following will submit the YAML to the Kubernetes REST API:

$ kubectl create -f definitions/pod-definition.yaml 
$ kubectl get pods -o wide

NAME              READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES
client            1/1     Running   4          16h   192.168.144.103   w-node2   <none>           <none>
kube-service      1/1     Running   0          53s   192.168.129.246   w-node1   <none>           <none>

- The metadata section contains the resource name, labels, and namespace.

---------------------------------------------------------------------------------------------------------------------

- Applications running within containers typically write log files to standard
  out and standard error streams.

- The logs for a givin Pod can be see by issuing the following command:
$ kubectl logs kube-service


      Overriding address(es) 'http://+:80'. Binding to endpoints defined in UseKestrel() instead.
info: Microsoft.Hosting.Lifetime[0]
      Now listening on: http://[::]:6200
info: Microsoft.Hosting.Lifetime[0]
      Application started. Press Ctrl+C to shut down.
info: Microsoft.Hosting.Lifetime[0]
      Hosting environment: Production
info: Microsoft.Hosting.Lifetime[0]
      Content root path: /microservice


- If the Pod contains multiple containers, you must specify the container's name using the -c parameter.

- A Pod's logs are deleted when the Pod no longer exists.  If logs need to be persisted, they need to be
  written to a central error log store.


######################################################################################
### Pod Details
######################################################################################

- Pods can contain multiple containers.  The containers within a pod will always run
  on the same worker node.

- This can be used if the containers must be located on the same machine.  This can be 
  the case for inter-process communication or if the processes running within each of
  the containers need to access a shared file.

- Pods allows containers to be bound together and managed as a single unit.

- The containers within the pod can share certain resources and are not fully 
  isolated as when running within Docker.  Kubernetes does this by sharing the
  same set of Linux namespace between the two containers.  In Docker, each 
  container has its own set of namespaces.

- The containers within a Pod are all under the same network they share the same 
  host name and network interfaces.

- Each container running with in a Pod has a file systems isolated from each other.  
  However, the containers can share files by using volumes.

* Since the containers run within the same network namespace, they both have the
  the same IP address so ports must be unique across all containers.

- All Pods within a cluster are connected to the same software network so
  containers can call one another by using each others IP address.  However, 
  this is not a good idea since Pods and be stopped, deleted, or moved and
  will be assigned a new IP Address.  The following shows this:

------------------------------------------------------------------------------------------------------------

- List the details of all the pods to determine the node on which they are running:

$ kubectl get pods -o wide

NAME              READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES
client            1/1     Running   5          16h   192.168.144.103   w-node2   <none>           <none>
kube-service      1/1     Running   0          15m   192.168.129.246   w-node1   <none>           <none>

- The kube-service Pod is running on w-node1 and the client Pod is running on w-node2.

- The following will determine if a Pod can be accessed from the VM Host on which it is running:
- ssh into w-node1 and run the following to determine if the IP address of the pod(192.168.129.246) can be accessed:

$ ssh w-node1@192.168.56.5
    $ ping 192.168.129.246
    $ curl 192.168.129.246:6200/api/pod/app-settings
    $ exit

At the above commands shows, the IP assigned to the Pod can be accessed form Node VM on which it runs.

------------------------------------------------------------------------------------------------------------

- Next, determine if the same pod can be access from another Node VM that is part of the cluster.
- The following will SSH into w-node2 to determine if the IP address assigned to the Pod running 
  on w-node1 can be called:

$ ssh w-node2@192.168.56.6
    $ ping 192.168.129.246
    $ curl 192.168.129.246:6200/api/pod/app-settings

- Again, the Pod can be accessed.

-------------------------------------------------------------------------------------------------------------

- Lastly, determine if the service-api running within a Pod on w-node1 can be accessed from a shell
  running within a container on another Pod.  In this case w-node2:


  $ kubectl exec -it client -- sh
      # ping 192.168.129.246
      # curl 192.168.129.246:6200/api/pod/app-settings
      # exit
    
  And the above also works. 

  *** The above is just researching what components can access one another within the cluster.
  *** There would be of no reason to be doing the above as the IP addresses of Pods are not 
  *** static.  Also, it would be a bad design if a Container running within one Pod on a 
  *** node would use the IP address of another Node within the cluster.

-------------------------------------------------------------------------------------------------------------

- However, the service-api running within Pod cannot be accessed from outside the cluster 
  until a Service resource is created.  The next section will show how the Pod can be 
  accessed from outside the cluster for development needs by running a local proxy that
  will forward all calls from the Host computer to the pod running within the cluster.

- The above shows that an IP address assigned to a Pod running on a Node can be accessed by any
  other node that is part of the Cluster.

- All the containers comprising of a solution should not be all located within the same Pod.
  Only very closely related containers should be located within the same Pod.

- Since all Containers within a Pod will be executed on the same worker machine,
  they will all consume resources on that node and can't be relocated to a Node
  with additional resources.  

- Another reason why two containers many not be best located with the same Pod
  is that it make scaling harder.  In Kubernetes scaling (replication) is at the
  Pod level.  So if the Pod contains multiple containers, then they must be scaled 
  together at the same time.

- The main reason for having multiple containers in a Pod, is when one of the
  containers contains the core process and the other is more of a "side-car"
  process.  The side-car container executes code periodically (i.e. checks an API
  for updates to data consumed by the main container processes.)


######################################################################################
### Port Forwarding
######################################################################################

- Unless additional Kubernetes network resources are defined, a Pod cannot be 
  accessed without having to ssh into a node that is part of the cluster as
  shown above.  However, for testing specific nodes during development, 
  port forwarding can be used.

- The newly created pod cannot be accessed from outside the cluster since a service
  has not yet been defined.  However, the Pod can be communicated with using port
  forwarding to test the Pod for a local development box.

- Within one terminal, run the following:

$ kubectl port-forward kube-service 8888:6200

  Forwarding from 127.0.0.1:8888 -> 6200
  Forwarding from [::1]:8888 -> 6200

- The above is telling Kubernetes that the port 8888 on the host operating system
  should be forwarded to port 6200 of the pod named kube-service.

- In another terminal window, type the following to communicate with the container
  running within the Pod.

$ curl localhost:8888/api/pod/app-settings

- Cancel port forwarding in terminal by issuing Ctl-C

######################################################################################
### Creating Replication Controller
######################################################################################
  
- Before continuing, make sure the kube-service Pod is deleted:

 $ kubectl delete pod kube-service

- In kubernetes, Pod definition YAML files are rarely used and instead ReplicationController
  resources are defined.  This will be shown but note that ReplicationController resources 
  are no longer used by instead the very closely related ReplicaSet.

$ cat definitions/replication-ctl-definition.yaml 

apiVersion: v1
kind: ReplicationController
metadata:
  name: kube-service-rc
spec:
  replicas: 3
  selector:
    app: kube-srv
  template:
    metadata:
      name: kube-srv-pod
      labels:
        app: kube-srv
    spec:
      containers:      
      -  name: kube-srv-container
         image: blgreco72dev/kube-service
         ports:
          - containerPort: 6200


- The Selector does not need to be specified.  If not specified, it will be set to the 
  same value specified within (template/labels).


$ kubectl create -f definitions/replication-ctl-definition.yaml
$ kubectl get pods -o wide

NAME                    READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES
client                  1/1     Running   5          17h   192.168.144.103   w-node2   <none>           <none>
kube-service-rc-k5wqp   1/1     Running   0          32s   192.168.144.104   w-node2   <none>           <none>
kube-service-rc-mksth   1/1     Running   0          32s   192.168.138.38    w-node3   <none>           <none>
kube-service-rc-w6wj8   1/1     Running   0          32s   192.168.129.247   w-node1   <none>           <none>

- The following command can be used to view Replication Controllers:

$ kubectl get rc -o wide

NAME              DESIRED   CURRENT   READY   AGE   CONTAINERS           IMAGES                      SELECTOR
kube-service-rc   3         3         3       90s   kube-srv-container   blgreco72dev/kube-service   app=kube-srv

- The above shows that the Replication Controller specifies that 3 Pods are desired and that there
  are currently 3 that are ready.

- A Replication Controller is part of the Control Plane and is responsible for monitoring that a specified
  number of Pods remain running at all times.  On each worker node, the Kubelet process is responsible for
  making sure Pod containers remaining running.  If the Controller determines that a Pod is no longer running,
  it will communicate with the Scheduler so another Pod is created on a Node belonging to the cluster.

- The above shows that Kubernetes created 3 instances of the Pod defined within the YAML definition.

- Now that there are multiple Pods, each one has a random postfix value appended it its name.

- To validate that Kubernetes will maintain three running kube-service-rc Pods, the following will 
  delete one of the running Pods to see if a new one is created:


$ kubectl delete pod kube-service-rc-mksth
$ kubectl get pods -o wide

NAME                    READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
client                  1/1     Running   5          17h     192.168.144.103   w-node2   <none>           <none>
kube-service-rc-k5wqp   1/1     Running   0          4m51s   192.168.144.104   w-node2   <none>           <none>
kube-service-rc-r5p7n   1/1     Running   0          13s     192.168.138.39    w-node3   <none>           <none>
kube-service-rc-w6wj8   1/1     Running   0          4m51s   192.168.129.247   w-node1   <none>           <none>

- The above shows that the Pod deleted on w-node3 was recreated since the Pod name is now different.
- The scheduler determined that the new Pod should again be created on w-node3.

- Next, ssh into w-node3 and shut it down.

  $ ssh w-node3@192.168.56.7 
      $ sudo shutdown now

- Run the following command until it shows that Kubernetes had detected that w-node3 is offline and
  creates a new Pod instance on one of the other worker nodes.  Note:  It can take several minutes 
  for Kubernetes to detect a missing Node within the cluster.

$ watch kubectl get pods -o wide

NAME                    READY   STATUS        RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES
client                  1/1     Running       5          17h   192.168.144.103   w-node2   <none>           <none>
kube-service-rc-k5wqp   1/1     Running       0          15m   192.168.144.104   w-node2   <none>           <none>
kube-service-rc-r5p7n   1/1     Terminating   0          10m   192.168.138.39    w-node3   <none>           <none>
kube-service-rc-w459b   1/1     Running       0          93s   192.168.129.249   w-node1   <none>           <none>
kube-service-rc-w6wj8   1/1     Running       0          15m   192.168.129.247   w-node1   <none>           <none>

- The above shows that the Pod running on w-node3 has a status of Terminating and a new Pod is running w-node1.  
- Now w-node1 has two instances of the pod running.

- Next, start back up w-node3.  Once started, the Pod with the Terminating status will be deleted.  The new Pod created 
  on w-node1 however will remain and will not be terminated and automatically created back on w-node3 now that it is online.

$ watch kubectl get pods -o wide

NAME                    READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
client                  1/1     Running   5          17h     192.168.144.103   w-node2   <none>           <none>
kube-service-rc-k5wqp   1/1     Running   0          20m     192.168.144.104   w-node2   <none>           <none>
kube-service-rc-w459b   1/1     Running   0          6m52s   192.168.129.249   w-node1   <none>           <none>
kube-service-rc-w6wj8   1/1     Running   0          20m     192.168.129.247   w-node1   <none>           <none>

- In order to get one of the Pods off of w-node1 and back to w-node3, either the Replication Controller can be 
  deleted and recreated or one of the nodes running on w-node1 can be deleted.  There is no guarantee if one Pod
  is deleted on w-node1 that the new one will be created on w-node3, but since there is no reason for Kubernetes
  not to select w-node3 due to limited resources, it will most likely be created there:

$ kubectl delete pod kube-service-rc-w6wj8 

- Unlike taking w-node3 offline, Kubernetes will almost instantly detect the missing POd and create a new one.

NAME                    READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
client                  1/1     Running   6          17h     192.168.144.103   w-node2   <none>           <none>
kube-service-rc-k5wqp   1/1     Running   0          21m     192.168.144.104   w-node2   <none>           <none>
kube-service-rc-w459b   1/1     Running   0          8m13s   192.168.129.249   w-node1   <none>           <none>
kube-service-rc-wprvc   1/1     Running   0          33s     192.168.138.40    w-node3   <none>           <none>


- Since the "client" Pod was not created by a ReplicationController, it might no longer exist if it was also
  located on w-node3.  Since it was manually created at the command line, there is no ReplicationController
  watching the number of instances currently running.

- If you do not see the "client" pod, run the following to create another instance:

Next, create another instance of the client Pod:

$ kubectl run client --image blgreco72dev/kube-tools

----------------------------------------------------------------------------------------------------------

- The Describe command can be used to see ReplicationController details.

$ kubectl get rc 

NAME              DESIRED   CURRENT   READY   AGE
kube-service-rc   3         3         3       118m

$ kubectl describe rc kube-service-rc 


Name:         kube-service-rc
Namespace:    default
Selector:     app=kube-srv
Labels:       app=kube-srv
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=kube-srv
  Containers:
   kube-srv-container:
    Image:        blgreco72dev/kube-service
    Port:         6200/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  119m  replication-controller  Created pod: kube-service-rc-w6wj8
  Normal  SuccessfulCreate  119m  replication-controller  Created pod: kube-service-rc-mksth
  Normal  SuccessfulCreate  119m  replication-controller  Created pod: kube-service-rc-k5wqp
  Normal  SuccessfulCreate  115m  replication-controller  Created pod: kube-service-rc-r5p7n
  Normal  SuccessfulCreate  106m  replication-controller  Created pod: kube-service-rc-w459b
  Normal  SuccessfulCreate  98m   replication-controller  Created pod: kube-service-rc-wprvc


------------------------------------------------------------------------------------------------

- There is not a direct tie between Pods and ReplicationControllers.  The Pods associated
  with a controller is any Pod for which its labels match the controller's selector at
  any specific point in time.

- So if a Pods label no longer matches the controller's selector, the controller will 
  communicate with the schedule so another Pod is created on a node.

- The running Replication Controller has the following selector:  

    app=kube-srv

- The following command can be used to display a specific label value associated with Pods:

 $ kubectl get pods -o wide -L app

client                  1/1     Running   7          19h    192.168.144.106   w-node2   <none>           <none>            
kube-service-rc-k5wqp   1/1     Running   1          122m   192.168.144.109   w-node2   <none>           <none>            kube-srv
kube-service-rc-w459b   1/1     Running   1          109m   192.168.129.252   w-node1   <none>           <none>            kube-srv
kube-service-rc-wprvc   1/1     Running   1          101m   192.168.138.41    w-node3   <none>           <none>            kube-srv

- Next, the "app" label for one of the pods will be changed to no longer match the selector:

 $ kubectl label pod kube-service-rc-wprvc app=something --overwrite=true
 $ kubectl get pods -o wide -L app

NAME                    READY   STATUS    RESTARTS   AGE    IP                NODE      NOMINATED NODE   READINESS GATES   APP
client                  1/1     Running   7          19h    192.168.144.106   w-node2   <none>           <none>            
kube-service-rc-k5wqp   1/1     Running   1          123m   192.168.144.109   w-node2   <none>           <none>            kube-srv
kube-service-rc-sfhjg   1/1     Running   0          7s     192.168.138.42    w-node3   <none>           <none>            kube-srv
kube-service-rc-w459b   1/1     Running   1          110m   192.168.129.252   w-node1   <none>           <none>            kube-srv
kube-service-rc-wprvc   1/1     Running   1          102m   192.168.138.41    w-node3   <none>           <none>            something

- Almost instantly, Kubernetes detected that for a short period of time that only two Pods matched the selector as specified
  by the replication controller.  After this was detected, the Controller communicated to the Scheduler that another Pod 
  matching the template defined within the Replication Controller needed to be created.  

- The updated pod however will still exist.  It is just no longer associated with the controller.
- Next, the Pod with the renamed label will be changed back.

$ kubectl label pod kube-service-rc-wprvc app=kube-srv --overwrite=true
$ kubectl get pods -o wide -L app

NAME                    READY   STATUS        RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES   APP
client                  1/1     Running       7          19h     192.168.144.106   w-node2   <none>           <none>            
kube-service-rc-k5wqp   1/1     Running       1          126m    192.168.144.109   w-node2   <none>           <none>            kube-srv
kube-service-rc-sfhjg   0/1     Terminating   0          2m14s   <none>            w-node3   <none>           <none>            kube-srv
kube-service-rc-w459b   1/1     Running       1          112m    192.168.129.252   w-node1   <none>           <none>            kube-srv
kube-service-rc-wprvc   1/1     Running       1          104m    192.168.138.41    w-node3   <none>           <none>            kube-srv

- Now that there are 4 nodes matching the selector of the Replication Controller, Kubernetes detected this and is terminating
  one of the nodes so only a total of 3 will be running.

$ kubectl get pods -o wide -L app

NAME                    READY   STATUS    RESTARTS   AGE    IP                NODE      NOMINATED NODE   READINESS GATES   APP
client                  1/1     Running   7          19h    192.168.144.106   w-node2   <none>           <none>            
kube-service-rc-k5wqp   1/1     Running   1          127m   192.168.144.109   w-node2   <none>           <none>            kube-srv
kube-service-rc-w459b   1/1     Running   1          113m   192.168.129.252   w-node1   <none>           <none>            kube-srv
kube-service-rc-wprvc   1/1     Running   1          105m   192.168.138.41    w-node3   <none>           <none>            kube-srv


- The Pod's template within a ReplicationController can be updated.  However, any existing pods will not be affected.
  To apply the update the the existing pods, they need to be first deleted so new ones will be created by the updated
  replication controller.  For example, if the selector is updated to be more specific by adding a check on an additional 
  lab label, not of the existing pods would match and new ones would be created.

- The following will delete the replication controller and any Pods matching the selector:

$ kubectl delete -f definitions/replication-ctl-definition.yaml 
$ kubectl get pods -o wide

NAME              READY   STATUS    RESTARTS   AGE    IP                NODE      NOMINATED NODE   READINESS GATES
client            1/1     Running   7          19h    192.168.144.106   w-node2   <none>           <none>

- Before moving on to the next section, recreate the ReplicationController:

$ kubectl create -f definitions/replication-ctl-definition.yaml 

-----------------------------------------------------------------------------------------------------------------------------

- A resource definition can be edited within the terminal.
- Before doing so you can specify the shell editor if vi is not the preference.
- The following will set nano as the editor:

$ export KUBE_EDITOR="/usr/bin/nano"

- Execute the following command to load the definition so it can be edited:

$ kubectl edit rc kube-service-rc

- Next, an additional label will be added to the Pod's template.  After making the change, the
  pods and their labels will be listed.  Note that the addition label has not effect.  To update
  the nodes, they need to be deleted.  This will trigger the node to be recreated since the 
  current number no longer matches the desired value of 3.

- Add a label:   ver: v1 
- To the Replication Controller selector and also to the label of the Pod template.

- Once saved, Kubernetes will determine that there are no nodes containing both the app and ver
  labels and will use the associated template to create three new nodes:

kubectl get pods -o wide -L app -L  ver

AME                    READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES   APP        VER
client                  1/1     Running   7          19h     192.168.144.106   w-node2   <none>           <none>                       
kube-service-rc-54qsb   1/1     Running   0          5m46s   192.168.138.43    w-node3   <none>           <none>            kube-srv   
kube-service-rc-8v4bq   1/1     Running   0          28s     192.168.129.254   w-node1   <none>           <none>            kube-srv   v1
kube-service-rc-c2fvg   1/1     Running   0          5m46s   192.168.144.110   w-node2   <none>           <none>            kube-srv   
kube-service-rc-mjv97   1/1     Running   0          5m46s   192.168.129.253   w-node1   <none>           <none>            kube-srv   
kube-service-rc-v5mz7   1/1     Running   0          28s     192.168.144.111   w-node2   <none>           <none>            kube-srv   v1
kube-service-rc-w4fmh   1/1     Running   0          28s     192.168.138.44    w-node3   <none>           <none>            kube-srv   v1

- Now there are a total of 6 Pods running.  Three of which are no longer associated with the Replication Controller.
- To clean up the pods so there are only 3 matching the updated Replication Controller definition, delete all pods
  having a label of "app=kube-srv".  This will cause Kubernetes to detected that there are now no Pods matching the 
  Replication Controllers label and create 3 new ones.

$ kubectl delete pods -l app=kube-srv

- After all the Pods are deleted, run the following to confirm the new state:

$ kubectl get pods -o wide -L app -L  ver

NAME                    READY   STATUS    RESTARTS   AGE    IP                NODE      NOMINATED NODE   READINESS GATES   APP        VER
client                  1/1     Running   7          19h    192.168.144.106   w-node2   <none>           <none>                       
kube-service-rc-btbqq   1/1     Running   0          20s    192.168.138.45    w-node3   <none>           <none>            kube-srv   v1
kube-service-rc-crp9f   1/1     Running   0          20s    192.168.144.112   w-node2   <none>           <none>            kube-srv   v1
kube-service-rc-shmd7   1/1     Running   0          20s    192.168.129.255   w-node1   <none>           <none>            kube-srv   v1


=====================================================================================================
 Horizontally scaling pods
=====================================================================================================

- The number of replicas can be changed for a Replication Controller.

$ kubectl scale rc kube-service-rc --replicas=5
$ kubectl get pods -o wide

NAME                    READY   STATUS    RESTARTS   AGE    IP                NODE      NOMINATED NODE   READINESS GATES
client                  1/1     Running   7          19h    192.168.144.106   w-node2   <none>           <none>
kube-service-rc-btbqq   1/1     Running   0          99s    192.168.138.45    w-node3   <none>           <none>
kube-service-rc-crp9f   1/1     Running   0          99s    192.168.144.112   w-node2   <none>           <none>
kube-service-rc-ct5m9   1/1     Running   0          7s     192.168.138.46    w-node3   <none>           <none>
kube-service-rc-ll98d   1/1     Running   0          7s     192.168.129.193   w-node1   <none>           <none>
kube-service-rc-shmd7   1/1     Running   0          99s    192.168.129.255   w-node1   <none>           <none>

- The above shows that two more Pods where created.

- When deleting a ReplicationController, the associated Pods will also be deleted.

- But since the ReplicationController and Pods can exist independently, the Pods
  can remain and only the ReplicationController controller deleted.

$ kubectl delete rc kube-service-rc --cascade=false

$ kubectl get rc
    No resources found in default namespace.

$ kubectl get pods -o wide -L app -L  ver

NAME                    READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES   APP        VER
client                  1/1     Running   7          19h     192.168.144.106   w-node2   <none>           <none>                       
kube-service-rc-btbqq   1/1     Running   0          4m49s   192.168.138.45    w-node3   <none>           <none>            kube-srv   v1
kube-service-rc-crp9f   1/1     Running   0          4m49s   192.168.144.112   w-node2   <none>           <none>            kube-srv   v1
kube-service-rc-ct5m9   1/1     Running   0          3m17s   192.168.138.46    w-node3   <none>           <none>            kube-srv   v1
kube-service-rc-ll98d   1/1     Running   0          3m17s   192.168.129.193   w-node1   <none>           <none>            kube-srv   v1
kube-service-rc-shmd7   1/1     Running   0          4m49s   192.168.129.255   w-node1   <none>           <none>            kube-srv   v1

- The above Pods are not longer being monitored by a Replication Controller.  Now if one of the Pods is deleted, a new
  replacement Pod will no longer be created:

$ kubectl delete pod kube-service-rc-shmd7

- After this command is executed, there will now only be 4 kube-service-rc pods running and an additional Pod
  will not be created since they are no longer under management of a ReplicationController.

$ kubectl get pods -o wide -L app -L  ver

NAME                    READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES   APP        VER
client                  1/1     Running   7          19h     192.168.144.106   w-node2   <none>           <none>                       
kube-service-rc-btbqq   1/1     Running   0          6m34s   192.168.138.45    w-node3   <none>           <none>            kube-srv   v1
kube-service-rc-crp9f   1/1     Running   0          6m34s   192.168.144.112   w-node2   <none>           <none>            kube-srv   v1
kube-service-rc-ct5m9   1/1     Running   0          5m2s    192.168.138.46    w-node3   <none>           <none>            kube-srv   v1
kube-service-rc-ll98d   1/1     Running   0          5m2s    192.168.129.193   w-node1   <none>           <none>            kube-srv   v1

- The Pods can be re-associated with a Controller by creating a new controller with a selector
  matching the desired nodes.

- The following will re-create the controller by submitting the yaml file that is saved to disk.
  The definition saved to disk will be used which specifies that 3 Pod instances should be running
  that match the selector of:  app: kube-srv.  The above Pod listed all match this selector so there
  is an extra Pod running which Kubernetes will terminate.


$ cat definitions/replication-ctl-definition.yaml

apiVersion: v1
kind: ReplicationController
metadata:
  name: kube-service-rc
spec:
  replicas: 3
  selector:
    app: kube-srv
  template:
    metadata:
      name: kube-srv-pod
      labels:
        app: kube-srv
    spec:
      containers:      
      -  name: kube-srv-container
         image: blgreco72dev/kube-service
         ports:
          - containerPort: 6200


$ kubectl create -f  definitions/replication-ctl-definition.yaml 
$ kubectl get pods -o wide -L app -L  ver

NAME                    READY   STATUS        RESTARTS   AGE    IP                NODE      NOMINATED NODE   READINESS GATES   APP        VER
client                  1/1     Running       7          19h    192.168.144.106   w-node2   <none>           <none>                       
kube-service-rc-btbqq   1/1     Running       0          13m    192.168.138.45    w-node3   <none>           <none>            kube-srv   v1
kube-service-rc-crp9f   1/1     Running       0          13m    192.168.144.112   w-node2   <none>           <none>            kube-srv   v1
kube-service-rc-ct5m9   0/1     Terminating   0          11m    192.168.138.46    w-node3   <none>           <none>            kube-srv   v1
kube-service-rc-ll98d   1/1     Running       0          11m    192.168.129.193   w-node1   <none>           <none>            kube-srv   v1

- The above shows one of the Pods is being terminated to match the number of replicas specified within the ReplicationController.

$ kubectl get pods -o wide -L app -L  ver

NAME             READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES   APP     VER
client           1/1     Running   4          71m   192.168.129.203   w-node1   <none>           <none>                    
kubia-rc-66hxp   1/1     Running   0          20m   192.168.129.206   w-node1   <none>           <none>            kubia   one
kubia-rc-9pbwb   1/1     Running   0          17m   192.168.144.73    w-node2   <none>           <none>            kubia   one
kubia-rc-b8wjd   1/1     Running   0          20m   192.168.138.6     w-node3   <none>           <none>            kubia   one

- Now the number of pods match the desired number.
- Next, delete one of the Pods so a new replacement Pod is created.  Since the template within the Replication Controller does
  not specify the "ver" label, the newly created pod will only have the "app" label:

$ kubectl get pods -o wide -L app -L  ver

NAME                    READY   STATUS    RESTARTS   AGE    IP                NODE      NOMINATED NODE   READINESS GATES   APP        VER
client                  1/1     Running   7          19h    192.168.144.106   w-node2   <none>           <none>                       
kube-service-rc-btbqq   1/1     Running   0          13m    192.168.138.45    w-node3   <none>           <none>            kube-srv   v1
kube-service-rc-crp9f   1/1     Running   0          13m    192.168.144.112   w-node2   <none>           <none>            kube-srv   v1
kube-service-rc-ll98d   1/1     Running   0          12m    192.168.129.193   w-node1   <none>           <none>            kube-srv   v1

- Next, delete one of the running Pods so a new instance is created by the ReplicationController:

$ kubectl delete pod kube-service-rc-crp9f
$ kubectl get pods -o wide -L app -L  ver

NAME                    READY   STATUS    RESTARTS   AGE    IP                NODE      NOMINATED NODE   READINESS GATES   APP        VER
client                  1/1     Running   7          20h    192.168.144.106   w-node2   <none>           <none>                       
kube-service-rc-btbqq   1/1     Running   0          17m    192.168.138.45    w-node3   <none>           <none>            kube-srv   v1
kube-service-rc-ll98d   1/1     Running   0          15m    192.168.129.193   w-node1   <none>           <none>            kube-srv   v1
kube-service-rc-wbdtg   1/1     Running   0          24s    192.168.144.113   w-node2   <none>           <none>            kube-srv 

- Note that a new Pod was created by the ReplicationController but it does not have the VER label specified.  The last ReplicationController
  definition submitted to Kubernetes only had the "app: kube-srv" label specified within the Template section.  The template is used to determine
  how a Pod is to be created.   

- Lastly, delete the Replication Controller and any Pods associated by having matching labels:

$ kubectl delete -f definitions/replication-ctl-definition.yaml 
$ kubectl get pods -o wide 

NAME              READY   STATUS    RESTARTS   AGE    IP                NODE      NOMINATED NODE   READINESS GATES
client            1/1     Running   7          20h    192.168.144.106   w-node2   <none>           <none>


######################################################################################
### ReplicaSet
######################################################################################

- This is another resource type similar to a ReplicationController.
- The ReplicaSet now is mostly used instead of the ReplicationController.

- These two type of controllers are basically the same.  However, the ReplicaSet has 
  a more extensive selector support when compared to a ReplicationController.

$ cat definitions/replica-set-definition.yaml 

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: kube-service-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kube-srv
  template:
    metadata:
      labels:
        app: kube-srv
    spec:
      containers:
      - name: kube-srv-container
        image: blgreco72dev/kube-service
        ports:
        - containerPort: 6200


$ kubectl create -f definitions/replica-set-definition.yaml 

- Execute the following to see the created replication set:

$ kubectl get rs

NAME              DESIRED   CURRENT   READY   AGE
kube-service-rs   3         3         3       14s

- The following now shows the current running Pods:

$ kubectl get pods -o wide

NAME                    READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
client                  1/1     Running   8          22h     192.168.144.114   w-node2   <none>           <none>
kube-service-rs-6d6bk   1/1     Running   0          37s     192.168.138.47    w-node3   <none>           <none>
kube-service-rs-tw97t   1/1     Running   0          37s     192.168.144.117   w-node2   <none>           <none>
kube-service-rs-x2ck5   1/1     Running   0          37s     192.168.129.196   w-node1   <none>           <none>


- The describe command can be used to view the details of a ReplicaSet.

$  kubectl describe rs kube-service-rs


Name:         kube-service-rs
Namespace:    default
Selector:     app=kube-srv
Labels:       <none>
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=kube-srv
  Containers:
   kube-srv-container:
    Image:        blgreco72dev/kube-service
    Port:         6200/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  106s  replicaset-controller  Created pod: kube-service-rs-6d6bk
  Normal  SuccessfulCreate  106s  replicaset-controller  Created pod: kube-service-rs-x2ck5
  Normal  SuccessfulCreate  106s  replicaset-controller  Created pod: kube-service-rs-tw97t


- In terms of functionality, the service-api running within each of the above pods can
  be accessed on Port 6200 from either a VM shell or or a shell session running within
  a Pods container.

- As with a ReplicationController, each of the created Pods have an unique value appended 
  to the name of the Pod.  The name of the Pod is also its Host name.  Below shows accessing
  a Pod from a VM and from a shell running within a Pod's container.  

- SSH into w-node3 by running the following:

  $ ssh w-node3@192.168.56.7 

- Next, sent a request to the Api running in w-node1 and w-node2 from the command line of w-node3.

$ curl 192.168.144.117:6200/api/pod/host-name
    kube-service-rs-tw97t

$ curl 192.168.129.196:6200/api/pod/host-name
    kube-service-rs-x2ck5

- The service-api running within a Pod's container returns the host-name when requested from a specific Pod's IP.
- As the output shows, the host name is the same as the Pod's name.


- Delete the ReplicaSet and the currently managed nodes:

$  kubectl delete rs kube-service-rs


######################################################################################
### Replication and Controllers
######################################################################################

- Pods are usually not created directly.

- Instead, Replication Controllers and Deployments are used.  These two resource
  types monitor the Pods to which they are associated and will assure the correct
  number remain running on the cluster of nodes.

- Controllers monitor pods and are responsible for keeping the needed number 
  running within the cluster.

- On each node, the Kubelet process monitors each running container and will make
  sure it remains running.  If the Container's main process crashes, the Kubelet
  process will recreate a new container.

- However, a container can become unhealthy without the main process crashing so the
  container must provide a health-check that can be checked by Kubelet.

- These are called Liveness Probes and are specified within a Pod's specification.  
  These probes can be a HTTP GET method that is called or a command that is executed
  within the container.

- When a container is restarted, it is not the same container.  Instead a completely 
  new container is created.

- An initial delay can be specified that will delay the first call to the Liveness 
  check.  This can be important if the container can't receive requests with first
  started since Kubelet will start calling the Liveness probe after the container
  is created.

- It is important that the Liveness check is based on only the state of the 
  container and not on any external ones.  If another container cannot be called,
  it is the other container that needs to be restarted and not the current.

- Kubelet = monitors containers to make sure they remain running.
- Controllers = monitors Pods to make sure they remain running.

- Controllers usually monitor several replicas of a certain Pod type and assure 
  the number of running pods configured remain running.

- Pods are identified by labels, and when creating a Controller, these labels are
  used to identify the Pods that the Controller should monitor and assure that
  the configured desired number matches the current running.

- If a Pods labels are changed, from what is specified within the Controller's
  selector, the Pod is not stopped - it is just no longer managed by the controller.  
  If a label is added or changed that results in the Pod matching the Controller's
  selector, it will then be managed by the controller.


######################################################################################
### Services
######################################################################################

- Service Resources define how one service can discover other services for which it 
  needs to communicate with as part of a Microservice solution running in a Kubernetes 
  cluster.

  https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0

- These communications may be between Pods within the cluster and also from clients
  existing outside of the cluster.

- Services provide the following (Dependent of Type of Service):
    - Provides a Single point of Entry to a specific set of Pods all providing the same
      service.  The set of Pods are normally created by a ReplicaSet.

    - When a Service resource is created, it is given a static IP address and port.
      This static address will remain as long as the Service is not deleted.

    - The Service Resource knows the IP addresses and port of all the Pods running a 
      specific service component within the cluster.

    - When the Service receives a request, it routes to one of the Pods.

    - A service can also be configured with an external IP address that can be accessed by
      client outside of the cluster.  Services are also created internal to the cluster so
      services-apis running in other Pods can easily find other Pods service-apis to which
      the need to communicate.  This allows the calling service-api to find another service-api
      by one of the following two methods:

        - Environment Variable
        - DNS 

    - A service also load balances across the Pods running the service-api.


=====================================================================================================
LoadBalancer Service Type
=====================================================================================================

*** It was found that a LoadBalancer type of service will only work within a Cluster implementation
*** providing a LoadBalancer. The LoadBalancer examples will work using minikube or a Cloud provider
*** such as AWS, Google Cloud Services, or Azure.  For a bare-mental cluster created using kubeadm,
*** a Load Balancer is not configured. 

*** This is normally not an issue since an Ingress service type if often better.

- A LoadBalancer service is assigned an External-IP address that can be used to call a specific 
  service.  When the service-api is requested using the External-IP, the request is routed to one
  of the Pods running the service-api.  A LoadBalancer service is build on top of the ClusterIP and
  NodePort - which it uses to route the request.  When a LoadBalancer service is created, a ClusterIP
  and NodePort services are automatically created.


https://stackoverflow.com/questions/44110876/kubernetes-service-external-ip-pending


- Next, a service will be created to expose the service-api running within pods associated
  with the kubia-set.

$ kubectl expose replicaset kubia-set --type=LoadBalancer --port=8080

$ kubectl get services

NAME         TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)          AGE
kubernetes   ClusterIP      10.96.0.1       <none>          443/TCP          2d8h
kubia-set    LoadBalancer   10.104.193.98   10.104.193.98   8080:32041/TCP   27s

- Since the type of LoadBalancer was specified, the service will create an External-IP that
  can be used to call the service from outside the cluster.

- Running the following command several times shows how requests are load balanced and set
  to different Node/Pods:

curl 10.104.193.98:8080

greco@ubuntupc:~/_dev/git/dev-blog/kubernetes$ curl 10.104.193.98:8080
You've hit kubia-set-5ndp2
greco@ubuntupc:~/_dev/git/dev-blog/kubernetes$ curl 10.104.193.98:8080
You've hit kubia-set-pd6pb
greco@ubuntupc:~/_dev/git/dev-blog/kubernetes$ curl 10.104.193.98:8080
You've hit kubia-set-5ndp2
greco@ubuntupc:~/_dev/git/dev-blog/kubernetes$ curl 10.104.193.98:8080
You've hit kubia-set-5ndp2
greco@ubuntupc:~/_dev/git/dev-blog/kubernetes$ curl 10.104.193.98:8080

- Next, delete the service.

$ kubectl delete service kubia-set

=====================================================================================================
NodePort Service Type
=====================================================================================================

- Currently, the only way to communicate with a service-api running within a Pod's container
  is to directly use the IP address of the Pod and the Port the service is running on.  While
  Port Forwarding can be used, this is really only an option that should be used when developing.

- Like other higher-level resources, the Pods to which a Service relates is defined by 
  labels matching the selector defined the Service definition.

- A NodePort Service allows a Pod's service-api, running within a container, to be accessed from
  outside the Custer.  A NodePort achieves this by mapping a service-api's Port to an unique Port
  that is exposed off of the Nodes.  Then a service-api can then be referenced by using the IP address
  of the Node and the associated Port.

- Also, a NodePort adds this externally mapped Node Port to all Nodes and not just the Node containing
  the Pod running the service-api.  So any Node IP Address within the Cluster can be used to communicate
  with a service-api.

- When a Node receives a request on a Port for a service-api, the request is routed to a Pod on a Node
  that is running the service-api.  Even if the Node to which the initial request is sent has a Pod 
  running the service-api, routing is still applied and can be handled by any Node running the needed Pod.

-----------------------------------------------------------------------------------------------------------

- A Service Resource can be either created at the command line or by defining an YAML definition.

- First, the Service will be created at the command line.

- As with the ReplicationController and ReplicaSet controllers, Pods are not directly tied to the
  service.  Pods are associated with the Service by those having labels matching the Service's 
  Selector.

- Therefore, Service and Controllers can be defined independently from each others.

----------------------------------------------------------------------------------------------------------

- First start by running the following so there are multiple Pods running the same service-api.
- The created Pods will then have a NodePort service created.

$ kubectl create -f definitions/replica-set-definition.yaml 
$ kubectl get pods -o wide -L app

NAME                    READY   STATUS    RESTARTS   AGE    IP                NODE      NOMINATED NODE   READINESS GATES   APP
client                  1/1     Running   12         26h    192.168.144.114   w-node2   <none>           <none>            
kube-service-rs-7vm88   1/1     Running   0          78s    192.168.144.118   w-node2   <none>           <none>            kube-srv
kube-service-rs-dcwhn   1/1     Running   0          78s    192.168.138.48    w-node3   <none>           <none>            kube-srv
kube-service-rs-h7wk2   1/1     Running   0          78s    192.168.129.198   w-node1   <none>           <none>            kube-srv

- The following can be used to list all defined services.  The only service should be the one for the REST API
  interface used by kubectl.

$ kubectl get services -o wide

NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE    SELECTOR
kubernetes       ClusterIP   10.96.0.1      <none>        443/TCP        6d4h   <none>

-------------------------------------------------------------------------------------------------------------------------------------

- Run the following command to create a NodePort service:

$ kubectl expose replicaset kube-service-rs --type=NodePort --port=6200

- This command will define a new Service of type NodePort based on the selector defined within by the kube-service-rs
  ReplicaSet YAML definition file.  After running the command, list the current services:

$ kubectl get services -o wide

NAME              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE    SELECTOR
kube-service-rs   NodePort    10.102.102.249   <none>        6200:30561/TCP   9s     app=kube-srv
kubernetes        ClusterIP   10.96.0.1        <none>        443/TCP          6d4h   <none>

- The above shows they newly created Service matching the name of the ReplicaSet that was
used to determine which Pods it should be defined for.

- When the NodePort Service Resource was created, note that the IP 10.102.102.249  was assigned
  to the CLUSTER-IP.  When a NodePort is created, a ClusterIP service is also defined.

- Before running the above command, the only way to access a service-api running within
  a Container of a Pod was to use the IP address of the Pod.  Since Pods are not static
  and can be recreated for several scenarios, the IP address of a Pod should never be 
  used.

- Also, the only way to access a service-api externally to the Cluster was to use Port
  Forwarding which is only an option when developing.

- Defining the above NodePort Service has added the following to the Cluster:

      - The service-apis running within Pods, matching the ReplicaSet on which the
        NodePort Service Resource was created, can now be accessed from outside the
        cluster using the Port mapped to the service-api Port.

      - Above, the list of Services shows that Port 30561 has been mapped to the 
        internal service-api port 6200.

      - Now, the service-api can now be accessed by sending a request to any node
        within the Cluster using Port 30561 - regardless if it contains the needed
        running Pod.  So any Node within the Cluster can be used to invoke the 
        service-api regardless of where the Pod hosting the service-api is located
        within the cluster.

      - Next, several requests will be made to invoke the service-api from outside the Cluster.

------------------------------------------------------------------------------------------------

- For reference, run the following command to list all the Pods:

  $ kubectl get pods -o wide

    NAME                    READY   STATUS    RESTARTS   AGE    IP                NODE      NOMINATED NODE   READINESS GATES
    client                  1/1     Running   13         27h    192.168.144.121   w-node2   <none>           <none>
    kube-service-rs-7vm88   1/1     Running   1          25m    192.168.144.122   w-node2   <none>           <none>
    kube-service-rs-dcwhn   1/1     Running   1          25m    192.168.138.49    w-node3   <none>           <none>
    kube-service-rs-h7wk2   1/1     Running   1          25m    192.168.129.200   w-node1   <none>           <none>

- Since a NodePort allows sending external requests to Nodes, list all the Node to obtain their IP addresses: 

$ kubectl get nodes -o wide

  NAME      STATUS   ROLES    AGE    VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
  m-node1   Ready    master   6d4h   v1.19.2   192.168.56.4   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.11
  w-node1   Ready    <none>   6d4h   v1.19.2   192.168.56.5   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.11
  w-node2   Ready    <none>   6d1h   v1.19.2   192.168.56.6   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.11
  w-node3   Ready    <none>   6d     v1.19.2   192.168.56.7   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.11


- The following shows calling the service-api running within a Container of a Pod by now using the IP address 
  of a Node and no longer of a Pod:

  $ curl http://192.168.56.4:30561/api/pod/host-name
  $ curl http://192.168.56.5:30561/api/pod/host-name
  $ curl http://192.168.56.6:30561/api/pod/host-name
  $ curl http://192.168.56.7:30561/api/pod/host-name

    Note:  The Port specified on the above Web Api calls is not 6200 which was used when calling the service-api 
            directly on a Pod when using the Pod's IP address.  Above in the service listing, the Ports column
            for the kubia-set service is:  6200:30561/TCP

    - Notice how any Node IP Address of any Node can be used to call the service-api using Port 30561.
    - The Kubernetes Cluster built by using kubeadm defaults so no non-kubernetes related Pods are run on the Master node.
    - However, the service-api can be called using the IP Address of the Master Node as well:

       $ curl http://192.168.56.4:30561/api/pod/host-name


    - Also, run one of the above Curl commands multiple times on one of worker nodes containing a Pod running the service-api.
    - In this example, all Worker nodes have a an instance of the pod running.
    - Notice how even when the Worker node has a Pod with the service-api, requests are routed to any of the Worker Nodes.
    
      $ curl http://192.168.56.6:30561/api/pod/host-name

    - The above IP Address is for w-node2 which is running the Pod named:  kube-service-rs-7vm88
    - The following shows the Pod to which the request was routed after being called several times:

    kube-service-rs-h7wk2
    kube-service-rs-h7wk2
    kube-service-rs-dcwhn
    kube-service-rs-dcwhn
    kube-service-rs-h7wk2
    kube-service-rs-dcwhn
    kube-service-rs-7vm88
    kube-service-rs-dcwhn
    kube-service-rs-h7wk2
    kube-service-rs-7vm88
    kube-service-rs-7vm88
    kube-service-rs-7vm88
    kube-service-rs-h7wk2

-------------------------------------------------------------------------------------------------------

- Run the following command to show more details of the created service:

    $ kubectl describe service kube-service-rs

    Name:                     kube-service-rs
    Namespace:                default
    Labels:                   <none>
    Annotations:              <none>
    Selector:                 app=kube-srv
    Type:                     NodePort
    IP:                       10.102.102.249
    Port:                     <unset>  6200/TCP
    TargetPort:               6200/TCP
    NodePort:                 <unset>  30561/TCP
    Endpoints:                192.168.129.200:6200,192.168.138.49:6200,192.168.144.122:6200
    Session Affinity:         None
    External Traffic Policy:  Cluster
    Events:                   <none>

- The above shows the IP addresses for all the Pods running the service-api matching the definition
  of the Service (which uses the same Selector of the ReplicaSet on which it was based).

------------------------------------------------------------------------------------------------------

  - While the NodePort allows an api-service to be accessed externally from the Cluster and is a step
    in the right direction, it has the following short comings:

        - If the caller is using an IP address of a specific Node, the caller will no longer be able to 
          call the service-api if that node is taken offline.

        - The caller would have to maintain a list of all the IP address for the Nodes running within the cluster.
          However, additional nodes can be added in the future or existing ones decommissioned and the clients 
          would need to be updated.  

        - There is not a Single IP address that can be used to access the service.  NOTE:  When using the
          LoadBalancer service type, EXTERNAL-IP will be set to a static IP for the life of the service to
          which clients can send requests.  The LoadBalancer then forwards the request to one of the Pods
          IPs listed within the Service Description above.  Then the service is again forwarded to one of the 
          Pods as we already seen.  However, the LoadBalancer Service type is not supported by all clusters.

        - The LoadBalancer and NodePort Service types also have a short comming in terms of managing access
          to multiple service APIs.  For these types of Services, a new one must be created for each and every
          service-api.  So while the LoadBalancer does have a single IP for entry, it is only for a given serivce.

        - Later, the Ingress Service will be shown which addresses these issues by having a single IP address
          and defines URL Routes used to identify a specific service-api. 

---------------------------------------------------------------------------------------------------------

  - A NodePort type Service can also be created using an YAML definition as follows:

    cat definitions/nodeport-srv-definition.yaml  

    apiVersion: v1
    kind: Service
    metadata:
      name: kube-service-np
    spec:
      type: NodePort
      ports:  
      - port: 80
        targetPort: 6200
        nodePort: 31490
      selector:
        app: kube-srv

    - Before creating the above service, first delete the current one created earlier:

    $ kubectl delete service kube-service-rs

    - Then create the service as follows:

    $ kubectl create -f definitions/nodeport-srv-definition.yaml

    $ kube get services -o wide

NAME              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE     SELECTOR
kube-service-np   NodePort    10.103.203.208   <none>        80:31490/TCP   19s     app=kube-srv
kubernetes        ClusterIP   10.96.0.1        <none>        443/TCP        7d19h   <none>

    - Validate that the service-api can be called on any Node that is a member of the cluster:

    $ curl http://192.168.56.4:31490/api/pod/host-name
    $ curl http://192.168.56.5:31490/api/pod/host-name
    $ curl http://192.168.56.6:31490/api/pod/host-name
    $ curl http://192.168.56.7:31490/api/pod/host-name

    - When done, the serivce can be deleted:

    $ kube delete services kube-service-np

================================================================================================================
ClusterIP Service Type:
================================================================================================================

- Above showed that when a NodePort service was created, a ClusterIP service type was also created
  as seen by the Cluster-IP column having a value.

- While a NodePort exposes the service externally on any Node contained within the Custer at a specific 
  externally defined Port, not all services within a Microservice architecture need be accessed by external
  clients.

- Only the services that must be called by external clients should be exposed.

- This section will show how to create ClusterIP service type and how it can only be accessed within
  the cluster.

- When defining a Microservice based solution, all services should be considered to determine which
  ones should be make external and which ones should remain internal.  An example of Microservices 
  that should remain internal are services providing confidential information, services that process
  messages received on a message bus, or Microservices providing common functionality internally to 
  all other Microservices running with the cluster.

$ cat definitions/clusterip-srv-definition.yaml

apiVersion: v1
kind: Service
metadata:
  name: kube-service-ci
spec:
  ports:
  - port: 80
    targetPort: 6200
  selector:
    app: kube-srv

$ kubectl create -f definitions/clusterip-srv-definition.yaml 
$ kubectl get services -o wide

NAME              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE     SELECTOR
kube-service-ci   ClusterIP   10.97.105.95   <none>        80/TCP         34s     app=kube-srv
kubernetes        ClusterIP   10.96.0.1      <none>        443/TCP        6d20h   <none>


- The above is declaring a ClusterIP service using an YAML file.  If the type of service
  is not specified, it will default to ClusterIP.

- The above shows that just a ClusterIP service was created.  Unlike a NodePort, the PORTS(S) 
  column does not specify the IP address to call the service-api on the VM Nodes contained 
  within the cluster.  Therefore, the service can only be access from within the cluster.

- Note that the Service definition shows the service should available on port 80 by mapping
  it to port 6200 on which the service-api is listening.

- Validate this by calling the service-api from within a Node and also from within a container
  running within a Pod:

    $ kubectl exec -it client -- sh

       - Then type the following curl command:
       
       # curl 10.97.105.95/api/pod/host-name

       - You will get a response from one of the Pod's container running the service-api

       # exit

  - Next ssh into one of the Nodes that is a member of a cluster:

    $ ssh w-node1@192.168.56.5
      curl 10.97.105.95/api/pod/host-name

    - You will get a response from one of the service-apis running within a Pod container. 

-------------------------------------------------------------------------------------------------

- While a client service running within the cluster can use the static IP address of the service
  to call any Pod hosting the service-api, there are better way rather than using the port directly.
  The client service can discover the IP address using the following methods.

- If a Pod is created after the service has been created, Kubernetes defines an Environment Variable
  that the client service-api can use containing the value of the IP address.  

- Delete the client Pod and recreate it so it will have been created after the Services.

$ kubectl delete pod client
$ kubectl run client --image blgreco72dev/kube-tools


- List the serivces to use as a reference:

NAME              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
kube-service-ci   ClusterIP   10.104.166.225   <none>        80/TCP    89m
kubernetes        ClusterIP   10.96.0.1        <none>        443/TCP   7d21h


- Next exec into a Pod's container and list the environment variables.

$ kubectl exec client -- env


PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=client
** KUBE_SERVICE_CI_PORT_80_TCP_ADDR=10.104.166.225
KUBERNETES_SERVICE_HOST=10.96.0.1
** KUBE_SERVICE_CI_SERVICE_HOST=10.104.166.225
** KUBE_SERVICE_CI_PORT_80_TCP=tcp://10.104.166.225:80
** KUBERNETES_PORT_443_TCP_PROTO=tcp
** KUBE_SERVICE_CI_SERVICE_PORT=80
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
** KUBE_SERVICE_CI_PORT=tcp://10.104.166.225:80
** KUBE_SERVICE_CI_PORT_80_TCP_PROTO=tcp
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_PORT_443_TCP_PORT=443
** KUBE_SERVICE_CI_PORT_80_TCP_PORT=80
HOME=/root

--------------------------------------------------------------------------------------------

- The other, and better option, is to use the build in DNS server.
- This DNS service can be accessed by all Pod service-apis running in the cluster.
- The DNS service is a member of the kube-system namespace:


- Each service giets a DNS entry in the internal DNS server, and client pods that know the name
  of the service can access it through it fully qualified domain name (FQDN) instead of of using
  the corresponding environment variable.

- The following is the DNS entry for the kube-service:

kube-service-ci.default.svc.cluster.local

- If the calling node is within the same Namespace, then just kube-service-ci can be used.

- This can be tested by running bash inside one of the pod containers:

$ kubectl exec -it client -- bash

- Once inside the container the following will work:

# curl http://kube-service-ci.default.svc.cluster.local/api/pod/host-name
# curl http://kube-service-ci/api/pod/host-name

- If the run the commands above a few times, you will again see that the service 
  routes to different pods belonging to the service.


=====================================================================================
Service Endpoints
=====================================================================================

- When either a NodePort or ClusterIP servce types are created, Service Endpoint
  resources are also created and used by the services.

- Endpoints are located between Serivces and Pods.
- Endpoints are a list of Pod IP Addresses and Ports exposing a serivce. 
- Service Selectors are used to create the list of Endpoints based on the IP and Ports
  of all Pods matching the selector.

- The following will list the currently defined Service Endpoint:

$ kubectl get endpoints 

NAME              ENDPOINTS                                                        AGE
kube-service-ci   192.168.129.201:6200,192.168.138.50:6200,192.168.144.123:6200    11m
kubernetes        192.168.56.4:6443     

- The Ingress Service type discussed next does not create endpoints as done by the
  NodePort and Cluster service types.  Therefore, one of these two service types
  must be created first.

=====================================================================================
Ingress Serivce Type
=====================================================================================

- The issue with creating services is that one is needed for each service-api.  
  Each such created service will have its own static IP address and Port (for the case of the
  LoadBalancer & ClusterIp service types).  For the NodePort serivce type, an Port is exposed
  on all Nodes and can be access by using any of the Node's corresponding IP addresses.  

- Ingress is the next step in that it exposes only a single static IP address and Port that can 
  be used to route requests to multiple internal defines service-apis running within Pods.

- For this to function, an Ingress controller needs to be running within the cluster.  This is already
  the case for most cloud hosted Kubernetes services.  But if using minikube, it must be enabled.

$ minikube addons list
$ minikube addons enable Ingress

-----------------------------------------------
Installing Ingress Controller (bare Metal)
-----------------------------------------------

- When creating a Kubernetes cluster using kubeadm, an Ingress controller is not configured by default.
- The following URL provides a list of Ingress Controllers:

  https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/

- For this Kubernetes Cluster, the NGINX Ingress controller will be utilized.  I found the documentation to
  be rather lacking in this area but was able to get Ingress working for this controller.  Additional time,
  should be taken to research in greater depth.  However, this is the most popular option it seems.

- The Bare-Metal (Using NodePort) installation was used:

  https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal

- This was completed by running the following:

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.35.0/deploy/static/provider/baremetal/deploy.yaml

- After the above installs, the following shows the created resources providing the Ingress NGINX implementation.

$ kubectl get pods -n ingress-nginx -o wide

NAME                                        READY   STATUS      RESTARTS   AGE    IP                NODE      NOMINATED NODE   READINESS GATES
ingress-nginx-admission-create-q67mx        0/1     Completed   0          3d4h   192.168.129.228   w-node1   <none>           <none>
ingress-nginx-admission-patch-2hhw8         0/1     Completed   0          3d4h   192.168.144.91    w-node2   <none>           <none>
ingress-nginx-controller-84cb46fccd-84pgt   1/1     Running     5          26h    192.168.144.125   w-node2   <none>           <none>


- The above shoes the NGINX pod running that provides the implementation.  The other two pods with
a status of Completed at first seems alerting as if something when wrong.  However, after doing
some research, these are jobs that executed during the installation.  In Kubernetes, Pods running 
one-time jobs are not deleted when done so the logs can be inspected before deletion.

$ kubectl get jobs -n ingress-nginx

NAME                             COMPLETIONS   DURATION   AGE
ingress-nginx-admission-create   1/1           2s         24h
ingress-nginx-admission-patch    1/1           2s         24h

- The following shows the NGINX services created:

$ kubectl get services -n ingress-nginx

NAME                                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             NodePort    10.99.185.220   <none>        80:31201/TCP,443:30557/TCP   24h
ingress-nginx-controller-admission   ClusterIP   10.102.236.16   <none>        443/TCP                      24h

- The above shows two services.  The NodePort service is used to invoke the NGINX Ingress implementation to route
  specific web request URLS to specific service-apis running within Pods.  Above, 31201 is listed as the Port on
  which the ingress service listens for requests.  As with any other type of NodePort based Service, this port 
  can be used on any of the Nodes running within the cluster to access the service.

*** However, a single static IP address is assigned to the Ingress Controller and is what should be used.
*** As discussed before, clients should never need to know the IPs of the Nodes running within the cluster.
*** This will be shown in the next section after an Ingress Controller has been created.

------------------------------------------------------------------------------------------------------------------

============================================================================================
Creating Ingress Controller for ReplicaSet
============================================================================================

- The following are the steps required for exposing a service to outside the Cluster using an Ingress Controller:
    1. Define a ReplicaSet for the Pod to host the service-api
    2. Create a ClusterIP or NodePort Service that will select the Pods defined by the ReplicaSet
    3. Define the Ingress Controller so it's selector references the Service.

- Run the following to create Pods running the .NET Core Microservice:

   $ kubectl create -f definitions/replica-set-definition.yaml 
  
- Next, run the following to create a ClusterIP service for the Pods:

    $ kubectl create -f definitions/clusterip-srv-definition.yaml 

- The above two commmands have already been explained in prior sections.  At this point, the service-api can
   only be access from whin the Cluster.  Next, an Ingress Controller will be created to expose the serivce.

cat definitions/ingress-controller-definition.yaml 

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: kube-service
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: kube-apis.info
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: kube-service-ci
            port:
              number: 6200

- Next, create the ingress controller by submitting the above definition file to Kubernetes.

$ kubectl create -f definitions/ingress-controller-definition.yaml 

- Run the following command to view the created Ingress Controller and Ingress Service:

$ kubectl get ingress

NAME           CLASS    HOSTS            ADDRESS        PORTS   AGE
kube-service   <none>   kube-apis.info   192.168.56.6   80      2m30s

$ kube get services -n ingress-nginx

NAME                                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             NodePort    10.99.185.220   <none>        80:31201/TCP,443:30557/TCP   4d2h
ingress-nginx-controller-admission   ClusterIP   10.102.236.16   <none>        443/TCP                      4d2h


- Above the path "/" was used.  But to add additional service-apis, multiple paths
  can be defined, one for each CusterIP service (which is defined on a set of pods
  having matching labels) that Ingress matches to the request of the URL to determine 
  the service to call and forward the request.

----------------------------------------------------------------------------------------------------------------------

- Note the following about the above ingress information and service-api call:
      - An address will be assigned to the Ingress controller.  This can take a minute
        before an IP address is shown.
      - The listed IP address, based on our Kubernetes Cluster setup, is for one of the Nodes 
        running within the Cluster.
      - Within the list of Services belonging to the ingress-nginx namespace, the first one listed
        if of type NodePort.  It is of type NodePort since this was the option used when the creating
        the cluster.  This serivce has 31201 listed as the node-port.
      - The IP address assigned to the Ingress controller (192.168.56.6) and the port assigned to the
        NodePort ingress Controller (31201) are used to invoke configured ingress services.

      
- Since the Host name kube-apis.info was specified for the ingress entry that exposes the serivce-api,
  it must be specified when making the call as follows:

$ curl 192.168.56.6:31201/api/pod/host-name -H "Host:kube-apis.info"

- As before when using the other serivce types within an Ingress Controller, making multiple calls will
  route the request to any of the Pods running the service-api:

    kube-service-rs-txzmd
    kube-service-rs-txzmd
    kube-service-rs-jdd9g
    kube-service-rs-h4lkd
    kube-service-rs-txzmd

- The other option is to add the following entry to your /etc/hosts file:
    
  $ sudo nano /etc/hosts

  192.168.56.6 kube-apis.info

- After the above entry is made, the service-api can be called as follows:

  $ curl kube-apis.info:31201/api/pod/host-name

---------------------------------------------------------------------------------------------------------

- Additional notes about Ingress Controllers:

  - Since the Ingress-nginx Serivce is being implemented as a NodePort serivce, the IP of any Node within
    the cluster can be use to invoke the service-api as can be done with any type of NodePort serivce:

        $ curl 192.168.56.7:31201/api/pod/host-name -H "Host:kube-apis.info"
        $ curl 192.168.56.4:31201/api/pod/host-name -H "Host:kube-apis.info"

  - For an Ingress controller to function, it must have Endpoints defined for the serivce-api indicating
    which Pods provide the service-api implementation.  The Endpoints are not created directly but are
    done so by defining a NodePort or ClusterIP service.  These endpoints can be listed:

        $ kubectl get endpoints

        NAME              ENDPOINTS                                                     AGE
        kube-service-ci   192.168.129.217:6200,192.168.138.1:6200,192.168.144.76:6200   29m
        kubernetes        192.168.56.4:6443 

  - For this example, the Endpoints where created by declaring a ClusterIP service.  A NodePort service
    could have also been used.  However, it is best to reduce the surface area for how a service-api
    can be called.  If a NodePort was used, the service-api could have also been invoked using the specified
    Port on any Node's IP Address within the cluster. 

  - Ingress Controllers should only expose the service-apis that need to be accessed from outside the cluster.

----------------------------------------------------------------------------------------------------------------

All the details of the Ingress NGINX service will be viewed as follows: 

$ kubectl describe service -n ingress-nginx ingress-nginx-controller

Name:                     ingress-nginx-controller
Namespace:                ingress-nginx
Labels:                   app.kubernetes.io/component=controller
                          app.kubernetes.io/instance=ingress-nginx
                          app.kubernetes.io/managed-by=Helm
                          app.kubernetes.io/name=ingress-nginx
                          app.kubernetes.io/version=0.35.0
                          helm.sh/chart=ingress-nginx-2.13.0
Annotations:              <none>
Selector:                 app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx
Type:                     NodePort
IP:                       10.99.185.220
Port:                     http  80/TCP
TargetPort:               http/TCP
NodePort:                 http  31201/TCP
Endpoints:                192.168.144.73:80
Port:                     https  443/TCP
TargetPort:               https/TCP
NodePort:                 https  30557/TCP
Endpoints:                192.168.144.73:443
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>

- The above shows the details of the NodePort based service and how Node Port 31201 maps to a Pod with an IP
  of 192.168.144.73 on port 80 for HTTP traffic.

$ kubectl get pods -o wide -n ingress-nginx

NAME                                        READY   STATUS      RESTARTS   AGE    IP                NODE      NOMINATED NODE   READINESS GATES
ingress-nginx-admission-create-q67mx        0/1     Completed   0          4d3h   192.168.129.228   w-node1   <none>           <none>
ingress-nginx-admission-patch-2hhw8         0/1     Completed   0          4d3h   192.168.144.91    w-node2   <none>           <none>
ingress-nginx-controller-84cb46fccd-84pgt   1/1     Running     7          2d1h   192.168.144.73    w-node2   <none>           <none>

- The above shows that the IP address of 192.168.144.73  corresponds to a Pod running on w-node2.  As with all other Pods, this Pod can 
  be moved to another Node based on the current status of the cluster.  This Pod provides the NGINX Ingress implementation.

- The following shows the details for the Ingress:

$ kubectl describe ingress kube-service

Name:             kube-service
Namespace:        default
Address:          192.168.56.6
Default backend:  default-http-backend:80 (<error: endpoints "default-http-backend" not found>)
Rules:
  Host            Path  Backends
  ----            ----  --------
  kube-apis.info  
                  /   kube-service-ci:6200   192.168.129.217:6200,192.168.138.1:6200,192.168.144.76:6200)
Annotations:      nginx.ingress.kubernetes.io/rewrite-target: /
Events:           <none>


**** TLS NOTE:
  The communication between the client anbd the ingress-controller is encripted (assuming you setup TLS), where as the communication between the controller and the backend pod isn't.  
  The application running in the pod doesn't need to support TLS.  For example, if the pod runs a web server, it can accept only HTTP traffic and let the Ingress controller take care 
  of everything related to TLS.
****

######################################################################################
### Exposing Services to External Clients Additional Notes
######################################################################################

- This allows services-apis to be exposed so they can be accessed from outside the cluster.

- There are different ways of exposing a service-api contained within a container of a Pod.

- The first way to to define a service of type NodePort.  What this does is exposes the 
  service-api on all nodes contained within the cluster (regardless if the node actually
  has the pod containing the service-api).  The NodePort defines an Port that is exposed
  by each node running within the cluster to which can be used to access the service using
  the IP address of any node with the defined node-port to access the service.

- Then the request to forward to one of the nodes within the cluster actually running a 
  Pod containing a container with the service-api.  Also note that the static IP/Port of
  the service is used to invoke the service-api.

- NOTE:  If you are using a Kubernetes Cloud service, the IP addresses and ports of the
  Nodes might need to be configured to alow access within the firewall.  Which is fine
  since a higher-level service type like LoadBalancer or Ingress should be used.

- Now, the service-api is technically exposed to external client using the defined port
  on any of the nodes within the cluster.

- The issue with the above configuration is that if you provide an external client with
  one of the node's IP addresses and exposed port, used to the call the service-api, there
  will be an issue if the Node is taking offline.

- The next step is to place a load balancer in front of all the nodes so incoming requests
  are routed to any of the cluster nodes.

- Most cloud based Kubernetes hosted services provision such a load balancer and all that
  is required to to specify LoadBalancer for the Service resource type.

- The following is an overview:
    - When a Pod is created to run a container's service-api, by default, it can only be
      accessed from within the cluster from other Pod container service-apis using the IP
      address of the Pod directly.
    
    - Container service-apis can call the other service by using the IP address and Port
      of one of the Pods with the cluster hosting the service-api.  The issue with this is
      Pods can be moved to other hosts after they are stopped or crashed or as they are scaled
      up and down.  When this happens, the IP address associated with the Pod will not 
      longer exist.
    
    - To address the prior issue, a ClusterIP Service can be defined for a specific service-api.
      The service definition uses labels to determine the Pods that should be associated with
      the created service.  After the service is created, it defines a static IP address 
      known as the Cluster-IP that all clients can call (client inside the cluster) to
      invoke the service-api.  The Service Resource will forward the request to one of the
      Pods running a container hosting the service-api.  The client can then use the static
      IP address and Port to call the service.  The client's can obtain the service static
      IP address and Port using either Environment Variables defined by Kubernetes or the
      DNS server that is running within the cluster.
    
    - The next case is for service-apis that need to be accessed from outside of the cluster.

    - The first approach is to define a service as being a NodePort type.  This type of service
      defines a Port that is exposed on each Node within the Cluster that can be used to call the
      service-api running within a Pod's container.  Regardless if the Node called is actually
      hosting the service-api, the call will be routed to one of the Pods's that is running the
      service-api.  The defined port is accessible from outside the cluster by clients.  Clients,
      can be given the IP address of any of the nodes and they call invoke the service using the
      exposed port.  Note that this method will usually require opening the port on the firewall.

    - The issue with the last item is that the client is hard-coded to a specific Node IP address.
      If the node is taken offline, the client will no longer be able to access the service-api.
      So the next step is to define a service of type LoadBalancer which contains the functionally
      of a NodePort but also defines a Static IP address and Port that will load balance to any of
      the internal nodes using the defined node-port.  When creating a LoadBalancer service the 
      NodePorts are created automatically.  

    - With the load balancer service defined, clients can now call the service using the static IP 
      address and port of the LoadBalancer and the request will be routed to on of the internal nodes
      defined within the cluster.  Then the request will be routed when received by the node to one
      of the nodes actually hosting the service-api.

    - The one issue with the above is that a service (and thus a separate static IP address and Port)
      need to be create for each and every microservice.  

    - So the next step in progresses to what is called an Ingress Resource which is documented next.


######################################################################################
### Connecting to Services Outside the Cluster
######################################################################################

- This is the case when a service-api, running within a Pod contained within the cluster,
  needs to call another Api that is located outside of the Cluster.

- As mentioned within the Endpoints section, Services are not directly tied to Pods but are
  indirectly related by maintaining a list of Endpoints (IP/Port) for the Pods providing the
  serivce API.  This list is build by finding all Pods having labels matching the Service's 
  selector.

- Given this, Endpoints can be manually created to serivce-apis outside of the cluster and
  assocated with a serivce.  This provides a single place where the external IP address and
  port can be configured.

- This is accomplished by defining both a Service and Endpoint resources having the 
  same names.   

- Pods created after the Service can reference the external serivce using the 
  created environment variable.

$ cat definitions/external-cluster-endpoints.yaml 

  apiVersion: v1
  kind: Endpoints
  metadata:
    name: external-service
  subsets:
    - addresses:  
      - ip: 172.217.2.110 
      - ip: 205.251.242.103
      ports:
      - port: 80
     

$ kubectl create -f  definitions/external-cluster-endpoints.yaml 


$ cat definitions/external-cluster-service.yaml 

  apiVersion: v1
  kind: Service
  metadata:
    name: external-service
  spec:
    ports:
    - port: 80

$ kubectl create -f definitions/external-cluster-service.yaml

** NOTE:  The IP address for the external-services endpoints is Google and Amazon.
          This is probably not the best of examples... but good enough for testing.


- List the serivces and the associated endpoints and not the ClientIP assigned to the serivce:

$ kubectl get services

NAME               TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
external-service   ClusterIP   10.98.228.33   <none>        80/TCP    91s


$ kubectl get endpoints

NAME               ENDPOINTS                             AGE
external-service   172.217.2.110:80,205.251.242.103:80   108s


- Next, delete and recreate the client Pod so the above serivce will be available 
  as an environment variable:

$ kubectl delete pod client
$ kubectl run client --image blgreco72dev/kube-tools

- Next execute a bash session within the Container running within the client Pod:

$ kubectl exec -it client --bash

- List the environment variables from the bash command line:
  # env

    EXTERNAL_SERVICE_PORT_80_TCP=tcp://10.98.228.33:80
    EXTERNAL_SERVICE_SERVICE_HOST=10.98.228.33

- Run the following command several time and since HTTPS is not being used a HTTP 301
  will be returned from both Google and Amazon randomaly as one on the IPs are choosen.
  Of course, this example does not really make since given that multple IP addresses
  should point to different enpoints providing the same service!!!

  # curl 10.98.228.33


######################################################################################
### Labels
######################################################################################

- Labels are used to tag resources (such as pods) so they can be categorized 
  and referenced as a set.

- Labels allows a group of related pods to be operated on as a whole Instead
  of individually.

- They are key/value parts associated with a resource.  The other resources,
  can reference all tagged resources using a label selector.

$ cat definitions/labeled-pod-definition.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: labeled-kube-service
  labels:
    app:  accounting
    area: invoices
    env: prod
spec:
  containers:
  - image: blgreco72dev/kube-service
    name: kube-service
    ports:
    - containerPort: 6200
      protocol: TCP

$ kubectl create -f definitions/labeled-pod-definition.yaml 

- The following can be used to view a pod's labels:

$ kubectl get pods --show-labels

NAME                  READY   STATUS    RESTARTS   AGE   LABELS
client                1/1     Running   2          29h   run=client
labled-kube-service   1/1     Running   0          58s   app=accounting,area=invoices,env=prod

- The following will update and existing label.

$ kubectl label pod labled-kube-service env=test --overwrite
$ kubectl get pods --show-labels

NAME                  READY   STATUS    RESTARTS   AGE    LABELS
client                1/1     Running   2          29h    run=client
labled-kube-service   1/1     Running   0          2m1s   app=accounting,area=invoices,env=test

- The following will add an new take.

$ kubectl label pod labled-kube-service version=2.0.4 
$ kubectl get pods --show-labels

NAME                  READY   STATUS    RESTARTS   AGE     LABELS
client                1/1     Running   2          29h     run=client
labled-kube-service   1/1     Running   0          3m54s   app=accounting,area=invoices,env=test,version=2.0.4

- Resources can be filtered based on their assigned labels:

$ kubectl get pods -l env=test 

NAME                  READY   STATUS    RESTARTS   AGE
labled-kube-service   1/1     Running   0          4m26s

- Multiple selectors can be specified within a filter by separating each by a comma.

Node Selection Using Labels
--------------------------------

- This should only be used when needed.  Kubernetes should be in charge of determining
  the node on which a pod should be executed.

- While an exact node cannot be specified for running a pod, the characteristics of the
  node (specified by labels) can be used to select a pod.

- The following will display all Node labels:

$  kubectl get nodes --show-labels

NAME      STATUS   ROLES    AGE     VERSION   LABELS
m-node1   Ready    master   2d21h   v1.19.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=m-node1,kubernetes.io/os=linux,node-role.kubernetes.io/master=
w-node1   Ready    <none>   2d20h   v1.19.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=w-node1,kubernetes.io/os=linux
w-node2   Ready    <none>   2d17h   v1.19.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=w-node2,kubernetes.io/os=linux
w-node3   Ready    <none>   2d17h   v1.19.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=w-node3,kubernetes.io/os=linux


- When the Kubernetes Cluster is built, some default labels are added.
- The following is used to add a label to a node:

$ kubectl get nodes

NAME       STATUS   ROLES    AGE   VERSION
minikube   Ready    master   24h   v1.18.3

$ kubectl label node minikube uat=true

- Labels can be used to filter matching nodes:

$ kubectl get nodes -l kubernetes.io/arch=amd64

- Within a Pod's resource definition file, the characteristics of the node to deploy the
  pod can be specified within the "spec" section with the property named "nodeSelector".


######################################################################################
### Resource Annotations
######################################################################################

- Annotations are key/value pairs similar to labels but are not used to select resources.
- There are are a from of metadata.

- Within a resource definition file, Annotations are specified using the "annotations" property
  defined within the "metadata" section.

- Annotations can also be added after the resource is created:

$ kubectl annotate pod labled-kube-service somecompany.com/build="144"

- A pod's annotations can be seen as follows:

$ kubectl describe pod labled-kube-service 

Annotations:  cni.projectcalico.org/podIP: 192.168.129.222/32
              cni.projectcalico.org/podIPs: 192.168.129.222/32
              somecompany.com/build: 144
Status:       Running
IP:           192.168.129.222
IPs:
  IP:  192.168.129.222


######################################################################################
### Namespaces
######################################################################################

- Lables can be used to organize and classify Pods.  However, labels will often be used
  across several pods that might might otherwise not be related.  

- Namespaces can be used to group a set of pods.
- Pods within a Namespace can be operated on at the sametime.
- Resource names only need to be unique within a namespace.

- The following will list all defined namespaces:

  $ kubectl get namespaces


- If a namespace is not specified when operating on pods, they are always scoped to the 
  "default" namespace.  For example, when listing Pods without specifying a namespace,
  only the pods within the default namespace will be listed.

- The following can be used to list Pods scoped to a specific namespace:

NAME                                        READY   STATUS      RESTARTS   AGE
ingress-nginx-admission-create-q67mx        0/1     Completed   0          5d9h
ingress-nginx-admission-patch-2hhw8         0/1     Completed   0          5d9h
ingress-nginx-controller-84cb46fccd-84pgt   1/1     Running     9          3d7h


- Like other resources, namespace can be created at the command line or by using an YAML definition file.

- The following will create a new namespace using kubeclt:

  $ kubectl create namespace application-one
  $ kubectl get namespaces

  NAME              STATUS   AGE
  application-one   Active   34s
  calico-system     Active   9d

$ cat definitions/namespaced-pod.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: namespaced-kube-service
  labels:
    app:  accounting
    area: invoices
    env: prod
  namespace: application-one
spec:
  containers:
  - image: blgreco72dev/kube-service
    name: kube-service
    ports:
    - containerPort: 6200
      protocol: TCP

$ kubectl create -f  definitions/namespaced-pod.yaml 
$ kubectl get pods -n application-one

NAME                      READY   STATUS    RESTARTS   AGE
namespaced-kube-service   1/1     Running   0          48s

- Namespaces (these are not Linux namespace) do not provide any type of Pod isolation.  
  A Pod running in one namespace can still communicate with a pod in another.

----------------------------------------------------------------------------------------

- Labels and Namespaces can be used to apply an operation to multiple resources at the same time.
- The following will delete all nodes defining a label with a specific value:

$ kubectl delete pods -l area=invoices

- Not that the following deleted any pods within the "default" namespace with the specified label.
- The Pod within the "application-one" namespace was not deleted even though it also has the same
  label.

- Also, all Pods contained within a namespace can be deleted by deleting the namespace.  This will
  also delete the namespace.

$ kubectl delete namespace application-one

- If you want to keep the namespace and just want to delete all Pods within the namespace, 
  the following command can be used:

$ kubectl delete pods -n application-one --all

- The above command will delete all resources contained within the "application-one" namespace.

The current namespace to which all commands will apply can be set as follows:

$ kubectl config set-context --current --namespace=application-one

- The following command can be used to view the current context:

$ kubectl config current-context


######################################################################################
### Liveness Probes
######################################################################################

- Called by Kubernetes Controllers to determine if the service-api running within
  a pod is considered to be healthy.  

- If determined that the service-api is no longer healthy, Kubernetes will recreate 
  another Pod.  If there is not a Liveness Probe, the only time the Pod would be
  recreated would be if there was a resource issue on the Pod's current node or if
  the Node goes offline.

- $ cat definitions/liveness-prob-definition.yaml 


apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: kube-service-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kube-srv
  template:
    metadata:
      labels:
        app: kube-srv
    spec:
      containers:
      - name: kube-srv-container
        image: blgreco72dev/kube-service
        livenessProbe:
          httpGet:
            path: /api/pod/health-check
            port: 6200
        ports:
        - containerPort: 6200

$ kubectl create -f definitions/liveness-prob-definition.yaml 

- The above specifies that the route /api/pod/health-check should be called to determine
  if the pod is healthy.  

$ kube get pods -o wide

NAME                    READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES
client                  1/1     Running   5          31h   192.168.138.5     w-node3   <none>           <none>
kube-service-rs-fmdvh   1/1     Running   0          9s    192.168.129.227   w-node1   <none>           <none>
kube-service-rs-hw5gc   1/1     Running   0          9s    192.168.138.10    w-node3   <none>           <none>
kube-service-rs-p46zc   1/1     Running   0          9s    192.168.144.84    w-node2   <none>           <none>


- Instead of creating a Serivce, Port Forwarding will be used:

  $ kubectl port-forward kube-service-rs-hw5gc  8888:6200

- Within a another terminal widow, run the following:

  $ watch kubectl get pods

- Next, in a sparate window, call the above heath-check endpoint to test that the call works:

curl -v http://localhost:8888/api/pod/health-check 

- The healh-check API will return a HTTP 200 if it is health and a 404 if not.
- The service also provides another route that can be called to toggle the current state 
  of its health.

$ curl -X POST -v -d "" http://localhost:8888/api/pod/toggle-health

- The the above POST is executed, you will see the number of Restarts increase after a short
  period of time.  After the pod is restarted, the the health status will reference back to being healthy.

NAME                    READY   STATUS    RESTARTS   AGE
client                  1/1     Running   5          32h
kube-service-rs-fmdvh   1/1     Running   0          15m
kube-service-rs-hw5gc   1/1     Running   3          15m
kube-service-rs-p46zc   1/1     Running   0          15m

- The above shows the results after toggling the health status 3 time.  Also note that the name of the
  pod is still the same since Kubernetes is recreating the container running within the Pod.  The kubelet
  process running on each Node is who calls the Probe and not the Kubernetes Control Plane runnin on the
  Master Nodes.

- Running the following command will show within the Events listing that the Pod was unhealthy:

$ kubectl describe pod kube-service-rs-hw5gc


Liveness:       http-get http://:6200/api/pod/health-check delay=0s timeout=1s period=10s #success=1 #failure=3

...

Events:
  Type     Reason     Age                  From               Message
  ----     ------     ----                 ----               -------
  Normal   Scheduled  21m                  default-scheduler  Successfully assigned default/kube-service-rs-hw5gc to w-node3
  Normal   Pulled     21m                  kubelet, w-node3   Successfully pulled image "blgreco72dev/kube-service" in 483.360985ms
  Normal   Pulled     12m                  kubelet, w-node3   Successfully pulled image "blgreco72dev/kube-service" in 545.058789ms
  Normal   Pulled     8m3s                 kubelet, w-node3   Successfully pulled image "blgreco72dev/kube-service" in 529.053588ms
  Warning  Unhealthy  6m24s (x9 over 13m)  kubelet, w-node3   Liveness probe failed: HTTP probe failed with statuscode: 404
  Normal   Killing    6m24s (x3 over 12m)  kubelet, w-node3   Container kube-srv-container failed liveness probe, will be restarted
  Normal   Pulling    6m24s (x4 over 21m)  kubelet, w-node3   Pulling image "blgreco72dev/kube-service"
  Normal   Created    6m23s (x4 over 21m)  kubelet, w-node3   Created container kube-srv-container
  Normal   Started    6m23s (x4 over 21m)  kubelet, w-node3   Started container kube-srv-container
  Normal   Pulled     6m23s                kubelet, w-node3   Successfully pulled image "blgreco72dev/kube-service" in 488.67478ms


  - Also included within the describe results is additional information about the probe:
      - delay: the probe will be called as soon as the container is ready.
      - timeout:  the probe must return a response within 1 second.
      - period: the prove is called every 10 seconds.

  - These parameters can also be defined when defining the liveness probe for a Pod.

  *** You should always specify an inital delay so that the Container running the serivce-api has enough time to start 
      so it can respond to health-check calls.  If the service-api cannot repond, then the Pod will be considered unhealthy
      after the number of failures is greather than a given number of trys.

  - The health endpoint should not require authentication.

  - The implemention of the health-check should base the outcome on its current state and not the state of other service-apis 
    it calls.  If a service-api cannot be contacted on which it is dependent, that other serivce's Pod should indicate that it
    is not healthy so its Container is recreated.

  - Probes should execute quickly.
 
 -------------------------------------------------------------------------------------------------------

######################################################################################
### Ready Checks
######################################################################################

- As soon as a pod is crated, Kubernetes assumes it is ready to receive connections.
- However, this might not alway be the case.
- A pod can signal when it is ready.
- This is mostly needed during container service-api startup.

######################################################################################
### DaemonSet
######################################################################################

- Allows running a specific Pod on all Nodes belonging to the cluster.

- If a new Node is added to the Cluster, the DaemonSet will automatically create a new Pod 
  instance on that node.

- Also, a DaemonSet can be defined to run a Pod on a set of Nodes meeting a specific criteria.

- The following is a definition for a DaemonSet:

$ cat definitions/deamonset-definition.yaml 

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: daemon-kube-service
spec:
  selector:
    matchLabels:
      app: service-api
  template:
    metadata:
      labels:
        app: service-api
    spec:
      containers:
      - name: main
        image: blgreco72dev/kube-service

$ kubectl create -f definitions/deamonset-definition.yaml 

- At this point, zero Pods will have been created.  Currently there are no nodes
  labeled with drive: ssd

$ kubectl get pods

No resources found in default namespace.

$ kubectl get nodes

NAME      STATUS   ROLES    AGE     VERSION
m-node1   Ready    master   2d22h   v1.19.2
w-node1   Ready    <none>   2d21h   v1.19.2
w-node2   Ready    <none>   2d18h   v1.19.2
w-node3   Ready    <none>   2d17h   v1.19.2

- Next label some nodes with the needed label:

$ kubectl label node w-node2 disk=ssd
$ kubectl label node w-node3 disk=ssd

$ kubectl get pods -o wide

NAME                READY   STATUS    RESTARTS   AGE   IP               NODE      NOMINATED NODE   READINESS GATES
daemon-test-lbncm   1/1     Running   0          11s   192.168.138.15   w-node3   <none>           <none>
daemon-test-rclsc   1/1     Running   0          26s   192.168.144.79   w-node2   <none>           <none>

$ kubectl get daemonset

NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemon-test   2         2         2       2            2           disk=ssd        4m37s

- The following then can be used to delete all daemons:

$ kubectl delete daemonset daemon-test

$ kubectl get pods

NAME                READY   STATUS        RESTARTS   AGE
daemon-test-lbncm   1/1     Terminating   0          2m51s
daemon-test-rclsc   1/1     Terminating   0          3m6s


######################################################################################
### Jobs
######################################################################################

- The other controller types create a number of Pods and makes sure they
  are always running to match the number of specified replica.

- If a node goes down when a Job is running, a Pod will be created on another node to continue
  until the job successfully completes.

- A Pod for a Job does not run indefinitely so the restartPolicy must be specified as OnFailure or Never.

apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
      - name: main
        image: luksa/batch-job


- After the job completes, the Pod will remain.  This allows you to check the logs if needed.
- However, the Pod will be deleted if the Job that created it is deleted.

- Jobs can be specified to create more than one job instance and they can be either executed
  sequentially or in parallel.


$ kubectl get pods

NAME                READY   STATUS    RESTARTS   AGE
batch-job-tkcl6     1/1     Running   0          2m55s

NAME                READY   STATUS      RESTARTS   AGE
batch-job-tkcl6     0/1     Completed   0          2m35s


----------------------------------------
CronJob
----------------------------------------
- These are similar to Jobs but they can be scheduled to run on an interval 
  and at a specific time in the future.


######################################################################################
### Volumes
######################################################################################

- Each Container within a Pod has an isolated file system based on the image from which it was created.
- Only the Container can write it its one file system.  When the Container is recreated, any local
  change are lost.

- Volumes allow Container to persist data between restarts.  Volumes have the same lifecycle as
  Pods and are created and deleted at the same time the Pod is.  However, based on the type of
  Volume, their lifecycle can extend that of the Pod.

- Volumes and declared within a Pod's template definition.

- Volumes can be accessed and shared by all Containers running within the same Pod.
- Volumes are mounted to local directories within the container.
- By mounting the same Volume between Containers, they can access the same data.

- If a volume start out empty and not initialized from existing files, then the "emptyDir"
  volume type can be used.  

- There are the other common types:
    - hostPath: Allows mapping a volume to an existing directory located within a directory on the worker node.
    - gitRepo: When a Pod is created, it can have a volume based on pulling files from GitHub.
    - gcePersistentDisk: Cloud provider storage
    - configMap, secret, downloadAPI - Volumes defined to access information defined within Kubernetes.
    - persistentVolumeClaim - Allows the use of pre-provisioned persistent storage.

-----------------------------------------------------------------------------------------

- The example Microservice will be extended to write files to a Volume.

$ cat definitions/volume-emptyfile.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: kube-service
spec:
  containers:
  - image: blgreco72dev/kube-service
    name: kube-srv
    volumeMounts:
    - name: received-data
      mountPath: /data/files
    ports:
    - containerPort: 6200
      protocol: TCP
  volumes:
  - name: received-data
    emptyDir: {}

$ kubectl create -f definitions/volume-emptyfile.yaml 
$ kubectl port-forward kube-service  8888:6200

- Next, requests will be made that will write data to files contained within the volume and tested
  by making another call returing an array of the contents for all files.

$ curl -X POST -v  -d "" http://localhost:8888/api/volume/add-file?value=test-data1
$ curl -X POST -v  -d "" http://localhost:8888/api/volume/add-file?value=test-data2
$ curl -X POST -v  -d "" http://localhost:8888/api/volume/add-file?value=test-data3

$ curl  http://localhost:8888/api/volume/file-data

["test-data1","test-data3","test-data2","Some"]

- Next, restart the Node on which the Pod is located and validate that the data still exits.

$ kubectl get pods -o wide

NAME           READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
client         1/1     Running   4          17h     192.168.138.19    w-node3   <none>           <none>
kube-service   1/1     Running   0          6m26s   192.168.129.234   w-node1   <none>           <none>

- In this case, restart w-node1.  After the node is fully started, check that the data still exists:

$ kubectl port-forward kube-service  8888:6200    (Note: stop the current running port-forward and re-execute)
$ curl  http://localhost:8888/api/volume/file-data

["test-data1","test-data3","test-data2","Some"]

---------------------------------------------------------------------------------------------

- Next will will dig deep to see where the data is stored...
- Run the following to get the ID of the container

$ kubectl get pods -o custom-columns=PodName:.metadata.name,PodUID:.metadata.uid

PodName        PodUID
client         75cc3336-b8c2-4414-9dee-36cba2584978
kube-service   eb9729eb-0a71-4842-a3c5-6f118e82486e

- Since the Pod is running on w-node1, ssh into this worker node and run as the root:

$ ssh w-node1@192.168.56.5
    sudo -1

- Then navigate to the following directory based on the ID of the Pod:

$ cd /var/lib/kubelet/pods/eb9729eb-0a71-4842-a3c5-6f118e82486e/volumes/kubernetes.io~empty-dir/received-data
$ ls

    tmp9x31zW.tmp  tmpMenmZL.tmp  tmpNjRHoj.tmp  tmpVuOJaT.tmp

$ cat tmp9x31zW.tmp 
    test-data2

---------------------------------------------------------------------------------------------

- For better performance, a volume can be configured to write to memory:

$ cat definitions/volume-memory.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: kube-service
spec:
  containers:
  - image: blgreco72dev/kube-service
    name: kube-srv
    volumeMounts:
    - name: received-data
      mountPath: /data/files
    ports:
    - containerPort: 6200
      protocol: TCP
  volumes:
  - name: received-data
    emptyDir:
      medium: Memory


$ kubectl create -f definitions/volume-memory.yaml 
$ kubectl port-forward kube-service  8888:6200    (Note: stop the current running port-forward and re-execute)

$ curl -X POST -v  -d "" http://localhost:8888/api/volume/add-file?value=test-data1
$ curl -X POST -v  -d "" http://localhost:8888/api/volume/add-file?value=test-data2

$ curl http://localhost:8888/api/volume/file-data

$ kubectl delete pod kube-service

------------------------------------------------------------------------------------------------------------------

- The gitRepo volumn type can be used to clone a Git repository.

$ cat definitions/volume-git.yml 

apiVersion: v1
kind: Pod
metadata:
  name: kube-service
spec:
  containers:
  - image: blgreco72dev/kube-service
    name: kube-srv
    volumeMounts:
    - name: repo-data
      mountPath: /data/git/repo
    ports:
    - containerPort: 6200
      protocol: TCP
  volumes:
  - name: repo-data
    gitRepo:
      repository: https://github.com/grecosoft/dev-blog.git
      revision: master
      directory: .

$ kubectl create -f definitions/volume-git.yml 
$ kubectl port-forward kube-service  8888:6200    (Note: stop the current running port-forward and re-execute)

$ curl http://localhost:8888/api/volume/git-repo?file=kube-data.dat

     ["This file was loaded from a volumn cloned from Git!!!!"]

$ kubectl delete pod kube-service

-----------------------------------------------------------------------------------------

- The hostPath Volume type references a directory located on the Node that the Pod is running.
- The contents of the Volumn will be available between Pod creations.

- However, this type type of Volume should really only be used for reading static data stored
  on the Host - for example log files or system configurations.  

- Since a Pod with state could save data to a file on one Node but then be created on another 
  Node where the data would not be present.

$ cat definitions/volume-host.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: kube-service
spec:
  containers:
  - image: blgreco72dev/kube-service
    name: kube-srv
    volumeMounts:
    - name: host-data
      mountPath: /host/settings
    ports:
    - containerPort: 6200
      protocol: TCP
  nodeSelector:
    kubernetes.io/hostname: w-node3    
  volumes:
  - name: host-data
    hostPath:
      path: /home/w-node3/settings

$ kube create -f definitions/volume-host.yaml 
$ kubectl port-forward kube-service 8888:6200

- Note that the above is specifying that this Pod should run on w-node3 so we can assure
  the data will be present for this example.

- ssh into w-node3 and complete the following:
  $ ssh w-node3@192.168.56.7
  $ mkdir settings
  $ cd settings
  $ touch keyvalue.dat
  $ nano keyvalue.dat

  - Enter and save the following:

  a=100
  b=99
  c=18
  d=11

- Make calls to the serivce-api to test that the host data can be read:

$ curl -v http://localhost:8888/api/volume/host-data?file=keyvalue.dat
    ["a=100","b=99","c=18","d=11"]

- If you delte and re-create the Pod, the data will still be available.


######################################################################################
### Persistent Storage
######################################################################################

- Enables data to be saved that can be accessed by a Pod's container regardless of the node on which it is running.
- Network storage can be used and all of the Cloud providers have such a service.
- This allows storing the data external to the Kubernetes Node.
- You can also mount network file-shares.


######################################################################################
### PersistentVolumes and PersistentVolumeClaims
######################################################################################

- Allows volumes to be created without having the Pod definition bound to a specific implementation.

- This externalizes the volume configuration so the same definition can be used on another Kubernetes
  Cloud provider where the volume implementation will be different.

  - This works by specifing a PersistentVolumeClaim indicating the characteristics of the needed storage.  
    The claim is submitted to the Kubernetes REST api and it responds by finding a Persistent Volume meeting
    the needs of the claim.  The claim is then bound to the volume. The the PersistentVolumeClaim can be used 
    as a volume within the Pod.
  - The PersistentVolumeClaim cannot be used by other Pods until the assocated PersistentVolumeClaim 
    association to the PersistentVolume is deleted.

  - Some of the volume characteristics expressed in the Claim are:
      - Nedded SIZE
      - Read/Write Needs


######################################################################################
### Application Configuration
######################################################################################   
    
- Configuration files should never be part of a Docker image from which Containers are created.
- Configuration files usually contain environment specific settings and if the settings are 
  stored within a file, then a different image would need to be create per environment.

- Kubernetes provides a Resource type called a ConfigMap used to store configuration settings
  external to the container created from an image.

- The Dockerfile from which containers are creates specific the following:
    ENTRYPOINT: Specifies the executable to be ran when the container is started.
    CMD: Specifies the arguments passed to ENTRYPOINT.

- If you have a node application for example, it is best to use what is called the "exec" from:
    ENTRYPOINT ["node", "app.js"]

  This will run the application directly and not via the shell as the following would:
    ENTRYPOINT node app.js

- When defining a Pod, the command arguments can be overriden.  Also, not as common, the 
  executable ENTRYPOINT specified can also be overriden. 


---------------------------------------------------------------------------------------------
Command Line Arguments
---------------------------------------------------------------------------------------------

$ cat definitions/config-cmd-args.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: kube-service
spec:
  containers:
  - image: blgreco72dev/kube-service
    name: kube-srv
    args: ["100", "M", "YES"]
    ports:
    - containerPort: 6200
      protocol: TCP

$ kubectl create -f definitions/config-cmd-args.yaml  
$ kubectl port-forward kube-service 8888:6200

$ curl http://localhost:8888/api/config/command-args

    ["/microservice/Kube.Service.WebApi.dll","100","M","YES"]

  *** Strings don't need to be enclosed inquotation marks, but numbers must be.
  *** This sounds backwards... but ok.


---------------------------------------------------------------------------------------------
Envrionment Variables
---------------------------------------------------------------------------------------------



######################################################################################
### Deployments
######################################################################################








[HERE][HERE][HERE][HERE][HERE][HERE][HERE][HERE][HERE][HERE][HERE]



TODO:

######################################################################################
### Additional Kubernetes Cluster/Controllers/Services/Pods Notes
######################################################################################

- The role of the ReplicationController is to replicate pods and to make sure they
  remain running.  ReplicationController resource are not created very often are 
  a ReplicaSet (a type of ReplicationController) is commonly used.  When the deployment
  above was created, it created an associated ReplicaSet.

- If a pod crashes or the node is stopped, this is detected by the ReplicaSet which
  schedules another pod with the Scheduler to be created.  The Schedule determines 
  the best worker node for the pod.  The Kubelet process running on the node is called
  by the scheduler.  The Kubelet process then pulls the image and then creates the container
  and associated pod. 

- When the prod is relocated to a new node, the newly created pod will have a new IP address.
- The IP address of the LoadBalancer Service is static (for the life of the service).  The 
  LoadBalancer Service keeps track of the nodes where the exposed pods are running and forwards
  any requests to one of the nodes where the pod is running.

* Services represent a static location for a group of one or more pods that all provide 
  the same service.


















???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????


######################################################################################
### Connecting to Services outside of Cluster
######################################################################################

- This is for the case where a service-api, running within the cluster, needs to
  call a service-api residing outside of the cluster.  Not only can Service resources
  be created for service-apis running within the cluster but for also service-apis
  that are hosted outside the cluster.

- When a service is created for a specific service-api running within a Pod's container,
  the service provides a single static IP and port for a single point of entry.  Services
  do not directly point to a Pod but point indirectly via the Endpoints resources.

- The Endpoints resource contains a list if IP/ports for Pods's containing a specific service-api.
- The Service resource then routes a request to its static IP/Port to one of the defined endpoints.

- These endpoints can also be viewed:

$ kubectl get endpoints

NAME         ENDPOINTS                                            AGE
kubernetes   172.17.0.3:8443                                      15h
kubia-srv    172.18.0.10:8080,172.18.0.11:8080,172.18.0.12:8080   15h

- Since Service Resources are not directly tied to Pod Resources but indirectly by Endpoint Resources,
  Endpoints (to external service-apis) can be manually configured and associated with a Service Resource.

- YAML files can be created to manually define the Service and Endpoint Resources.

- The following defines a service that can be called on port 80:

apiVersion: v1
kind: Service
metadata:
  name: external-services
spec:
  ports:
  - port: 80

$ kubectl create -f service-manual.yaml 

$ kubectl get services

NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
external-services   ClusterIP   10.101.84.49     <none>        80/TCP    39s
kubernetes          ClusterIP   10.96.0.1        <none>        443/TCP   15h
kubia-srv           ClusterIP   10.103.158.206   <none>        80/TCP    15h

- Since the above service does not have a Pod selector, no Endpoint Resources are created and
  therefore has to be done manually.


apiVersion: v1
kind: Endpoints
metadata:
  name: external-services
subsets:
  - addresses:
    - ip: 199.16.172.225
    - ip: 172.217.13.228
    ports:
    - port: 80

$ kubectl get services

NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
external-services   ClusterIP   10.97.41.239     <none>        80/TCP    7m46s

- Now from a container contained within a Pod within the cluster, a request can be made
  to IP 10.97.41.239 and the request will be forwarded to one of the endpoint entries.

######################################################################################
### Access Api from outside of the cluster
######################################################################################

https://minikube.sigs.k8s.io/docs/handbook/accessing/#using-minikube-tunnel

- The following will create a deployment and expose the service running within
the container of the Kubernetes pod so it can be accessed from outside the 
cluster.

- Run in command console:
-----------------------------------
$ minikube start
$ minikube dashboard

- Run the following in another command console:
-----------------------------------
$ minikube tunnel

- Run the following in another command console:
$ kubectl create deployment kubia-deploy --image blgreco72dev/kubia

- Run the following to view details about the deployment and created resources:
-----------------------------------
$ kubectl get deployments
$ kubectl describe deployments kubia-deploy
$ kubectl get pods
$ kubectl describe pods kubia

- Run the following to expose the pod created for the container within the deployment:
-----------------------------------
$ kubectl expose deployment kubia-deploy --type=LoadBalancer --port=8080

- The above is telling Kubernetes to expose all pods created from the named deployment.

$ kubectl get service


NAME           TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)          AGE
kubernetes     ClusterIP      10.96.0.1      <none>         443/TCP          13m
kubia-deploy   LoadBalancer   10.98.69.127   10.98.69.127   8080:32214/TCP   12m

- Note the IP that has been assigned to External-IP for the service.

$ curl 10.98.69.127:8080


######################################################################################
### Expecting the Node
######################################################################################

- The minikube node has an ip address of:  172.17.0.3 as can be found
  by running kubectl describe node minikube.  This is the IP address of
  VM created running the single Kubernetes node.

- The following will shh into the node and expect its state:

$ minikube ssh

- Once in the node, run the following to install curl command line:
sudo apt-get update
sudo apt-get install curl

- Run the following to see the IP address of the VM:
$ ip addr show

eth0@if55: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.3/16 brd 172.17.255.255 scope global eth0

- The IP address will match what was reported by kubectl describe minikube

- Exit the ssh session.
$ exit

- Next, run the following to get the details of the pod running within the cluster:

$ kubectl describe pod kubia

Namespace:    default
Priority:     0
Node:         minikube/172.17.0.3
Start Time:   Wed, 02 Sep 2020 11:57:59 -0400
Labels:       app=kubia-deploy
              pod-template-hash=5d9f894958
Annotations:  <none>
Status:       Running
IP:           172.18.0.3

The IP address of the pod is 172.18.0.3

- ssh into the node again:
$ minikube ssh 

- Next, execute the following command to against the IP address of
  the pod to call the service (api) running within the pod from a 
  bash session within the node containing the pod:

curl 172.18.0.3:8080

- So the above is accessing the Pod from the bash command line on the node
  that was ssh into.  The above IP address 172.18.0.3 is specific to the node
  and can only be accessed from the node that was just ssh into.

- The following are the details for the service created to expose the container
  running from within the Pod:

NAME           TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)          AGE
kubia-deploy   LoadBalancer   10.98.69.127   10.98.69.127   8080:32214/TCP   12m


- The Cluster-IP can be used to access the Pod from another node within the cluster.
- Minikube is only a one node cluster, but still logged into the node, this IP address
  can also be used to call the service running within the Pods container:

curl 10.98.69.127:8080
  
- So a microservice based solution should be designed to carefully to only expose
  the services that should be allowed to be invoked from outside the cluster while
  other more internal services should only communicate on the internal custer-ip.

** Later a multi node cluster will be created using Google Cloud Services and more
   details will be provided.


minikube service kubia-deploy

|-----------|--------------|-------------|-------------------------|
| NAMESPACE |     NAME     | TARGET PORT |           URL           |
|-----------|--------------|-------------|-------------------------|
| default   | kubia-deploy |        8080 | http://172.17.0.3:32214 |
|-----------|--------------|-------------|-------------------------|

- 10.98.69.127:8080 (LoadBalancer)  -->   http://172.17.0.3:32214 (node) -->  172.18.0.3:8080(pod)

-  This is the ip and port exposed        - This is the ip and port exposed     - This is the ip and port
   by the load balancer.                    to allows the container service       of the internal pod.
                                            for access outside of the cluster

- These layers of indirection are needed since after exposing the pod to be called from outside the
  node, the pod can be moved around or replicated to several different nodes.  So clients should 
  never point to a service on a given node since it might be moved and therefore the ip and port
  would change.  The ip and port of the LoadBalancer is static and load balances requests across
  the nodes running the pod.

$ minikube stop                             
$ minikube delete

- The following outlines what is happening when the above deployment was created 
  using kubeclt from the command line:

1.  The kubectl commands sends request to the Kubernetes HTTP REST Api.
2.  A new Deployment is created within the cluster.
3.  The Deployment creates a Replica Set identifying how the pods should be created to satisfy deployment.
4.  The Replica Set coordinates the creation of a new Pod to run container by sending request to the Scheduler.
5.  The Scheduler determines the node on which the container should be created and sends request to the Kubelet 
    running on the node.
6.  The Worker node uses Docker to pull the image from a repository and creates a Docker Container.
7.  The Docker Container is associated with the Pod in which is executed.
8.  Next, a service is created to expose the api running within the pod's docker container.  


- However, this IP address is internal to the cluster can cannot be accessed from outside.
- To expose the service running within a pod's container, a service must be created.
- There are different types of services:  LoadBalancer and ClusterIP.  A ClusterIP exposes
  the pod's container service(api) within the custer and a LoadBalancer will expose the
  pod's container service(api) to outside the cluster.
- LoadBalancer services creates an external load balancer with an exposed IP address to 
  which external clients use to call a pod's container service(api).







