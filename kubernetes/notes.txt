######################################################################################
### Installing minikube
######################################################################################

https://phoenixnap.com/kb/install-minikube-on-ubuntu

######################################################################################
### Google Cloud Platform
######################################################################################

Google Kubernetes Engine (GKE)


######################################################################################
### Basic Container Commands
######################################################################################

- To view information about the Kubernetes cluster:

kubectl cluster-info   

- If using Minikube, you can ssh into the node as follows:
    - minikube ssh


- Each node in the cluster runs Docker, Kubelet, and the kube-proxy
The kubeclt command line sends HTTP REST requests to the Kubernetes API server running on master node.

- If you ssh into the node you can see these running processes:

    ps uax

- Each node runs Docker, Kubelet, and kube-proxy.


- To view all the nodes belonging to the cluster:

kubectl get nodes


- If using Minikube, there will be only one node.  This node acts as the Master and Worker node.
- To view more details about a an object, in this case a node, kubectl describe can be used

kubectl describe node minikube

######################################################################################
### Running Container in Kubernetes
######################################################################################

- It is a best practice to define an YAML or JSON file to describe the container to be executed.
- However, it can also be done at the command line for testing.

- The following will create a pod.  This is of very limited use since its execution is not belonging
  managed by a ReplicaSet. 

kubectl run kubia --image=blgreco72dev/kubia --port=8080 


- The above command will run the container within a Kubernetes pod.  A pod can contain multiple containers
  but most often only one container is contained within a pod.

- Each pod has its own IP address, host name, processes.
- You cannot list containers but the current pods can be listed.

kubectl get pods

The following command can be used to view details of a specific pod:

kubectl describe pod kubia

- Next, ssh into the node and complete the following:

minikube ssh

- List all the containers running on the node in Docker:

docker container ls

- The container for which the Pod was created will be listed.
- Next, view the logs of the container that was created:

docker container logs 16e1

- Next, view the images that are downloaded to the node:

docker image ls

- The image from which the container running in the pod will be listed.

- Next, attempt to stop the running container to see if we can break $hit.

docker container stop 16e1

- After a few moments, the container will have been recreated (not just restarted) by Kubernetes.
- Usually this monitoring is completed by a ReplicationController.  However, in the Web Client, there is no ReplicationController
  listed as being created.  This is how it worked in older versions of docker.

- Next, attempt to kill that bad boy by removing the container:

docker container rm -f f5

- Listing the containers shows that the container was created.

- Let really be jerks and remove the image and then remove the container.

docker image rm blgreco72dev/kubia -f
docker container rm 892 -f 

- If you list the images and containers, the image will have been re-pulled down and
  a new container will have been created.

- NOTE:  The Kubelet running on the node is responsible for making sure the container remains running in the pod.

- Exit the node's shell:

$ exit

- Running the following command will show that the pod was restarted twice.  So, when the container will stopped
  and removed above, Kubernetes recreates a new pod (not just the container).

$ kubectl get pods
$ kubectl describe pod kubia

In the events section, it shows the pod was created due to restarting a failed container.


- The following command will delete the pod.  Not that this will take a few seconds to complete:

kubectl delete pod kubia

######################################################################################
### Access Api from outside of the cluster
######################################################################################

https://minikube.sigs.k8s.io/docs/handbook/accessing/#using-minikube-tunnel

- The following will create a deployment and expose the service running within
the container of the Kubernetes pod so it can be accessed from outside the 
cluster.

- Run in command console:
-----------------------------------
$ minikube start
$ minikube dashboard

- Run the following in another command console:
-----------------------------------
$ minikube tunnel

- Run the following in another command console:
$ kubectl create deployment kubia-deploy --image blgreco72dev/kubia

- Run the following to view details about the deployment and created resources:
-----------------------------------
$ kubectl get deployments
$ kubectl describe deployments kubia-deploy
$ kubectl get pods
$ kubectl describe pods kubia

- Run the following to expose the pod created for the container within the deployment:
-----------------------------------
$ kubectl expose deployment kubia-deploy --type=LoadBalancer --port=8080

- The above is telling Kubernetes to expose all pods created from the named deployment.

$ kubectl get service


NAME           TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)          AGE
kubernetes     ClusterIP      10.96.0.1      <none>         443/TCP          13m
kubia-deploy   LoadBalancer   10.98.69.127   10.98.69.127   8080:32214/TCP   12m

- Note the IP that has been assigned to External-IP for the service.

$ curl 10.98.69.127:8080


######################################################################################
### Expecting the Node
######################################################################################

- The minikube node has an ip address of:  172.17.0.3 as can be found
  by running kubectl describe node minikube.  This is the IP address of
  VM created running the single Kubernetes node.

- The following will shh into the node and expect its state:

$ minikube ssh

- Once in the node, run the following to install curl command line:
sudo apt-get update
sudo apt-get install curl

- Run the following to see the IP address of the VM:
$ ip addr show

eth0@if55: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.3/16 brd 172.17.255.255 scope global eth0

- The IP address will match what was reported by kubectl describe minikube

- Exit the ssh session.
$ exit

- Next, run the following to get the details of the pod running within the cluster:

$ kubectl describe pod kubia

Namespace:    default
Priority:     0
Node:         minikube/172.17.0.3
Start Time:   Wed, 02 Sep 2020 11:57:59 -0400
Labels:       app=kubia-deploy
              pod-template-hash=5d9f894958
Annotations:  <none>
Status:       Running
IP:           172.18.0.3

The IP address of the pod is 172.18.0.3

- ssh into the node again:
$ minikube ssh 

- Next, execute the following command to against the IP address of
  the pod to call the service (api) running within the pod from a 
  bash session within the node containing the pod:

curl 172.18.0.3:8080

- So the above is accessing the Pod from the bash command line on the node
  that was ssh into.  The above IP address 172.18.0.3 is specific to the node
  and can only be accessed from the node that was just ssh into.

- The following are the details for the service created to expose the container
  running from within the Pod:

NAME           TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)          AGE
kubia-deploy   LoadBalancer   10.98.69.127   10.98.69.127   8080:32214/TCP   12m


- The Cluster-IP can be used to access the Pod from another node within the cluster.
- Minikube is only a one node cluster, but still logged into the node, this IP address
  can also be used to call the service running within the Pods container:

curl 10.98.69.127:8080
  
- So a microservice based solution should be designed to carefully to only expose
  the services that should be allowed to be invoked from outside the cluster while
  other more internal services should only communicate on the internal custer-ip.

** Later a multi node cluster will be created using Google Cloud Services and more
   details will be provided.


minikube service kubia-deploy

|-----------|--------------|-------------|-------------------------|
| NAMESPACE |     NAME     | TARGET PORT |           URL           |
|-----------|--------------|-------------|-------------------------|
| default   | kubia-deploy |        8080 | http://172.17.0.3:32214 |
|-----------|--------------|-------------|-------------------------|

- 10.98.69.127:8080 (LoadBalancer)  -->   http://172.17.0.3:32214 (node) -->  172.18.0.3:8080(pod)

-  This is the ip and port exposed        - This is the ip and port exposed     - This is the ip and port
   by the load balancer.                    to allows the container service       of the internal pod.
                                            for access outside of the cluster

- These layers of indirection are needed since after exposing the pod to be called from outside the
  node, the pod can be moved around or replicated to several different nodes.  So clients should 
  never point to a service on a given node since it might be moved and therefore the ip and port
  would change.  The ip and port of the LoadBalancer is static and load balances requests across
  the nodes running the pod.

$ minikube stop                             
$ minikube delete

- The following outlines what is happening when the above deployment was created 
  using kubeclt from the command line:

1.  The kubectl commands sends request to the Kubernetes HTTP REST Api.
2.  A new Deployment is created within the cluster.
3.  The Deployment creates a Replica Set identifying how the pods should be created to satisfy deployment.
4.  The Replica Set coordinates the creation of a new Pod to run container by sending request to the Scheduler.
5.  The Scheduler determines the node on which the container should be created and sends request to the Kubelet 
    running on the node.
6.  The Worker node uses Docker to pull the image from a repository and creates a Docker Container.
7.  The Docker Container is associated with the Pod in which is executed.
8.  Next, a service is created to expose the api running within the pod's docker container.  


- However, this IP address is internal to the cluster can cannot be accessed from outside.
- To expose the service running within a pod's container, a service must be created.
- There are different types of services:  LoadBalancer and ClusterIP.  A ClusterIP exposes
  the pod's container service(api) within the custer and a LoadBalancer will expose the
  pod's container service(api) to outside the cluster.
- LoadBalancer services creates an external load balancer with an exposed IP address to 
  which external clients use to call a pod's container service(api).

######################################################################################
### Additional Kubernetes Cluster/Controllers/Services/Pods Notes
######################################################################################

- The role of the ReplicationController is to replicate pods and to make sure they
  remain running.  ReplicationController resource are not created very often are 
  a ReplicaSet (a type of ReplicationController) is commonly used.  When the deployment
  above was created, it created an associated ReplicaSet.

- If a pod crashes or the node is stopped, this is detected by the ReplicaSet which
  schedules another pod with the Scheduler to be created.  The Schedule determines 
  the best worker node for the pod.  The Kubelet process running on the node is called
  by the scheduler.  The Kubelet process then pulls the image and then creates the container
  and associated pod. 

- When the prod is relocated to a new node, the newly created pod will have a new IP address.
- The IP address of the LoadBalancer Service is static (for the life of the service).  The 
LoadBalancer Service keeps track of the nodes the exposed pods are running and forwards any
requests to one of the nodes where the pod is running.

** Services represent a static location for a group of one or more pods that all provide the same service.

- The following will show the created ReplicaSet resources:
$ kubectl get replicasets

NAME                      DESIRED   CURRENT   READY   AGE
kubia-deploy-5d9f894958   1         1         1       4h11m

- Next, the ReplicaSet will be sent a request to have the number of running pods
  scaled-up from one to 3.  Since these examples thus far are using minikube, there
  is only one node so the following command can be executed:
  
$ kubectl scale --replicas=3 rs/kubia-deploy-5d9f894958 
  
  In the dashboard, you will see the number of pending nodes increase and two additions 
  pods listed, but since this is a single node cluster, the pods will not be replicated.  
  This is just a limitation of using minikube.  This will be revisited after setting up 
  Kubernetes as a cloud service.


- The following will list all the pods and the nodes to which the are running.  Knowing
  which node a pod is running on is usually no of interest.

 $ kubectl get pods -o wide

NAME                            READY   STATUS    RESTARTS   AGE     IP           NODE       NOMINATED NODE   READINESS GATES
kubia-deploy-5d9f894958-5hhrn   1/1     Running   0          4h29m   172.18.0.3   minikube   <none>           <none>


And the following can be used to get details for a specific Pod:

$ kubectl describe pod kubia-deploy-5d9f894958-5hhrn


######################################################################################
### Pod Details
######################################################################################

- Pods can contain multiple containers.  The containers within a pod will always run
  on the same worker node.

- This can be used if the containers must be located on the same machine.  This can be 
  the case for inter-process communication or if the processes running within each of
  the containers need to access a shared file.

- Pods allows containers to be bound together and manage then as a single unit.
- The containers within the pod can share certain resources and are not fully 
  isolated as when running within Docker.  Kubernetes does this by sharing the
  same set of Linux namespace between the two containers.  In Docker, each 
  container has its own set of namespaces.

- The containers within a Pod are all under the same network they share the same 
  host name and network interfaces.

- Each container running with in a Pod has a file systems isolated from each other.  
  However, the containers can share files by using volumes.

* Since the containers run within the same network namespace, they both have the
  the same IP address so ports must be unique across all containers.

- All Pods within a cluster are connected to the same software network and the
  containers can call one another by using each others IP address.  However, 
  this is not a good idea since Pods and be stopped, deleted, or moved and
  will be assigned a new IP Address.  

- All the containers comprising of a solution should not be all located within the
  same Pod.  Only very closely related containers should be located within the same
  Pod.

  - Since all containers within a Pod will be executed on the same worker machine,
    the will both consume resources on that node and can't be relocated to a node
    with additional resources.  

  - Another reason why two containers many not be best located with the same Pod
    is that it make scaling harder.  In Kubernetes scaling (replication) is at the
    Pod level.  So if the Pod contains multiple containers, then they must be scaled 
    together at the same time.

  - The main reason for having multiple containers in a Pod, is when one of the
    containers contains the core process and the other is more of a "side-car"
    process.  The side-car container execute code periodically (i.e.  check an API
    for updates to data consumed by the main container process.)


######################################################################################
### Creating Resources using YAML
######################################################################################

- The above created resources (deployment) by using the command line.

- This approach should not be used within production since the configuration 
  is not documented.Instead resources can be defined in YAML.

- If an existing resource exists within Kubernetes that was created at the
  command line, the YAML can be generated.  This is also useful for viewing
  a resources current configuration on the server.

- The following command can be used to obtain an YAML definition for any
  type of resource.  Below shows requesting YAML for the created Pod and
  deployment.  The YAML can be very verbose and contains properties with
  default values that would not typically be set.

$ kubectl get pod

NAME                            READY   STATUS    RESTARTS   AGE
kubia-deploy-5d9f894958-smwgs   1/1     Running   0          40m

$ kubectl get pod kubia-deploy-5d9f894958-smwgs -o yaml

kubectl get deployment

NAME           READY   UP-TO-DATE   AVAILABLE   AGE
kubia-deploy   1/1     1            1           41m

$ kubectl get deployment kubia-deploy -o yaml

- The YAML file consists of the following sections:
    - Metadata
    - Specification
    - Status

- When defining a YAML file for a Pod, the Ports on which the container listens
  can be specified.  This is just for documentation and has no bearing on how
  the container or pod is created.

- The following is going to delete the deployment resource that was created
  by running the two command line calls and will rebuild the same resources
  but defined within YAML.

$ kubectl delete deployment deployment kubia-deploy
$ kubectl delete service kubia-deploy


Creating the Pod using YAML
----------------------------------

apiVersion: v1
kind: Pod
metadata:
  name: kubia-pod
spec:
  containers:
  - image: blgreco72dev/kubia
    name: kubia
    ports:
    - containerPort: 8080
      protocol: TCP



- The following will submit the YAML
$ kubectl create -f pod-resource.yaml 

$ kubectl get pod

NAME        READY   STATUS    RESTARTS   AGE
kubia-pod   1/1     Running   0          101s

- Applications running within containers typically write log files to standard
out and standard error streams.

- The logs for a givin pod can be see by issuing the following command:
$ kubectl logs kubia-pod

- If the Pod contains multiple containers, you must specify the container's name
using the -c parameter.

- A Pod's logs are deleted when the Pod no longer exists.  If logs need to be
  persisted, they need to be written to a central error log store.

######################################################################################
### Port Forwarding
######################################################################################

- The newly created pod cannot be accessed from outside the cluster since a service
  has not yet been defined.  However, the Pod can be communicated with using port
  forwarding to test the Pod.

$ kubectl port-forward kubia-pod 8888:8080

- In another terminal window, type the following to communicate with the container
  running within the Pod.

$ curl localhost:8888


######################################################################################
### Labels
######################################################################################

- Labels are used to tag resources (such as pods) so they can be categorized 
  and referenced as a set.

- Labels allows a group of related pods to be operated on as a whole Instead
  of individually.

- They are key/value parts associated with a resource.  The other resources,
  can reference all tagged resources using a label selector.

apiVersion: v1
kind: Pod
metadata:
  name: kubia-pod-labels
  labels:
    app: receipts
    env: prod
spec:
  containers:
  - image: blgreco72dev/kubia
    name: kubia
    ports:
    - containerPort: 8080
      protocol: TCP

- The following can be used to view a pod's labels:

kubectl get pods --show-labels

- Labels can be added and updated on pods:

$ kubectl label pod kubia-pod-labels env=test --overwrite
$ kubectl label pod kubia-pod-labels version=2.0.4 
$ kubectl label pod kubia-pod env=test

- Resources can be filtered based on their assigned labels:

$ kubectl get pods -l env=test --show-labels

- Multiple selectors can be specified within a filter by separating each by a comma.

Node Selection Using Levels
--------------------------------

- This should only be used when needed.  Kubernetes should be in charge of determining
  the node on which a pod should be executed.

- While an exact node cannot be specified for running a pod, the characteristics of the
  node (specified by labels) can be used to select a pod.

- The following is used to add a label to a node:

$ kubectl get nodes

NAME       STATUS   ROLES    AGE   VERSION
minikube   Ready    master   24h   v1.18.3

$ kubectl label node minikube uat=true

- Labels can be used to filter matching nodes:

$ kubectl get nodes -l uat=true
$ kubectl get nodes -l prod=true

- Within a Pod's resource definition file, the characteristics of the node to deploy the
  pod can be specified within the "spec" section with the property named "nodeSelector".

######################################################################################
### Resource Annotations
######################################################################################

- Annotations are key/value pairs similar to labels but are not used to select resources.
- There are are a from of metadata.

- Within a resource definition file, Annotations are specified using the "annotations" property
  defined within the "metadata" section.

- Annotations can also be added after the resource is created:

$ kubectl annotate pod kubia-pod somecompany.com/build="144"

- A pod's annotations can be seen as follows:

$ kubectl describe pod kubia-pod


######################################################################################
### Namespaces
######################################################################################

- Namespaces are used to associate a name with a group of resources.

- The following will add another label to the kubia-prod-labels pod so both pods will have
  a set of labels with the same set of key/value pairs.

$ kubectl label pod kubia-pod app=receipt

- The following will be used to define a namespace to group all "test" environment "receipt" pods 
  from which a given solution is comprised.

- Use the next command to list all defined namespace within a cluster:

$ kubectl get namespaces

- If not otherwise specified, resources are associated with the default namespace.  Also, only
  resources with the default namespace are returned when listing resources if not specified.

- Namespaces can be created using yaml files or from the command line.

apiVersion: v1
kind: Namespace
metadata:
  name: receipt-solution


$ kubectl create -f pod-namespace.yml

$ kubectl create namespace payment-solution


- Namespaces can be specified within a resource definition file by adding a "namespace" property
  to the "metadata" section.  Or the --namespace argument can be specified when calling kubectl
  create when applying a yaml file.

- Name spaces do not provide any type of isolation.  For example, pods assigned to different 
  namespaces can still communicate within the cluster.

- Labels and Namespaces can be used to apply an operation to multiple resources at the same time.
- The following will delete all nodes defining a label with a specific value:

$ kubectl delete pods -l app=receipt

- Also, all Pods contained within a namespace can be deleted by deleting the namespace.  This will
  also delete the namespace.

$ kubectl delete namespace receipt-solution

- If you want to keep the namespace and just want to delete all Pods within the namespace, 
  the following command can be used:

$ kubectl delete pods --all

- The above command will delete all resources contained within the current namespace.

The current namespace to which all commands will apply can be set as follows:

$ kubectl config set-context --current --namespace=receipt-solution

- The following command can be used to view the current context:

$ kubectl config current-context


######################################################################################
### Replication and Controllers
######################################################################################

- Pods are usually not created directly.

- Instead, Replication Controllers and Deployments are used.  These two resource
  types monitor the Pods to which they are associated and will assure the correct
  number remain running on the cluster of notes.

- Controllers monitor pods and are responsible for keeping the needed number 
  running within the cluster.

- On each node, the Kubelet process monitors each running container and will make
  sure it remaining running.  If the Container's main process crashes, the Kubelet
  process will restart the container.

- However, a container can become unhealthy without the main process crashing so the
  container must provide a health-check that can be checked by Kubelet.

- These are called Liveness Probes and are specified within a Pod's specification.  
  These probes can be a HTTP GET method that is called or a command that is executed
  within the container.

- When a container is restarted, it is not the same container.  Instead a completely
  new container is created.

- An initial delay can be specified that will delay the first call to the Liveness 
  check.  This can be important if the container can't receive requests with first
  started since Kubelet will start calling the Liveness probe after the container
  is created.

- It is important that the Liveness check is based only only the state of the 
  container and not on any external ones.  If another container cannot be called,
  it is the other container that needs to be restarted and not the current.

- Kubelet = monitors containers to make sure they remain running.
- Controller = monitors Pods to make sure they remain running.

- Controllers usually monitor several replicas of a certain Pod type and assure 
  the number of running pods configured remain running.

- Pods are identified by labels and when creating a Controller, these labels are
  used to identify the Pods that the Controller should monitor and assure that
  the configured desired number matches the current runnings.

- If a Pods labels are changed, from what is specified within the Controller,
  the Pod is not stopped - it is just no longer managed by the controller.  If
  a label is added or changed that results in the Pod matching the Controller's
  selector, it will then be managed by the controller.

######################################################################################
### Creating Replication Controller
######################################################################################
  
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia-rc
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      name: kubia-pod
      labels:
        app: kubia
    spec:
      containers:      
      -  name: kubia-container
         image: blgreco72dev/kubia
         ports:
          - containerPort: 8080

$ kubectl create -f replication-controller.yaml 
$ kubectl get pods

NAME             READY   STATUS    RESTARTS   AGE
kubia-rc-24sr6   1/1     Running   0          7m24s
kubia-rc-ngrwm   1/1     Running   0          7m24s
kubia-rc-qjjqc   1/1     Running   0          7m24s

- Next, a Pod listed above will be deleted to validate a new one is created
  by the Replication Controller.

$ kubectl delete pod kubia-rc-24sr6

$ kubectl get pods

NAME             READY   STATUS    RESTARTS   AGE
kubia-rc-ngrwm   1/1     Running   0          11m
kubia-rc-qjjqc   1/1     Running   0          11m
kubia-rc-rngqv   1/1     Running   0          51s

- The following command can be used to list all defined ReplicationControllers:

$ kubectl get rc

NAME             READY   STATUS    RESTARTS   AGE
kubia-rc-ngrwm   1/1     Running   0          11m
kubia-rc-qjjqc   1/1     Running   0          11m
kubia-rc-rngqv   1/1     Running   0          51s

- The Describe command can be used to see ReplicationController details.
$ kubectl describe rc kubia-rc

- The following will show nodes on which the pods are running.  Since minikube
  only has one note, the names listed will all be the same.

$ kubectl get pods -o wide

- There is not a direct tie between Pods and ReplicationControllers.  The Pods associated
  with a controller are any Pods for which its labels match the controller's selector at
  any specific point in time.

- So if a Pods label no longer matches the controller's selector, the controller will 
  communicate with the schedule so another Pod is created on a node.

$ kubectl label pod kubia-rc-ngrwm app=something --overwrite=true

- After running the above command, another Pod will be created since prior pod that was
  a member of the replication controller no longer matches the selector.  The updated 
  pod however will still exist.  It is just no longer associated with the controller.

  $ kubectl get pods -L app

NAME             READY   STATUS    RESTARTS   AGE     APP
kubia-rc-d2vjh   1/1     Running   0          2m42s   kubia
kubia-rc-ngrwm   1/1     Running   0          37m     something
kubia-rc-qjjqc   1/1     Running   0          37m     kubia
kubia-rc-rngqv   1/1     Running   0          26m     kubia


- Next, the Pod with the renamed label will be changed back.

$ kubectl get pods -L app

NAME             READY   STATUS        RESTARTS   AGE     APP
kubia-rc-d2vjh   1/1     Terminating   0          5m35s   kubia
kubia-rc-ngrwm   1/1     Running       0          39m     kubia
kubia-rc-qjjqc   1/1     Running       0          39m     kubia
kubia-rc-rngqv   1/1     Running       0          29m     kubia

- The above shows that this resulted in having 4 nodes and not 3 as defined by the 
controller.  The controller detected this and issues a call to a node to terminate 
a running instance of the Pod.

- The Pod's template within a ReplicationController can be updated.  However, any 
  existing pods will not be affected.  To apply the update the the existing pods,
  they need to be first deleted so new ones will be created by the replication 
  controller.

The following will delete the replication controller and any nodes matching the selector:

$ kubectl delete -f replication-controller-updated.yaml

- A resource definition can be edited within the terminal as follows:
$ kubectl edit rc kubia-rc

** By default vi is used as the editor.  The following will update so nano editor can be used:

export KUBE_EDITOR="/usr/bin/nano"

- Next, an additional label will be added to the Pod's template.  After making the change, the
  pods and their labels will be listed.  Note that the addition label has not effect.  To update
  the nodes, they need to be deleted.  This will trigger the node to be recreated since the 
  current number no longer matches the desired value of 3.

- Add a label:   version: "3.0.4"

- Exit the editor and run the following:

$ kubectl get pods -L app -L  version

NAME             READY   STATUS    RESTARTS   AGE   APP     VERSION
kubia-rc-5q684   1/1     Running   0          12m   kubia   
kubia-rc-mfkkr   1/1     Running   0          12m   kubia   
kubia-rc-vzhbb   1/1     Running   0          12m   kubia 

- The results show that none of the running pods have the new label.

- Delete one of the Pods so it will be recreated by the controller:

 $ kubectl delete pod kubia-rc-vzhbb

- After the Pod is delete, re-execute the prior command again:

$ kubectl get pods -L app -L  version

NAME             READY   STATUS    RESTARTS   AGE   APP     VERSION
kubia-rc-5q684   1/1     Running   0          14m   kubia   
kubia-rc-dn6fr   1/1     Running   0          78s   kubia   3.0.4
kubia-rc-mfkkr   1/1     Running   0          14m   kubia 

- The above shows that the newly created Pod now has the new version.

- The following will delete all the nodes so they can be recrated:

$ kubectl delete pods -l app=kubia


 Horizontally scaling pods
----------------------------------

- The number of replicas can be changed for a Replication Controller.

kubectl scale rc kubia-rc --replicas=6

- The replication controller can also have it definition edited:

kubectl edit rc kubia-rc

- When deleting a ReplicationController, the associated Pods will also be deleted.
- But since the ReplicationController and Pods can exist independently, the Pods
  can remain and only the ReplicationController controller deleted.

$ kubectl delete rc kubia-rc --cascade=false

$ kubectl get pods

NAME             READY   STATUS    RESTARTS   AGE
kubia-rc-c5jwb   1/1     Running   0          18m
kubia-rc-m2rhj   1/1     Running   0          13m
kubia-rc-mp842   1/1     Running   0          18m
kubia-rc-tdqmv   1/1     Running   0          18m

- Now the pods are not being managed by any controller.
- This can be shown by deleting one of the Pods.  Since there is no longer a replication Controller
  managing the pods, it will no longer be recreated.

$ kubectl delete pod kubia-rc-c5jwb
$ kubectl delete pod kubia-rc-m2rhj

- At this point only two Pods will remain.

- The Pods can be re-associated with a Controller by creating a new controller with a selector
  matching the desired nodes.

- The following will re-create the controller by submitting tye yaml file.
- After it is created, another Pod will be created since the ReplicationController specifies 
  that there should be 3 running pods.

######################################################################################
### ReplicaSet
######################################################################################

- This is another resource type similar to a ReplicationController.
- The ReplicaSet now is mostly used instead of the ReplicationController.

- These two type of controllers are basically the same.  However, the ReplicaSet has 
  a more extensive selector support when compared to a ReplicationController.

- Currently there are 3 Pods running.  The following is going to delete the ReplicationController
  but leave the Pods running.  
- Then a new ReplicaSet will be defined containing a selector that will included the running Pods.
- This will then cause the Pods to be managed by the new ReplicaSet.

- Delete the ReplicationController leaving the Pods:

$ kubectl delete rc kubia-rc --cascade=false

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: kubia-set
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia-container
        image: blgreco72dev/kubia

$ kubectl create -f replica-set.yaml 

- Execute the following to see the created replication set:

$ kubectl get rs

NAME        DESIRED   CURRENT   READY   AGE
kubia-set   3         3         3       2m23s

- And the describe command can be used to view the details of a ReplicaSet.

$  kubectl describe rs

- Delete the ReplicaSet and the currently managed nodes:

$ kubectl delete rs kubia-set

######################################################################################
### DaemonSet
######################################################################################

- Allows running a specific Pod on all Nodes belonging to the cluster.

- If a new Node is added to the Cluster, the DaemonSet will automatically create a new Pod 
  instance on that node.

- Also, a DaemonSet can be defined to run a Pod on a set of Nodes meeting a specific criteria.

- The following is a definition for a DaemonSet:

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: daemon-test
spec:
  selector:
    matchLabels:
      app: daemon-test
  template:
    metadata:
      labels:
        app: daemon-test
    spec:
      nodeSelector:
        disk: ssd
      containers:
      - name: main
        image: blgreco72dev/kubia

$ kubectl create -f daemon-set.yaml

- At this point, zero Pods will have been created.  Currently there are no nodes
  labeled with drive: ssd

$ kubectl get pods

No resources found in default namespace.

- Next label some nodes with the needed label:

$ kubectl label node minikube disk=ssd

$ kubectl get pods
NAME                READY   STATUS    RESTARTS   AGE
daemon-test-jlszm   1/1     Running   0          31s


######################################################################################
### Jobs
######################################################################################
- The other controller types create a number of Pods and makes sure they
  are always running to match the number of specified replica.

- Jobs create a Pod for executing a container process.  After the process completes,
  the Pod is removed.

- If a node goes down when a Job is running, a Pod will be created on another node to continue
  until the job successfully completes.

- A Pod for a Job does not run indefinitely so the restartPolicy must be specified as OnFailure or Never.

apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
      - name: main
        image: luksa/batch-job


- After the job completes, the Pod will remain.  This allows you to check the logs if needed.


$ kubectl get pods

NAME                READY   STATUS    RESTARTS   AGE
batch-job-tkcl6     1/1     Running   0          2m55s

NAME                READY   STATUS      RESTARTS   AGE
batch-job-tkcl6     0/1     Completed   0          2m35s


----------------------------------------
CronJob
----------------------------------------
- These are similar to Jobs but they can be scheduled to run on an interval.  


######################################################################################
### Services
######################################################################################

