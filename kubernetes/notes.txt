######################################################################################
Cluster Implementations
######################################################################################

--------------------------------------------------------------------------------------
Installing minikube
--------------------------------------------------------------------------------------

https://phoenixnap.com/kb/install-minikube-on-ubuntu

--------------------------------------------------------------------------------------
Google Cloud Platform
--------------------------------------------------------------------------------------

Google Kubernetes Engine (GKE)


--------------------------------------------------------------------------------------
Multiple Node Cluster running on 4 ubuntu server VMs
--------------------------------------------------------------------------------------

Steps for creating:  creating-cluster.txt



######################################################################################
### Basic kubectl Commands
######################################################################################

- To view information about the Kubernetes cluster:

kubectl cluster-info   


[minikube-specific]

- If using Minikube, you can ssh into the node as follows:
    - minikube ssh

[minikube-specific]


- Each node in the cluster runs Docker, Kubelet, and the kube-proxy
- The kubectl command line sends HTTP REST requests to the Kubernetes API server running on master node.

- If you ssh into one of the nodes you can see these running processes:

    ps uax

- Each node runs Docker, Kubelet, and kube-proxy.


- To view all the nodes belonging to the cluster:

kubectl get nodes

- For more details of each node, the following can be executed:

kubectl get nodes -o wide


[minikube-specific]

- If using Minikube, there will be only one node.  This node acts as the Master and Worker node.
- To view more details about a an object, in this case a node, kubectl describe can be used

kubectl describe node minikube

[minikube-specific]


- The following can be used to view all pods running across all namespaces:

kubectl get pods --all-namespaces

- Just the pods for a specific namespace can be listed.
- First find the available namespaces that are defined:

 kubectl get namespaces

 - Then filter on a namespace as follows:

kubectl get pods -n calico-system

- It is often useful to run commands within a Container running within a Pod.
- If the Pod only has a single container (which is most often the case), the
  command will be executed within that container.  If the Pod has more than 
  one container, the -c switch must be specified to select the container by
  name.

- The following will run a Container within a Pod based on an ubuntu image
  containing the ping and curl command line executables.

kubectl run client --image blgreco72dev/ubuntu-tools

- Then the following will start an interactive shell session within the 
  client Pod's running Container:

kubectl exec -it client -- sh

- The above will be used to sent http requests to specific Pods running
  within the cluster.



######################################################################################
### Running Containers in Kubernetes
######################################################################################

- It is a best practice to define an YAML or JSON file to describe the container to be executed.
- However, it can also be done at the command line for testing.

- The following will create a pod.  This is of very limited use since its execution is not being
  managed by a ReplicaSet.  The following image contains an simple application that will returned
  the host name in which it runs.


kubectl run kubia --image=blgreco72dev/kubia --port=8080 


- The above command will run the container within a Kubernetes pod.  A pod can contain multiple containers
  but most often only one container is contained within a pod.

- Each Pod has its own IP address, host name, processes.
- You cannot list containers but the current pods can be listed.

kubectl get pods -o wide

NAME     READY   STATUS    RESTARTS   AGE    IP                NODE      NOMINATED NODE   READINESS GATES
client   1/1     Running   1          17m    192.168.144.65    w-node2   <none>           <none>
kubia    1/1     Running   0          100s   192.168.129.199   w-node1   <none>           <none>

- The above results show the kubia running on worker node 1.  The IP of the created pod is 192.168.129.199.
- This pod has been created on the Pod network and there fore cannot be accessed from the host operating system.
- However, the Pod can be accessed from another Pod running within the same Pod notwork.  This can be shows by
  running a shell within a Container of another running Pod and pinging the Pod's IP address:

kubectl exec -it client -- sh

# ping 192.168.129.199

PING 192.168.129.199 (192.168.129.199) 56(84) bytes of data.
64 bytes from 192.168.129.199: icmp_seq=1 ttl=62 time=0.782 ms
64 bytes from 192.168.129.199: icmp_seq=2 ttl=62 time=0.560 ms
64 bytes from 192.168.129.199: icmp_seq=3 ttl=62 time=0.589 ms
64 bytes from 192.168.129.199: icmp_seq=4 ttl=62 time=0.618 ms
64 bytes from 192.168.129.199: icmp_seq=5 ttl=62 time=0.677 ms

Also, the host name of the Pod can be obtained by sending a HTTP request the the Pod on port 8080:

# curl http://192.168.129.199:8080
You've hit kubia


- The following command can be used to view details of a specific pod:

kubectl describe pod kubia

- The details contain the following:
    - The container running within the Pod and the image from which it was created.
    - The port the container exposes (this is specified within the Docker file used to build the image.)
    - volumes
    - List of events recorded pertaining to the Pod.


- Kubernetes is built on top of a container implementation.  The most common one being Docker.
- When the minikube VM or any of the Ubuntu VMs created for the multi node cluster running in 
  Virtual Box, Docker was one of the first installed packages.

- The following will ssh into the VM running the kubia Pod to view the allocated Docker images and Containers:
- There is only one node in minikube, but for the multi node cluster, execute the following to determine the
  node on which the kubia Pod is running:

NAME     READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES
client   1/1     Running   2          34m   192.168.144.65    w-node2   <none>           <none>
kubia    1/1     Running   0          18m   192.168.129.199   w-node1   <none>           <none>

- In the above multi-node cluster, the kubia Pod is running on w-node1.
- Next, run the following to determine the IP address of w-node1:

kubectl get nodes -o wide

NAME      STATUS   ROLES    AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
m-node1   Ready    master   5h7m    v1.19.2   192.168.56.4   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.11
w-node1   Ready    <none>   4h20m   v1.19.2   192.168.56.5   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.11
w-node2   Ready    <none>   74m     v1.19.2   192.168.56.6   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.11
w-node3   Ready    <none>   59m     v1.19.2   192.168.56.7   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.11

- If minikube is being used, run the following to ssh into the simple node:

minikube ssh

- For the multi node cluster above, ssh into w-node1:

 ssh w-node1@192.168.56.5

- Once logged into the Node running the Pod containing the kubia container, complete the following to inspect the Docker objects:

- List the images installed on the Node:

docker images

blgreco72dev/kube-service-1   latest              7ec71e420a81        10 hours ago        211MB
blgreco72dev/ubuntu-tools     latest              fcd47ed02192        36 hours ago        114MB
k8s.gcr.io/kube-proxy         v1.19.2             d373dd5a8593        10 days ago         118MB
calico/node                   v3.16.1             0f351f210d5e        2 weeks ago         164MB
calico/pod2daemon-flexvol     v3.16.1             4cbe1ed86c35        2 weeks ago         22.9MB
calico/cni                    v3.16.1             4ab373b1fac4        2 weeks ago         133MB
calico/typha                  v3.16.1             c5132b2bf06f        2 weeks ago         52.2MB
            latest              1bbf21a8a605        3 weeks ago         660MB
k8s.gcr.io/pause              3.2                 80d28bedfe5d        7 months ago        683kB

- The above shows several of the Kubernetes component images used to create containers on the VM.
- As expected, the blgreco72dev/kubia image exits.

- List the running containers:

docker container ls

CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS               NAMES
680032fbc6e7        blgreco72dev/kubia     "node app.js"            25 minutes ago      Up 25 minutes                           k8s_kubia_kubia_default_44d29c7e-7e5c-46c2-a898-0009410d2220_0
85e6b1da15d1        k8s.gcr.io/pause:3.2   "/pause"                 25 minutes ago      Up 25 minutes                           k8s_POD_kubia_default_44d29c7e-7e5c-46c2-a898-0009410d2220_0
a0b9fcb4a530        c5132b2bf06f           "/sbin/tini -- calic…"   2 hours ago         Up 2 hours                              k8s_calico-typha_calico-typha-6794d46fcc-946qr_calico-system_777553aa-11cf-484c-a362-ea529bd8b0d7_7
c287b032f875        0f351f210d5e           "start_runit"            2 hours ago         Up 2 hours                              k8s_calico-node_calico-node-rblrf_calico-system_b5a2b8a3-37be-4647-8259-e177f6d79f98_4
66ed65dd2f0c        d373dd5a8593           "/usr/local/bin/kube…"   2 hours ago         Up 2 hours                              k8s_kube-proxy_kube-proxy-dgxd4_kube-system_430660f7-28fa-49c7-8de8-6c331490eddd_3
6022da4c0363        k8s.gcr.io/pause:3.2   "/pause"                 2 hours ago         Up 2 hours                              k8s_POD_calico-node-rblrf_calico-system_b5a2b8a3-37be-4647-8259-e177f6d79f98_3
4389e6844e0f        k8s.gcr.io/pause:3.2   "/pause"                 2 hours ago         Up 2 hours                              k8s_POD_kube-proxy-dgxd4_kube-system_430660f7-28fa-49c7-8de8-6c331490eddd_3
498f6881bcf3        k8s.gcr.io/pause:3.2   "/pause"                 2 hours ago         Up 2 hours                              k8s_POD_calico-typha-6794d46fcc-946qr_calico-system_777553aa-11cf-484c-a362-ea529bd8b0d7_3

- The first listed container is the one running the image that was submitted to Kubernetes for execution.
- Next, inspect the logs of some of the containers:

docker logs a0b9fcb4a530
docker logs 680032fbc6e7


- When a Pod is created by submitting a command to the Kubernetes REST API, the service takes multiple Steps
  to run a Pod for the specified Docker image.  At a high level, the Scheduler is called with the Pod specification.
  The scheduler determines the Node on which the Pod could be created.  The Kubelet process running on the node 
  receives the request to create the Pod.  The Kubelet process downloads the Docker image and creates the corresponding
  container.  

- As shown below, the Kubelet process is also responsible for monitoring the running containers and assures they 
  remain running at all times.


- While still logged into the VM running the kubia example container, attempt to stop the running container to see 
 if it is recreated.  Note the ID of the current running container. 

docker container stop 680032fbc6e7

- Then list the containers again:

docker container ls


CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS               NAMES
5662829e4ec4        blgreco72dev/kubia     "node app.js"            39 seconds ago      Up 39 seconds                           k8s_kubia_kubia_default_44d29c7e-7e5c-46c2-a898-0009410d2220_1
85e6b1da15d1        k8s.gcr.io/pause:3.2   "/pause"                 40 minutes ago      Up 40 minutes                           k8s_POD_kubia_default_44d29c7e-7e5c-46c2-a898-0009410d2220_0
a0b9fcb4a530        c5132b2bf06f           "/sbin/tini -- calic…"   2 hours ago         Up 2 hours                              k8s_calico-typha_calico-typha-6794d46fcc-946qr_calico-system_777553aa-11cf-484c-a362-ea529bd8b0d7_7
c287b032f875        0f351f210d5e           "start_runit"            2 hours ago         Up 2 hours                              k8s_calico-node_calico-node-rblrf_calico-system_b5a2b8a3-37be-4647-8259-e177f6d79f98_4
66ed65dd2f0c        d373dd5a8593           "/usr/local/bin/kube…"   2 hours ago         Up 2 hours                              k8s_kube-proxy_kube-proxy-dgxd4_kube-system_430660f7-28fa-49c7-8de8-6c331490eddd_3
6022da4c0363        k8s.gcr.io/pause:3.2   "/pause"                 2 hours ago         Up 2 hours                              k8s_POD_calico-node-rblrf_calico-system_b5a2b8a3-37be-4647-8259-e177f6d79f98_3
4389e6844e0f        k8s.gcr.io/pause:3.2   "/pause"                 2 hours ago         Up 2 hours                              k8s_POD_kube-proxy-dgxd4_kube-system_430660f7-28fa-49c7-8de8-6c331490eddd_3
498f6881bcf3        k8s.gcr.io/pause:3.2   "/pause"                 2 hours ago         Up 2 hours                              k8s_POD_calico-typha-6794d46fcc-946qr_calico-system_777553aa-11cf-484c-a362-ea529bd8b0d7_3

- Note that the stopped container was not just restarted by kubia but a new container created with a new container ID.
- If the new container is not listed, run the command to list the containers several times as it can take a moment for
  it to be recreated.


- Next, attempt to remove the running container to determine if it is recreated:

docker container rm -f 5662829e4ec4

Listing the containers again, shows that it was almost instantly recreated:

CONTAINER ID        IMAGE                  COMMAND                  CREATED              STATUS              PORTS               NAMES
e1146150d0f7        blgreco72dev/kubia     "node app.js"            About a minute ago   Up About a minute                       k8s_kubia_kubia_default_44d29c7e-7e5c-46c2-a898-0009410d2220_1
85e6b1da15d1        k8s.gcr.io/pause:3.2   "/pause"                 46 minutes ago       Up 46 minutes                           k8s_POD_kubia_default_44d29c7e-7e5c-46c2-a898-0009410d2220_0

- As a last attempt, the image will be deleted and then the container removed.

docker image rm blgreco72dev/kubia -f
docker container rm e1146150d0f7 -f 

- If you list the images and containers, the image will have been re-pulled down and
  a new container will have been created.

- Exit the node's shell:

$ exit

- The following can be executed to view the events that took place for the pod:

$ kubectl get pods
$ kubectl describe pod kubia

- The following command will delete the pod.  Not that this will take a few seconds to complete:

kubectl delete pod kubia

- The following command is helpful to see the pods running across all nodes and namespaces:

kubectl get pods -o wide --all-namespaces


######################################################################################
### Creating Resources using YAML
######################################################################################

- A Pod was created at the command line above as follows:

  kubectl run kubia --image=blgreco72dev/kubia --port=8080 

- This approach should not be used within production since the configuration 
  is not documented. Instead resources can be defined in YAML.

- If an existing resource exists within Kubernetes that was created at the
  command line, the YAML can be generated.  This is also useful for viewing
  a resources current configuration on the server.

- The following command can be used to obtain an YAML definition for any type of resource.
  Below shows requesting YAML for the created Pod created above.  The YAML can be very verbose 
  and contains properties with default values that would not typically be set.

$ kubectl get pod

NAME     READY   STATUS    RESTARTS   AGE
client   1/1     Running   6          15h
kubia    1/1     Running   0          2m9s

$ kubectl get pod kubia -o yaml


- The YAML file consists of the following sections:
    - Metadata
    - Specification
    - Status

- The following will delete the above created Pod and it will be 
  recreated by defining an YAML file instead.

kubectl delete pod kubia

- When defining a YAML file for a Pod, the Ports on which the container listens
  can be specified.  This is just for documentation and has no bearing on how
  the container or pod is created.

Creating the Pod using YAML
----------------------------------

cat lessons/pod-definition.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: kubia-pod
spec:
  containers:
  - image: blgreco72dev/kubia
    name: kubia
    ports:
    - containerPort: 8080
      protocol: TCP


- The following will submit the YAML to the Kubernetes REST API:

$ kubectl create -f lessons/pod-definition.yaml  
$ kubectl get pods -o wide

NAME        READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES
client      1/1     Running   7          15h   192.168.144.66    w-node2   <none>           <none>
kubia-pod   1/1     Running   0          26s   192.168.129.201   w-node1   <none>           <none>

- Applications running within containers typically write log files to standard
out and standard error streams.

- The logs for a givin pod can be see by issuing the following command:
$ kubectl logs kubia-pod

- If the Pod contains multiple containers, you must specify the container's name
using the -c parameter.

- A Pod's logs are deleted when the Pod no longer exists.  If logs need to be
  persisted, they need to be written to a central error log store.


######################################################################################
### Pod Details
######################################################################################

- Pods can contain multiple containers.  The containers within a pod will always run
  on the same worker node.

- This can be used if the containers must be located on the same machine.  This can be 
  the case for inter-process communication or if the processes running within each of
  the containers need to access a shared file.

- Pods allows containers to be bound together and manage then as a single unit.

- The containers within the pod can share certain resources and are not fully 
  isolated as when running within Docker.  Kubernetes does this by sharing the
  same set of Linux namespace between the two containers.  In Docker, each 
  container has its own set of namespaces.

- The containers within a Pod are all under the same network they share the same 
  host name and network interfaces.

- Each container running with in a Pod has a file systems isolated from each other.  
  However, the containers can share files by using volumes.

* Since the containers run within the same network namespace, they both have the
  the same IP address so ports must be unique across all containers.

- All Pods within a cluster are connected to the same software network and the
  containers can call one another by using each others IP address.  However, 
  this is not a good idea since Pods and be stopped, deleted, or moved and
  will be assigned a new IP Address.  The following shows this:


- List the details of all the pods to determine the node on which they are running:

  kubectl get pods -o wide

  NAME        READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
  client      1/1     Running   7          15h     192.168.144.66    w-node2   <none>           <none>
  kubia-pod   1/1     Running   0          8m29s   192.168.129.201   w-node1   <none>           <none>

- The kubia-pod is running on w-node1 and the client Pod is running on w-node2.

- The following will determine if a Pod can be accessed from the VM Host on which it is running:
- ssh into w-node1 and run the following to determine if the IP address of the pod(192.168.129.210) can be accessed:

ssh w-node1@192.168.56.5
  ping 192.168.129.201
  curl http://192.168.129.201:8080

At the above commands shows, the IP assigned to the Pod can be accessed form Node VM on which it runs.

- Next, determine if the same pod can be access from another Node VM that is part of the cluster.
- The following will SSH into w-node2 to determine if the IP address assigned to the Pod running 
  on w-node1 can be called:

ssh w-node2@192.168.56.6
  ping 192.168.129.201
  curl http://192.168.129.201:8080

- Again, the Pod can be accessed.

- Lastly, determine if the service-api running within a Pod on w-node1 can be accessed from a shell
  running within a container on another Pod.  In this case w-node2:


  kubectl exec -it client -- sh
     http://192.168.129.201:8080

    
  And the above also works.  Also, the shell running as a container within a Pod on w-node2 can 
  access the IP address of w-node1:

  ping 192.168.56.5

  - However, the service-api running within Pod cannot be accessed from outside the cluster 
    until a Service resource is created.  The next section will show how the Pod can be 
    accessed from outside the cluster for development needs by running a local proxy that
    will forward all calls from the Host computer to the pod running within the cluster.

  *** The above is just researching what components can access one another within the cluster.
  *** There would be of no reason to be doing the above as the IP addresses of Pods are not 
  *** static.  Also, it would be a bad design if a Container running within one Pod on a 
  *** node would use the IP address of another Node within the cluster.

- The above shows that an IP address assigned to a Pod running on a node can be accessed by any
  other node that is part of the cluster.

- All the containers comprising of a solution should not be all located within the
  same Pod.  Only very closely related containers should be located within the same
  Pod.

- Since all containers within a Pod will be executed on the same worker machine,
  they will all consume resources on that node and can't be relocated to a node
  with additional resources.  

- Another reason why two containers many not be best located with the same Pod
  is that it make scaling harder.  In Kubernetes scaling (replication) is at the
  Pod level.  So if the Pod contains multiple containers, then they must be scaled 
  together at the same time.

- The main reason for having multiple containers in a Pod, is when one of the
  containers contains the core process and the other is more of a "side-car"
  process.  The side-car container executes code periodically (i.e.  check an API
  for updates to data consumed by the main container processes.)


######################################################################################
### Port Forwarding
######################################################################################

- Unless additional Kubernetes network resources are defined, a Pod cannot be 
  accessed without having to ssh into a node that is part of the cluster as
  shown above.  However, for testing specific nodes during development, 
  port forwarding can be used.

- The newly created pod cannot be accessed from outside the cluster since a service
  has not yet been defined.  However, the Pod can be communicated with using port
  forwarding to test the Pod.

- Within one terminal, run the following:

$ kubectl port-forward kubia-pod 8888:8080

- The above is telling Kubernetes that the port 8888 on the host operating system
  should be forwarded to port 8080 of the pod named kubia-pod.

- In another terminal window, type the following to communicate with the container
  running within the Pod.

$ curl localhost:8888


######################################################################################
### Creating Replication Controller
######################################################################################
  
- Before continuing, make sure the kubia-pod is deleted:

 kubectl delete pod kubia-pod

- In kubernetes, Pod definition YAML files are rarely used and instead ReplicationController
  resources are defined.  This will be shown but note that ReplicationController resources 
  are no longer used by instead the very closely related ReplicaSet.

cat lessons/replication-ctl-definition.yaml 

apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia-rc
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      name: kubia-pod
      labels:
        app: kubia
    spec:
      containers:      
      -  name: kubia-container
         image: blgreco72dev/kubia
         ports:
          - containerPort: 8080

$ kubectl create -f lessons/replication-ctl-definition.yaml 
$ kubectl get pods -o wide

NAME             READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES
client           1/1     Running   11         16h   192.168.144.66    w-node2   <none>           <none>
kubia-rc-c8t6g   1/1     Running   0          45s   192.168.129.202   w-node1   <none>           <none>
kubia-rc-dktdk   1/1     Running   0          45s   192.168.144.67    w-node2   <none>           <none>
kubia-rc-ldpxc   1/1     Running   0          45s   192.168.138.1     w-node3   <none>           <none>

- The following command can be used to view Replication Controllers:

kubectl get rc -o wide

NAME       DESIRED   CURRENT   READY   AGE   CONTAINERS        IMAGES               SELECTOR
kubia-rc   3         3         3       42m   kubia-container   blgreco72dev/kubia   app=kubia

- The above shows that the Replication Controller specifies that 3 Pods are desired and that there
  are currently 3 that are ready.

- A Replication Controller is part of the Control Plane and is responsible for monitoring that a specified
  number of Pods remain running at all times.  On each worker node, the Kubelet process is responsible for
  making sure Pod containers remaining running.  If the Controller determines that a Pod is no longer running,
  it will communicate with the Scheduler so another Pod is created on a Node belonging to the cluster.

- The above shows that Kubernetes created 3 instances of the Pod defined within the YAML definition.

- Now that there are multiple Pods, each one has a random postfix value appended it its name.

- To validate that Kubernetes will maintain three running kube-rc Pods, the following will delete one of
  the running Pods to see if a new one is created:


$ kubectl delete pod kubia-rc-dktdk 
$ kubectl get pods -o wide

NAME             READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES
client           1/1     Running   12         16h   192.168.144.66    w-node2   <none>           <none>
kubia-rc-5qsj2   1/1     Running   0          48s   192.168.144.68    w-node2   <none>           <none>
kubia-rc-c8t6g   1/1     Running   0          13m   192.168.129.202   w-node1   <none>           <none>
kubia-rc-ldpxc   1/1     Running   0          13m   192.168.138.1     w-node3   <none>           <none>

- The above shows that the Pod deleted on w-node2 was recreated since the Pod name is now different.
- The scheduler determined that the new Pod should again be created on w-node2.

- Next, ssh into w-node2 and shut it down.

  ssh w-node2@192.168.56.6
      sudo shutdown now

- Run the following command until it shows that Kubernetes had detected that w-node2 is offline and
  creates a new Pod instance on one of the other worker nodes.  Note:  It can take someone for 
  Kubernetes to detect a missing Node within the cluster.

$ kubectl get pods -o wide

NAME             READY   STATUS        RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
client           1/1     Terminating   12         16h     192.168.144.66    w-node2   <none>           <none>
kubia-rc-4twvc   1/1     Running       0          2m29s   192.168.138.2     w-node3   <none>           <none>
kubia-rc-5qsj2   1/1     Terminating   0          11m     192.168.144.68    w-node2   <none>           <none>
kubia-rc-c8t6g   1/1     Running       0          24m     192.168.129.202   w-node1   <none>           <none>
kubia-rc-ldpxc   1/1     Running       0          24m     192.168.138.1     w-node3   <none>           <none>

- The above shows that the Pod running on w-node2 has been terminated and a new Pod is running w-node3.  
- Now w-node3 has two instances of the pod running.
- Also note that the client Pod was also running on w-node2 but another instance has and will not be created.
- The client pod was not created by a controller resource an therefore nothing is monitoring it. 

- Next, start back up w-node2.  Once started, the two pending Pods will be deleted.  The new Pod created on 
  w-node3 however will remain and not be terminated and automatically created back on w-node2.


$ kubectl get pods -o wide

NAME             READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
kubia-rc-4twvc   1/1     Running   0          9m10s   192.168.138.2     w-node3   <none>           <none>
kubia-rc-c8t6g   1/1     Running   0          30m     192.168.129.202   w-node1   <none>           <none>
kubia-rc-ldpxc   1/1     Running   0          30m     192.168.138.1     w-node3   <none>           <none>

- In order to get one off the Pods off of w-node3 and back to w-node2, either the Replication Controller can be 
  deleted and recreated or one of the nodes running on w-node3 can be deleted.  There is not guarantee if one Pod
  is deleted on w-node3 that the new one will be created on w-node2, but since there is no reason for Kubernetes
  not to select w-node2 due to limited resources, it will most likely be created there:

kubectl delete pod kubia-rc-ldpxc

- Unlike taking w-node2 offline, Kubernetes will almost instantly detect this state and create a new Pod.

NAME             READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES
kubia-rc-4twvc   1/1     Running   0          13m   192.168.138.2     w-node3   <none>           <none>
kubia-rc-b9vrp   1/1     Running   0          47s   192.168.144.69    w-node2   <none>           <none>
kubia-rc-c8t6g   1/1     Running   0          34m   192.168.129.202   w-node1   <none>           <none>

Next, create another instance of the client Pod:

kubectl run client --image blgreco72dev/ubuntu-tools
ubectl get pods -o wide

NAME             READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
client           1/1     Running   0          31s     192.168.129.203   w-node1   <none>           <none>
kubia-rc-4twvc   1/1     Running   0          18m     192.168.138.2     w-node3   <none>           <none>
kubia-rc-b9vrp   1/1     Running   0          5m54s   192.168.144.69    w-node2   <none>           <none>
kubia-rc-c8t6g   1/1     Running   0          39m     192.168.129.202   w-node1   <none>           <none>


- The Describe command can be used to see ReplicationController details.
$ kubectl describe rc kubia-rc


Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  53m   replication-controller  Created pod: kubia-rc-c8t6g
  Normal  SuccessfulCreate  53m   replication-controller  Created pod: kubia-rc-dktdk
  Normal  SuccessfulCreate  53m   replication-controller  Created pod: kubia-rc-ldpxc
  Normal  SuccessfulCreate  40m   replication-controller  Created pod: kubia-rc-5qsj2
  Normal  SuccessfulCreate  31m   replication-controller  Created pod: kubia-rc-4twvc
  Normal  SuccessfulCreate  19m   replication-controller  Created pod: kubia-rc-b9vrp


- The above listed events show that first 3 Pods being created.  Then the next three 
  entries shows the new pods that where created as the above actions where taken to
  test the Replication Controller.

------------------------------------------------------------------------------------------------

- There is not a direct tie between Pods and ReplicationControllers.  The Pods associated
  with a controller are any Pods for which its labels match the controller's selector at
  any specific point in time.

- So if a Pods label no longer matches the controller's selector, the controller will 
  communicate with the schedule so another Pod is created on a node.

- The running Replication Controller has the following selector:  

    app: kubia

- The following command can be used to display a specific label value associated with Pods:

 $ kubectl get pods -o wide -L app

NAME             READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES   APP
client           1/1     Running   1          20m   192.168.129.203   w-node1   <none>           <none>            
kubia-rc-4twvc   1/1     Running   0          38m   192.168.138.2     w-node3   <none>           <none>            kubia
kubia-rc-b9vrp   1/1     Running   0          26m   192.168.144.69    w-node2   <none>           <none>            kubia
kubia-rc-c8t6g   1/1     Running   0          60m   192.168.129.202   w-node1   <none>           <none>            kubia

- Next, the "app" label for one of the pods will be changed to no longer match the selector:

 $ kubectl label pod kubia-rc-4twvc app=something --overwrite=true
 $ kubectl get pods -o wide -L app

NAME             READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES   APP
client           1/1     Running   1          22m   192.168.129.203   w-node1   <none>           <none>            
kubia-rc-4twvc   1/1     Running   0          39m   192.168.138.2     w-node3   <none>           <none>            something
kubia-rc-6nwxg   1/1     Running   0          14s   192.168.138.3     w-node3   <none>           <none>            kubia
kubia-rc-b9vrp   1/1     Running   0          27m   192.168.144.69    w-node2   <none>           <none>            kubia
kubia-rc-c8t6g   1/1     Running   0          61m   192.168.129.202   w-node1   <none>           <none>            kubia

- Almost instantly, Kubernetes detected that for a short period of time that only two Pods matched the selector as specified
  by the replication controller.  After this was detected, the Controller communicated to the Scheduler that another Pod 
  matching the template defined within the Replication Controller needed to be created.  

- The updated pod however will still exist.  It is just no longer associated with the controller.
- Next, the Pod with the renamed label will be changed back.

$ kubectl label pod kubia-rc-4twvc app=kubia --overwrite=true
$ kubectl get pods -o wide -L app

NAME             READY   STATUS        RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES   APP
client           1/1     Running       1          26m     192.168.129.203   w-node1   <none>           <none>            
kubia-rc-4twvc   1/1     Running       0          44m     192.168.138.2     w-node3   <none>           <none>            kubia
kubia-rc-6nwxg   1/1     Terminating   0          4m19s   192.168.138.3     w-node3   <none>           <none>            kubia
kubia-rc-b9vrp   1/1     Running       0          31m     192.168.144.69    w-node2   <none>           <none>            kubia
kubia-rc-c8t6g   1/1     Running       0          65m     192.168.129.202   w-node1   <none>           <none>            kubia

- Now that there are 4 nodes matching the selector of the Replication Controller, Kubernetes detected this and is terminating
  one of the nodes so only a total of 3 will be running.

$ kubectl get pods -o wide -L app

NAME             READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES   APP
client           1/1     Running   1          27m   192.168.129.203   w-node1   <none>           <none>            
kubia-rc-4twvc   1/1     Running   0          45m   192.168.138.2     w-node3   <none>           <none>            kubia
kubia-rc-b9vrp   1/1     Running   0          33m   192.168.144.69    w-node2   <none>           <none>            kubia
kubia-rc-c8t6g   1/1     Running   0          67m   192.168.129.202   w-node1   <none>           <none>            kubia


- The Pod's template within a ReplicationController can be updated.  However, any existing pods will not be affected.
  To apply the update the the existing pods, they need to be first deleted so new ones will be created by the updated
  replication controller.  For example, if the selector is updated to be more specific by adding a check on an additional 
  lab label, not of the existing pods would match and new ones would be created.

- The following will delete the replication controller and any nodes matching the selector:

$ kubectl delete -f lessons/replication-ctl-definition.yaml 
$ kubectl get pods -o wide

NAME             READY   STATUS        RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES
client           1/1     Running       2          34m   192.168.129.203   w-node1   <none>           <none>
kubia-rc-4twvc   0/1     Terminating   0          52m   <none>            w-node3   <none>           <none>
kubia-rc-b9vrp   0/1     Terminating   0          40m   192.168.144.69    w-node2   <none>           <none>
kubia-rc-c8t6g   0/1     Terminating   0          74m   192.168.129.202   w-node1   <none>           <none>

$ kubectl get pods -o wide

NAME     READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES
client   1/1     Running   2          35m   192.168.129.203   w-node1   <none>           <none>

- A resource definition can be edited within the terminal as follows:
$ kubectl edit rc kubia-rc

** By default vi is used as the editor.  The following will update so nano editor can be used:

export KUBE_EDITOR="/usr/bin/nano"

- Next, an additional label will be added to the Pod's template.  After making the change, the
  pods and their labels will be listed.  Note that the addition label has not effect.  To update
  the nodes, they need to be deleted.  This will trigger the node to be recreated since the 
  current number no longer matches the desired value of 3.

- Add a label:   ver: one 
- To the Replication Controller selector and also to the Pod template.

- Once saved, Kubernetes will determine that there are no nodes containing both the app and ver
  labels and will use the associated template to create three new nodes:

kubectl get pods -o wide -L app -L  ver

kubia-rc-2cwf4   1/1     Running   0          7m54s   192.168.129.204   w-node1   <none>           <none>            kubia   
kubia-rc-62md4   1/1     Running   0          7m54s   192.168.138.4     w-node3   <none>           <none>            kubia   
kubia-rc-b7lh8   1/1     Running   0          2m14s   192.168.144.71    w-node2   <none>           <none>            kubia   one
kubia-rc-cdvk2   1/1     Running   0          2m14s   192.168.138.5     w-node3   <none>           <none>            kubia   one
kubia-rc-g9q4b   1/1     Running   0          7m54s   192.168.144.70    w-node2   <none>           <none>            kubia   
kubia-rc-rhl4b   1/1     Running   0          2m14s   192.168.129.205   w-node1   <none>           <none>            kubia   one

- Now there are a total of 6 Pods running.  Three of which are no longer associated with the Replication Controller.
- To clean up the pods so there are only 3 matching the updated Replication Controller definition, delete all pods
  having a label of "app=kubia".  This will cause Kubernetes to detected that there are now no Pods matching the 
  Replication Controllers label and create 3 new ones.

$ kubectl delete pods -l app=kubia

- After all the Pods are deleted, run the following to confirm the new state:

kubectl get pods -o wide -L app -L  ver

NAME             READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES   APP     VER
client           1/1     Running   3          51m   192.168.129.203   w-node1   <none>           <none>                    
kubia-rc-66hxp   1/1     Running   0          50s   192.168.129.206   w-node1   <none>           <none>            kubia   one
kubia-rc-b8wjd   1/1     Running   0          50s   192.168.138.6     w-node3   <none>           <none>            kubia   one
kubia-rc-h7tjm   1/1     Running   0          50s   192.168.144.72    w-node2   <none>           <none>            kubia   one


------------------------------------------------------------------------------------------------
 Horizontally scaling pods
------------------------------------------------------------------------------------------------

- The number of replicas can be changed for a Replication Controller.

kubectl scale rc kubia-rc --replicas=5
kubectl get pods -o wide

NAME             READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
client           1/1     Running   3          54m     192.168.129.203   w-node1   <none>           <none>
kubia-rc-66hxp   1/1     Running   0          3m45s   192.168.129.206   w-node1   <none>           <none>
kubia-rc-9pbwb   1/1     Running   0          77s     192.168.144.73    w-node2   <none>           <none>
kubia-rc-b8wjd   1/1     Running   0          3m45s   192.168.138.6     w-node3   <none>           <none>
kubia-rc-bdwg8   1/1     Running   0          77s     192.168.138.7     w-node3   <none>           <none>
kubia-rc-h7tjm   1/1     Running   0          3m45s   192.168.144.72    w-node2   <none>           <none>

- The above shows that two more Pods where created.

- The replication controller can also have it definition edited:

kubectl edit rc kubia-rc

- When deleting a ReplicationController, the associated Pods will also be deleted.

- But since the ReplicationController and Pods can exist independently, the Pods
  can remain and only the ReplicationController controller deleted.

$ kubectl delete rc kubia-rc --cascade=false
$ kubectl get rc
$ kubectl get pods -o wide -L app -L  ver

NAME             READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES   APP     VER
client           1/1     Running   3          57m     192.168.129.203   w-node1   <none>           <none>                    
kubia-rc-66hxp   1/1     Running   0          7m13s   192.168.129.206   w-node1   <none>           <none>            kubia   one
kubia-rc-9pbwb   1/1     Running   0          4m45s   192.168.144.73    w-node2   <none>           <none>            kubia   one
kubia-rc-b8wjd   1/1     Running   0          7m13s   192.168.138.6     w-node3   <none>           <none>            kubia   one
kubia-rc-bdwg8   1/1     Running   0          4m45s   192.168.138.7     w-node3   <none>           <none>            kubia   one
kubia-rc-h7tjm   1/1     Running   0          7m13s   192.168.144.72    w-node2   <none>           <none>            kubia   one

$ kubectl get rc
No resources found in default namespace.

- The above Pods are not longer being monitored by a Replication Controller.  Now if one of the Pods is deleted, a new
  replacement Pod will no longer be created:

$ kubectl delete pod kubia-rc-h7tjm

- After this command is executed, there will now only be 4 kubia-rc pods running:

$ kubectl get pods -o wide -L app -L  ver

NAME             READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES   APP     VER
client           1/1     Running   4          62m     192.168.129.203   w-node1   <none>           <none>                    
kubia-rc-66hxp   1/1     Running   0          11m     192.168.129.206   w-node1   <none>           <none>            kubia   one
kubia-rc-9pbwb   1/1     Running   0          9m31s   192.168.144.73    w-node2   <none>           <none>            kubia   one
kubia-rc-b8wjd   1/1     Running   0          11m     192.168.138.6     w-node3   <none>           <none>            kubia   one
kubia-rc-bdwg8   1/1     Running   0          9m31s   192.168.138.7     w-node3   <none>           <none>            kubia   one

- The Pods can be re-associated with a Controller by creating a new controller with a selector
  matching the desired nodes.

- The following will re-create the controller by submitting the yaml file that is saved to disk.

- After it is created, there will be one additional Pod since the Replication Controller
  specifies that there should only be 3.  Also, the specified selector is a sub set of
  the labels existing on the currently running Pod (since we updated the Replication 
  Controller at once point) but it has since been deleted. 

$ cat lessons/replication-ctl-definition.yaml

apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia-rc
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      name: kubia-pod
      labels:
        app: kubia
    spec:
      containers:      
      -  name: kubia-container
         image: blgreco72dev/kubia
         ports:
          - containerPort: 8080


$ kubectl create -f lessons/replication-ctl-definition.yaml 
$ kubectl get pods -o wide -L app -L  ver

NAME             READY   STATUS        RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES   APP     VER
client           1/1     Running       4          69m   192.168.129.203   w-node1   <none>           <none>                    
kubia-rc-66hxp   1/1     Running       0          18m   192.168.129.206   w-node1   <none>           <none>            kubia   one
kubia-rc-9pbwb   1/1     Running       0          16m   192.168.144.73    w-node2   <none>           <none>            kubia   one
kubia-rc-b8wjd   1/1     Running       0          18m   192.168.138.6     w-node3   <none>           <none>            kubia   one
kubia-rc-bdwg8   1/1     Terminating   0          16m   192.168.138.7     w-node3   <none>           <none>            kubia   one

- The above shows one of the Pods are being terminated to match the number of replicas specified within the Replication Controller.

$ kubectl get pods -o wide -L app -L  ver

NAME             READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES   APP     VER
client           1/1     Running   4          71m   192.168.129.203   w-node1   <none>           <none>                    
kubia-rc-66hxp   1/1     Running   0          20m   192.168.129.206   w-node1   <none>           <none>            kubia   one
kubia-rc-9pbwb   1/1     Running   0          17m   192.168.144.73    w-node2   <none>           <none>            kubia   one
kubia-rc-b8wjd   1/1     Running   0          20m   192.168.138.6     w-node3   <none>           <none>            kubia   one

- Now the number of pods match the desired number.
- Next, delete one of the Pods so a new replacement Pod is created.  Since the template within the Replication Controller does
  not specify the "ver" label, the newly created pod will only have the "app" label:

$ kubectl get pods -o wide -L app -L  ver

NAME             READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES   APP     VER
client           1/1     Running   4          73m   192.168.129.203   w-node1   <none>           <none>                    
kubia-rc-66hxp   1/1     Running   0          23m   192.168.129.206   w-node1   <none>           <none>            kubia   one
kubia-rc-9pbwb   1/1     Running   0          20m   192.168.144.73    w-node2   <none>           <none>            kubia   one
kubia-rc-fghws   1/1     Running   0          38s   192.168.138.8     w-node3   <none>           <none>            kubia 

- Lastly, delete the Replication Controller and any Pods associated by having matching labels:

$ kubectl delete -f lessons/replication-ctl-definition.yaml 
$ kubectl get pods -o wide 

NAME             READY   STATUS        RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
client           1/1     Running       5          75m     192.168.129.203   w-node1   <none>           <none>
kubia-rc-66hxp   1/1     Terminating   0          24m     192.168.129.206   w-node1   <none>           <none>
kubia-rc-9pbwb   1/1     Terminating   0          22m     192.168.144.73    w-node2   <none>           <none>
kubia-rc-fghws   1/1     Terminating   0          2m13s   192.168.138.8     w-node3   <none>           <none>

$ kubectl get pods -o wide 

NAME     READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES
client   1/1     Running   5          75m   192.168.129.203   w-node1   <none>           <none>

######################################################################################
### ReplicaSet
######################################################################################

- This is another resource type similar to a ReplicationController.
- The ReplicaSet now is mostly used instead of the ReplicationController.

- These two type of controllers are basically the same.  However, the ReplicaSet has 
  a more extensive selector support when compared to a ReplicationController.


cat lessons/replica-set-definition.yaml 

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: kubia-set
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia-container
        image: blgreco72dev/kubia

$ kubectl create -f lessons/replica-set-definition.yaml 

- Execute the following to see the created replication set:

$ kubectl get rs

NAME        DESIRED   CURRENT   READY   AGE
kubia-set   3         3         3       26s

- The following now shows the current running Pods:

$ kubectl get pods -o wide

NAME              READY   STATUS    RESTARTS   AGE   IP                NODE      NOMINATED NODE   READINESS GATES
client            1/1     Running   5          81m   192.168.129.203   w-node1   <none>           <none>
kubia-set-6ddvq   1/1     Running   0          75s   192.168.129.207   w-node1   <none>           <none>
kubia-set-9987k   1/1     Running   0          75s   192.168.144.74    w-node2   <none>           <none>
kubia-set-bj6cz   1/1     Running   0          75s   192.168.138.9     w-node3   <none>           <none>

- And the describe command can be used to view the details of a ReplicaSet.

$  kubectl describe rs kubia-set

Name:         kubia-set
Namespace:    default
Selector:     app=kubia
Labels:       <none>
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia-container:
    Image:        blgreco72dev/kubia
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age    From                   Message
  ----    ------            ----   ----                   -------
  Normal  SuccessfulCreate  3m23s  replicaset-controller  Created pod: kubia-set-9987k
  Normal  SuccessfulCreate  3m23s  replicaset-controller  Created pod: kubia-set-bj6cz
  Normal  SuccessfulCreate  3m23s  replicaset-controller  Created pod: kubia-set-6ddvq


- Delete the ReplicaSet and the currently managed nodes:

$ kubectl delete rs kubia-set


######################################################################################
### Replication and Controllers
######################################################################################

- Pods are usually not created directly.

- Instead, Replication Controllers and Deployments are used.  These two resource
  types monitor the Pods to which they are associated and will assure the correct
  number remain running on the cluster of notes.

- Controllers monitor pods and are responsible for keeping the needed number 
  running within the cluster.

- On each node, the Kubelet process monitors each running container and will make
  sure it remaining running.  If the Container's main process crashes, the Kubelet
  process will recreate a new container.

- However, a container can become unhealthy without the main process crashing so the
  container must provide a health-check that can be checked by Kubelet.

- These are called Liveness Probes and are specified within a Pod's specification.  
  These probes can be a HTTP GET method that is called or a command that is executed
  within the container.

- When a container is restarted, it is not the same container.  Instead a completely
  new container is created.

- An initial delay can be specified that will delay the first call to the Liveness 
  check.  This can be important if the container can't receive requests with first
  started since Kubelet will start calling the Liveness probe after the container
  is created.

- It is important that the Liveness check is based only only the state of the 
  container and not on any external ones.  If another container cannot be called,
  it is the other container that needs to be restarted and not the current.

- Kubelet = monitors containers to make sure they remain running.
- Controller = monitors Pods to make sure they remain running.

- Controllers usually monitor several replicas of a certain Pod type and assure 
  the number of running pods configured remain running.

- Pods are identified by labels and when creating a Controller, these labels are
  used to identify the Pods that the Controller should monitor and assure that
  the configured desired number matches the current running.

- If a Pods labels are changed, from what is specified within the Controller,
  the Pod is not stopped - it is just no longer managed by the controller.  If
  a label is added or changed that results in the Pod matching the Controller's
  selector, it will then be managed by the controller.
  


######################################################################################
### Services
######################################################################################

- Services device how one service can discover other services for which it needs to
  communicate with as part of a software solution running in a Kubernetes cluster.

- These communications may be between Pods with in the cluster and also from clients
  existing outside of the cluster.

- Services provide the following:
    - Provides a Single point of Entry to a specific set of Pods all providing the same
      service.  The set of Pods are normally created by a ReplicaSet.
    - When a Service resource is created, it is given a static IP address and port.
      This static address will remain as long as the Service is not deleted.
    - The service knows the IP addresses of all the Pods running the component-service API
      within the cluster.
    - When the Service receives a request, it routes to one of the Pods.
    - A service can also be configured with an external IP address that can be accessed by
      client outside of the cluster.  Services are also created internal to the cluster so
      services-apis running in other Pods can easily find other Pods service-apis to which
      the need to communicate.  This allows the calling service-api to find another service-api
      by one of the following two methods:

        - Environment Variable
        - DNS 

    - A service also load balances across the Pods running the service-api.

Creating a Service
-------------------------------

- Run the following to create a replica set consisting of three service instances that will
  then have services created:

$ kubectl create -f replica-set.yaml

- The following command list list any defined services with the default namespace:
- A service can be created using kubectl at the command line or defining an YAML definition file.

$ kubectl get services

- The only service should be the one for the REST API interface used by kubectl.

NAME         TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)          AGE
kubernetes   ClusterIP      10.96.0.1       <none>          443/TCP          2d8h

- Next, a service will be created to expose the service-api running within pods associated
  with the kubia-set.

$ kubectl expose replicaset kubia-set --type=LoadBalancer --port=8080

$ kubectl get services

NAME         TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)          AGE
kubernetes   ClusterIP      10.96.0.1       <none>          443/TCP          2d8h
kubia-set    LoadBalancer   10.104.193.98   10.104.193.98   8080:32041/TCP   27s

- Since the type of LoadBalancer was specified, the service will create an External-IP that
  can be used to call the service from outside the cluster.

- Running the following command several times shows how requests are load balanced and set
  to different Node/Pods:

curl 10.104.193.98:8080

greco@ubuntupc:~/_dev/git/dev-blog/kubernetes$ curl 10.104.193.98:8080
You've hit kubia-set-5ndp2
greco@ubuntupc:~/_dev/git/dev-blog/kubernetes$ curl 10.104.193.98:8080
You've hit kubia-set-pd6pb
greco@ubuntupc:~/_dev/git/dev-blog/kubernetes$ curl 10.104.193.98:8080
You've hit kubia-set-5ndp2
greco@ubuntupc:~/_dev/git/dev-blog/kubernetes$ curl 10.104.193.98:8080
You've hit kubia-set-5ndp2
greco@ubuntupc:~/_dev/git/dev-blog/kubernetes$ curl 10.104.193.98:8080

- Next, delete the service.

$ kubectl delete service kubia-set

Creating Service using YAML File:
-----------------------------------------


- If the export command is not used, then the service will not be exposed to 
  clients outside of the cluster.  This is the default behavior when creating
  a service defined within a YAML file.

apiVersion: v1
kind: Service
metadata:
  name: kubia-srv
spec:
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia

$ kubectl create -f service.yaml 
$ kubectl get services

kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP   17m
kubia-srv    ClusterIP   10.103.158.206   <none>        80/TCP    22s

- In the above listing, the created service only has a Cluster-IP so it can only
  be accessed from within the cluster.

- The kubectl exec command allows you to remotely run a command inside an existing container
  of a pod.  We will do this to show that the cluster-ip above can be used within a pod's 
  container to call the service-api.

- List the Pods so one can be used.

$ kubectl get pods

NAME                            READY   STATUS    RESTARTS   AGE
kubia-deploy-5d9f894958-cbsz9   1/1     Running   0          12m
kubia-set-7cjcv                 1/1     Running   0          11m
kubia-set-9frcf                 1/1     Running   0          11m
kubia-set-ctzcx                 1/1     Running   0          11m

kubectl exec kubia-set-ctzcx -- curl -s http://10.103.158.206

- Running the above will access the service using the cluster-ip from within a container running
  within a Pod, located on a node that is a member of the cluster.

- Since this internal request was made to a service, it forwards the request to one of the Pods
  running the service-api.

- While a client service running within the cluster can use the static IP address of the service
  to call any Pod hosting the service-api, there are better way rather than using the port directly.
  The client service can discover the IP address using the following methods.

- If a Pod is created after the service has been created, Kubernetes defines an Environment Variable
  that the client service-api can use containing the value of the IP address.  

- First delete all pods so new onces (containing the Environment Variable) will be created.

$ kubectl delete pods --all

- Next exec into a Pod's container and list the environment variables.

$ kubectl exec kubia-set-z946b env

KUBIA_SRV_SERVICE_PORT=80
KUBIA_SRV_PORT=tcp://10.103.158.206:80
KUBIA_SRV_PORT_80_TCP=tcp://10.103.158.206:80
KUBIA_SRV_PORT_80_TCP_PROTO=tcp
KUBIA_SRV_PORT_80_TCP_PORT=80
KUBIA_SRV_PORT_80_TCP_ADDR=10.103.158.206

- The other, and better option, is to use the build in DNS server.
- This DNS service can be accessed by all Pod service-apis running in the cluster.
- The DNS service is a member of the kube-system namespace:

$ kubectl get pods --namespace=kube-system

NAME                               READY   STATUS    RESTARTS   AGE
coredns-66bff467f8-2gm85           1/1     Running   0          42m
etcd-minikube                      1/1     Running   0          42m
kube-apiserver-minikube            1/1     Running   0          42m
kube-controller-manager-minikube   1/1     Running   0          42m
kube-proxy-bhnbt                   1/1     Running   0          42m
kube-scheduler-minikube            1/1     Running   0          42m
storage-provisioner                1/1     Running   0          42m

- Each service giets a DNS entry in the internal DNS server, and client pods that know the name
  of the service can access it through it fully qualified domain name (FQDN) instead of of using
  the corresponding environment variable.


- The following is the DNS entry for the kubia-srv:

kubia-srv.default.svc.cluster.local

- If the calling node is within the same Namespace, then just kubia-srv can be used.

- This can be tested by running bash inside one of the pod containers:

$ kubectl exec -it kubia-set-z946b -- bash

- Once inside the container the following will work:

$ curl http://kubia-srv.default.svc.cluster.local
# curl http://kubia-srv

- If the run the commands above a few times, you will again see that the service 
  routes to different pods belonging to the service.


######################################################################################
### Connecting to Services outside of Cluster
######################################################################################

- This is for the case where a service-api, running within the cluster, needs to
  call a service-api residing outside of the cluster.  Not only can Service resources
  be created for service-apis running within the cluster but for also service-apis
  that are hosted outside the cluster.

- When a service is created for a specific service-api running within a Pod's container,
  the service provides a single static IP and port for a single point of entry.  Services
  do not directly point to a Pod but point indirectly via the Endpoints resources.

- The Endpoints resource contains a list if IP/ports for Pods's containing a specific service-api.
- The Service resource then routes a request to its static IP/Port to one of the defined endpoints.

- These endpoints can also be viewed:

$ kubectl get endpoints

NAME         ENDPOINTS                                            AGE
kubernetes   172.17.0.3:8443                                      15h
kubia-srv    172.18.0.10:8080,172.18.0.11:8080,172.18.0.12:8080   15h

- Since Service Resources are not directly tied to Pod Resources but indirectly by Endpoint Resources,
  Endpoints (to external service-apis) can be manually configured and associated with a Service Resource.

- YAML files can be created to manually define the Service and Endpoint Resources.

- The following defines a service that can be called on port 80:

apiVersion: v1
kind: Service
metadata:
  name: external-services
spec:
  ports:
  - port: 80

$ kubectl create -f service-manual.yaml 

$ kubectl get services

NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
external-services   ClusterIP   10.101.84.49     <none>        80/TCP    39s
kubernetes          ClusterIP   10.96.0.1        <none>        443/TCP   15h
kubia-srv           ClusterIP   10.103.158.206   <none>        80/TCP    15h

- Since the above service does not have a Pod selector, no Endpoint Resources are created and
  therefore has to be done manually.


apiVersion: v1
kind: Endpoints
metadata:
  name: external-services
subsets:
  - addresses:
    - ip: 199.16.172.225
    - ip: 172.217.13.228
    ports:
    - port: 80

$ kubectl get services

NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
external-services   ClusterIP   10.97.41.239     <none>        80/TCP    7m46s

- Now from a container contained within a Pod within the cluster, a request can be made
  to IP 10.97.41.239 and the request will be forwarded to one of the endpoint entries.


######################################################################################
### Exposing Services to External Clients
######################################################################################

- This allows services-apis to be exposed so they can be accessed from outside the cluster.

- There are different ways of exposing a service-api contained within a container of a Pod.

- The first way to to define a service of type NodePort.  What this does is exposes the 
  service-api on all nodes contained within the cluster (regardless if the node actually
  has the pod containing the service-api).  The NodePort defines an Port that is exposed
  by each node running within the cluster to which can be used to access the service using
  the IP address of any node with the defined node-port to access the service.

- Then the request to forward to one of the nodes within the cluster actually running a 
  Pod containing a container with the service-api.  Also note that the static IP/Port of
  the service can also be used to invoke the service-api.

- NOTE:  If you are using a Kubernetes Cloud service, the IP addresses and ports of the
  Nodes might need to be configured to alow access within the firewall.

- Now, the service-api is technically exposed to external client using the defined port
  on any of the nodes within the cluster.

- The issue with the above configuration is that if you provide an external client with
  one of the node's IP addresses and exposed port, used to the call the service-api, there
  will be an issue if the Node is taking offline.

- The next step is to place a load balancer in front of all the nodes so incoming requests
  are routed to any of the cluster nodes.

- Most cloud based Kubernetes hosted services provision such a load balancer and all that
  is required to to specify LoadBalancer for the Service resource type.

- The following is an overview:
    - When a Pod is created to run a container's service-api, by default, it can only be
      accessed from within the cluster from other Pod container service-apis.
    
    - Container service-apis can call the other service by using the IP address and Port
      of one of the Nodes hosting a Pod with the service-api.  The issue with this is Pods
      can be moved to other hosts after they are stopped or crashed or as they are scaled
      up and down.  When this happens, the IP address associated with the Pod will not 
      longer exist.
    
    - To address the prior issue, a Service can be defined for a specific service-api. The
      service definition uses labels to determine the Pods that should be associated with
      the created service.  After the service is created, it defines a static IP address 
      known as the Cluster-IP that all clients can call (client inside the cluster) to
      invoke the service-api.  The Service Resource will forward the request to one of the
      Pods running a container hosting the service-api.  The client can then use the static
      IP address and Port to call the service.  The client's can obtain the service static
      PI address and Port using either Environment Variables defined by Kubernetes or the
      DNS server that is running within the cluster.
    
    - The next case is for service-apis that need to be accessed from outside of the cluster.

    - The first approach is to define a service as being a NodePort type.  This type of service
      defines a Port that is exposed on each Node within the Cluster that can be used to call the
      service-api running within a Pod's container.  Regardless if the Node called is actually
      hosting the service-api, the call will be routed to one of the Pods's that is running the
      service-api.  The defined port is accessible from outside the cluster by clients.  Clients,
      can be given the IP address of any of the nodes and they call invoke the service using the
      exposed port.  Note that this method will usually require opening the port of the firewall.

    - The issue with the last item is that the client is hard-coded to a specific Node IP address.
      If the node is taken offline, the client will no longer be able to access the service-api.
      So the next step is to define a service of type LoadBalancer which contains the functionally
      of a NodePort but also defines a Static IP address and Port that will load balance to any of
      the internal nodes using the defined node-port.  When creating a LoadBalancer service the 
      NodePorts are created automatically.  

    - With the load balancer service defined, clients can now call the service using the static IP 
      address and port of the LoadBalancer and the request will be routed to on of the internal nodes
      defined within the cluster.  Then the request will be routed when received by the node to one
      of the nodes actually hosting the service-api.

    - The one issue with the above is that a service (and thus a separate static IP address and Port)
      need to be create for each and every microservice.  

    - So the next step in progresses to what is called an Ingress Resource which is documented next.

  
######################################################################################
### Exposing Services externally using Ingress Resource
######################################################################################

- The issue with creating a LoadBalancer as discussed in the last section is that a LoadBalancer
  service needs to be created for each service-api.  Each such created service will have its own
  defined static IP address and Port.  

- Ingress is the next logical step in that it exposes only a single static IP address and Port
  that can be used to route requests to multiple internal defines service-apis running within automatically
  Pod.

- For this to function, an Ingress controller needs to be running within the cluster.  This is already
  the case for most cloud hosted Kubernetes services.  But if using minikube, it must be enabled.

$ minikube addons list
$ minikube addons enable Ingress

- The following assumes that all service resources have been deleted but there is a ReplicaSet
  resource created with a replica count of 3.

- First a NodePort service needs to be created using a label to select the running Pods in the
  ReplicaSet.

apiVersion: v1
kind: Service
metadata:
  name: kubia-nodeport
spec:
  type: NodePort
  ports:  
  - port: 80
    targetPort: 8080
    nodePort: 30123
  selector:
    app: kubia

$ kubectl create -f ingress-node-port-service.yaml

apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: kubia-nodeport
          servicePort: 80

$ kubectl create -f ingress-service.yaml 

- Next, get the IP address of the Ingress controller:

$ kubectl get ingress

NAME    CLASS    HOSTS               ADDRESS      PORTS   AGE
kubia   <none>   kubia.example.com   172.17.0.3   80      3m

Add the following line to /etc/host file:

172.17.0.3 kubia.example.com

- Run the following several times and Ingress will route the calls to one of the
 nodes within the cluster using port 30123 which then will be routed to one of
 the node containing a Pod running the corresponding service-api.

$ curl http://kubia.example.com

- Above the path "/" was used.  But to add additional service-apis, multiple paths
  can be defined, one for each node-port service (which is defined on a set of pods
  having matching labels) then Ingress uses the path to determine the node-port 
  service to call the dispatch the request.



















































































######################################################################################
### Access Api from outside of the cluster
######################################################################################

https://minikube.sigs.k8s.io/docs/handbook/accessing/#using-minikube-tunnel

- The following will create a deployment and expose the service running within
the container of the Kubernetes pod so it can be accessed from outside the 
cluster.

- Run in command console:
-----------------------------------
$ minikube start
$ minikube dashboard

- Run the following in another command console:
-----------------------------------
$ minikube tunnel

- Run the following in another command console:
$ kubectl create deployment kubia-deploy --image blgreco72dev/kubia

- Run the following to view details about the deployment and created resources:
-----------------------------------
$ kubectl get deployments
$ kubectl describe deployments kubia-deploy
$ kubectl get pods
$ kubectl describe pods kubia

- Run the following to expose the pod created for the container within the deployment:
-----------------------------------
$ kubectl expose deployment kubia-deploy --type=LoadBalancer --port=8080

- The above is telling Kubernetes to expose all pods created from the named deployment.

$ kubectl get service


NAME           TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)          AGE
kubernetes     ClusterIP      10.96.0.1      <none>         443/TCP          13m
kubia-deploy   LoadBalancer   10.98.69.127   10.98.69.127   8080:32214/TCP   12m

- Note the IP that has been assigned to External-IP for the service.

$ curl 10.98.69.127:8080


######################################################################################
### Expecting the Node
######################################################################################

- The minikube node has an ip address of:  172.17.0.3 as can be found
  by running kubectl describe node minikube.  This is the IP address of
  VM created running the single Kubernetes node.

- The following will shh into the node and expect its state:

$ minikube ssh

- Once in the node, run the following to install curl command line:
sudo apt-get update
sudo apt-get install curl

- Run the following to see the IP address of the VM:
$ ip addr show

eth0@if55: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.3/16 brd 172.17.255.255 scope global eth0

- The IP address will match what was reported by kubectl describe minikube

- Exit the ssh session.
$ exit

- Next, run the following to get the details of the pod running within the cluster:

$ kubectl describe pod kubia

Namespace:    default
Priority:     0
Node:         minikube/172.17.0.3
Start Time:   Wed, 02 Sep 2020 11:57:59 -0400
Labels:       app=kubia-deploy
              pod-template-hash=5d9f894958
Annotations:  <none>
Status:       Running
IP:           172.18.0.3

The IP address of the pod is 172.18.0.3

- ssh into the node again:
$ minikube ssh 

- Next, execute the following command to against the IP address of
  the pod to call the service (api) running within the pod from a 
  bash session within the node containing the pod:

curl 172.18.0.3:8080

- So the above is accessing the Pod from the bash command line on the node
  that was ssh into.  The above IP address 172.18.0.3 is specific to the node
  and can only be accessed from the node that was just ssh into.

- The following are the details for the service created to expose the container
  running from within the Pod:

NAME           TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)          AGE
kubia-deploy   LoadBalancer   10.98.69.127   10.98.69.127   8080:32214/TCP   12m


- The Cluster-IP can be used to access the Pod from another node within the cluster.
- Minikube is only a one node cluster, but still logged into the node, this IP address
  can also be used to call the service running within the Pods container:

curl 10.98.69.127:8080
  
- So a microservice based solution should be designed to carefully to only expose
  the services that should be allowed to be invoked from outside the cluster while
  other more internal services should only communicate on the internal custer-ip.

** Later a multi node cluster will be created using Google Cloud Services and more
   details will be provided.


minikube service kubia-deploy

|-----------|--------------|-------------|-------------------------|
| NAMESPACE |     NAME     | TARGET PORT |           URL           |
|-----------|--------------|-------------|-------------------------|
| default   | kubia-deploy |        8080 | http://172.17.0.3:32214 |
|-----------|--------------|-------------|-------------------------|

- 10.98.69.127:8080 (LoadBalancer)  -->   http://172.17.0.3:32214 (node) -->  172.18.0.3:8080(pod)

-  This is the ip and port exposed        - This is the ip and port exposed     - This is the ip and port
   by the load balancer.                    to allows the container service       of the internal pod.
                                            for access outside of the cluster

- These layers of indirection are needed since after exposing the pod to be called from outside the
  node, the pod can be moved around or replicated to several different nodes.  So clients should 
  never point to a service on a given node since it might be moved and therefore the ip and port
  would change.  The ip and port of the LoadBalancer is static and load balances requests across
  the nodes running the pod.

$ minikube stop                             
$ minikube delete

- The following outlines what is happening when the above deployment was created 
  using kubeclt from the command line:

1.  The kubectl commands sends request to the Kubernetes HTTP REST Api.
2.  A new Deployment is created within the cluster.
3.  The Deployment creates a Replica Set identifying how the pods should be created to satisfy deployment.
4.  The Replica Set coordinates the creation of a new Pod to run container by sending request to the Scheduler.
5.  The Scheduler determines the node on which the container should be created and sends request to the Kubelet 
    running on the node.
6.  The Worker node uses Docker to pull the image from a repository and creates a Docker Container.
7.  The Docker Container is associated with the Pod in which is executed.
8.  Next, a service is created to expose the api running within the pod's docker container.  


- However, this IP address is internal to the cluster can cannot be accessed from outside.
- To expose the service running within a pod's container, a service must be created.
- There are different types of services:  LoadBalancer and ClusterIP.  A ClusterIP exposes
  the pod's container service(api) within the custer and a LoadBalancer will expose the
  pod's container service(api) to outside the cluster.
- LoadBalancer services creates an external load balancer with an exposed IP address to 
  which external clients use to call a pod's container service(api).


######################################################################################
### Additional Kubernetes Cluster/Controllers/Services/Pods Notes
######################################################################################

- The role of the ReplicationController is to replicate pods and to make sure they
  remain running.  ReplicationController resource are not created very often are 
  a ReplicaSet (a type of ReplicationController) is commonly used.  When the deployment
  above was created, it created an associated ReplicaSet.

- If a pod crashes or the node is stopped, this is detected by the ReplicaSet which
  schedules another pod with the Scheduler to be created.  The Schedule determines 
  the best worker node for the pod.  The Kubelet process running on the node is called
  by the scheduler.  The Kubelet process then pulls the image and then creates the container
  and associated pod. 

- When the prod is relocated to a new node, the newly created pod will have a new IP address.
- The IP address of the LoadBalancer Service is static (for the life of the service).  The 
LoadBalancer Service keeps track of the nodes the exposed pods are running and forwards any
requests to one of the nodes where the pod is running.

** Services represent a static location for a group of one or more pods that all provide the same service.

- The following will show the created ReplicaSet resources:
$ kubectl get replicasets

NAME                      DESIRED   CURRENT   READY   AGE
kubia-deploy-5d9f894958   1         1         1       4h11m

- Next, the ReplicaSet will be sent a request to have the number of running pods
  scaled-up from one to 3.  Since these examples thus far are using minikube, there
  is only one node so the following command can be executed:
  
$ kubectl scale --replicas=3 rs/kubia-deploy-5d9f894958 
  
  In the dashboard, you will see the number of pending nodes increase and two additions 
  pods listed, but since this is a single node cluster, the pods will not be replicated.  
  This is just a limitation of using minikube.  This will be revisited after setting up 
  Kubernetes as a cloud service.


- The following will list all the pods and the nodes to which the are running.  Knowing
  which node a pod is running on is usually no of interest.

 $ kubectl get pods -o wide

NAME                            READY   STATUS    RESTARTS   AGE     IP           NODE       NOMINATED NODE   READINESS GATES
kubia-deploy-5d9f894958-5hhrn   1/1     Running   0          4h29m   172.18.0.3   minikube   <none>           <none>


And the following can be used to get details for a specific Pod:

$ kubectl describe pod kubia-deploy-5d9f894958-5hhrn


######################################################################################
### Labels
######################################################################################

- Labels are used to tag resources (such as pods) so they can be categorized 
  and referenced as a set.

- Labels allows a group of related pods to be operated on as a whole Instead
  of individually.

- They are key/value parts associated with a resource.  The other resources,
  can reference all tagged resources using a label selector.

apiVersion: v1
kind: Pod
metadata:
  name: kubia-pod-labels
  labels:
    app: receipts
    env: prod
spec:
  containers:
  - image: blgreco72dev/kubia
    name: kubia
    ports:
    - containerPort: 8080
      protocol: TCP

- The following can be used to view a pod's labels:

kubectl get pods --show-labels

- Labels can be added and updated on pods:

$ kubectl label pod kubia-pod-labels env=test --overwrite
$ kubectl label pod kubia-pod-labels version=2.0.4 
$ kubectl label pod kubia-pod env=test

- Resources can be filtered based on their assigned labels:

$ kubectl get pods -l env=test --show-labels

- Multiple selectors can be specified within a filter by separating each by a comma.

Node Selection Using Levels
--------------------------------

- This should only be used when needed.  Kubernetes should be in charge of determining
  the node on which a pod should be executed.

- While an exact node cannot be specified for running a pod, the characteristics of the
  node (specified by labels) can be used to select a pod.

- The following is used to add a label to a node:

$ kubectl get nodes

NAME       STATUS   ROLES    AGE   VERSION
minikube   Ready    master   24h   v1.18.3

$ kubectl label node minikube uat=true

- Labels can be used to filter matching nodes:

$ kubectl get nodes -l uat=true
$ kubectl get nodes -l prod=true

- Within a Pod's resource definition file, the characteristics of the node to deploy the
  pod can be specified within the "spec" section with the property named "nodeSelector".

######################################################################################
### Resource Annotations
######################################################################################

- Annotations are key/value pairs similar to labels but are not used to select resources.
- There are are a from of metadata.

- Within a resource definition file, Annotations are specified using the "annotations" property
  defined within the "metadata" section.

- Annotations can also be added after the resource is created:

$ kubectl annotate pod kubia-pod somecompany.com/build="144"

- A pod's annotations can be seen as follows:

$ kubectl describe pod kubia-pod


######################################################################################
### Namespaces
######################################################################################

- Namespaces are used to associate a name with a group of resources.

- The following will add another label to the kubia-prod-labels pod so both pods will have
  a set of labels with the same set of key/value pairs.

$ kubectl label pod kubia-pod app=receipt

- The following will be used to define a namespace to group all "test" environment "receipt" pods 
  from which a given solution is comprised.

- Use the next command to list all defined namespace within a cluster:

$ kubectl get namespaces

- If not otherwise specified, resources are associated with the default namespace.  Also, only
  resources with the default namespace are returned when listing resources if not specified.

- Namespaces can be created using yaml files or from the command line.

apiVersion: v1
kind: Namespace
metadata:
  name: receipt-solution


$ kubectl create -f pod-namespace.yml

$ kubectl create namespace payment-solution


- Namespaces can be specified within a resource definition file by adding a "namespace" property
  to the "metadata" section.  Or the --namespace argument can be specified when calling kubectl
  create when applying a yaml file.

- Name spaces do not provide any type of isolation.  For example, pods assigned to different 
  namespaces can still communicate within the cluster.

- Labels and Namespaces can be used to apply an operation to multiple resources at the same time.
- The following will delete all nodes defining a label with a specific value:

$ kubectl delete pods -l app=receipt

- Also, all Pods contained within a namespace can be deleted by deleting the namespace.  This will
  also delete the namespace.

$ kubectl delete namespace receipt-solution

- If you want to keep the namespace and just want to delete all Pods within the namespace, 
  the following command can be used:

$ kubectl delete pods --all

- The above command will delete all resources contained within the current namespace.

The current namespace to which all commands will apply can be set as follows:

$ kubectl config set-context --current --namespace=receipt-solution

- The following command can be used to view the current context:

$ kubectl config current-context


######################################################################################
### DaemonSet
######################################################################################

- Allows running a specific Pod on all Nodes belonging to the cluster.

- If a new Node is added to the Cluster, the DaemonSet will automatically create a new Pod 
  instance on that node.

- Also, a DaemonSet can be defined to run a Pod on a set of Nodes meeting a specific criteria.

- The following is a definition for a DaemonSet:

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: daemon-test
spec:
  selector:
    matchLabels:
      app: daemon-test
  template:
    metadata:
      labels:
        app: daemon-test
    spec:
      nodeSelector:
        disk: ssd
      containers:
      - name: main
        image: blgreco72dev/kubia

$ kubectl create -f daemon-set.yaml

- At this point, zero Pods will have been created.  Currently there are no nodes
  labeled with drive: ssd

$ kubectl get pods

No resources found in default namespace.

- Next label some nodes with the needed label:

$ kubectl label node minikube disk=ssd

$ kubectl get pods
NAME                READY   STATUS    RESTARTS   AGE
daemon-test-jlszm   1/1     Running   0          31s


######################################################################################
### Jobs
######################################################################################
- The other controller types create a number of Pods and makes sure they
  are always running to match the number of specified replica.

- Jobs create a Pod for executing a container process.  After the process completes,
  the Pod is removed.

- If a node goes down when a Job is running, a Pod will be created on another node to continue
  until the job successfully completes.

- A Pod for a Job does not run indefinitely so the restartPolicy must be specified as OnFailure or Never.

apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
      - name: main
        image: luksa/batch-job


- After the job completes, the Pod will remain.  This allows you to check the logs if needed.


$ kubectl get pods

NAME                READY   STATUS    RESTARTS   AGE
batch-job-tkcl6     1/1     Running   0          2m55s

NAME                READY   STATUS      RESTARTS   AGE
batch-job-tkcl6     0/1     Completed   0          2m35s


----------------------------------------
CronJob
----------------------------------------
- These are similar to Jobs but they can be scheduled to run on an interval.



######################################################################################
### Signaling when a pod is read to accept Connections
######################################################################################
- As soon as a pod is crated, Kubernetes assumes it is ready to receive connections.
- However, this might not alway be the case.
- A pod can signal when it is ready.
- This is mostly needed during container service-api startup.

  



