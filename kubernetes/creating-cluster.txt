=============================================================================
== Objective
=============================================================================

- Create a Kubernetes Cluster consisting of 4 nodes running on 
  Ubuntu Server 20.04.1 LTS hosted as Virtual Box VMs.
  
- There will be one Master Node and 3 Worker nodes.
- Calico will be used as the Pods Network implementation.
- Kubernetes Desktop will be configured. 
- kubeadm will be used to build each of the nodes.

=============================================================================
== Generate SSH key
=============================================================================

- Before creating the Ubuntu servers, a SSH Key will be generated
  that will be copied to the created servers and used to SSH into
  the servers from main Ubuntu Desktop OS shell.

$ mkdir â€“p $HOME/.ssh
$ chmod 0700 $HOME/.ssh
$ cd .ssh

$ ssh-keygen
	name: kubenodes
	phrase: (left blank)


-- Resources:
https://phoenixnap.com/kb/how-to-enable-ssh-on-ubuntu
https://phoenixnap.com/kb/generate-setup-ssh-key-ubuntu



=============================================================================
== Virtual Box Common Settings for VMs
=============================================================================

- The following are the resources that will be allocated to each of the 
  server VMs:

2048 RAM
10 Gig Drive
2 CPUs

- It was determined the the following Host Network must be created to allow 
  all the VMs to communicate with each other over a network and to also allow
  access via SSH from the Ubuntu shell:

File|Host Network Manager
	vboxnet1	
	Enable Server
	Enable DHCP

-- After the above host network is created, it must be referenced by each
   of the crated VMs.  
VM Setting:
	Networking:
		Adapter1: NAS
		Adapter2: Host-only Adapter
			  vboxnet1


=============================================================================
== Creating Virtual Box VMs
=============================================================================

- The following will create and configure each of the Virtual Box VMs based
  on the above specified resources to be allocated and configuration of the
  needed network adapters.

- The following will be the naming convention for each VM:
	- M-Node1
	- W-Node1
	- W-Node2
	- W-Node3

- Create Virtual Box VMs for each of the above nodes by selecting Linux and Ubuntu (64-bit)
  for each of the VMs types.  Also specify 2048 for RAM allocation.  Select the default for
  all remaining settings.

- After all the above VMs are created, configure each with the following settings:
  - 2048 RAM
	- 2 CPUs  (System|Processor)
	- Add Adapter2 Host-only Adapter (Network|Adapter2) and select vboxnet1



=============================================================================
== Installing Ubuntu Server on all VMs
=============================================================================

- Next, Ubuntu Server 20.04.1 LTS needs to be installed on each VM. 

- A single VM could be created an cloned, but for this setup each
  VM will be independently configured.  If a single VM is cloned,
  then the kubeadm and other needed Kubernetes daemons should first
  be installed before cloning.  

- Complete the following steps on each crated Virtual Box VM:
	- Install Ubuntu Server 20.04.1 LTS.  The ISO can be downloaded from the Web.
	- Enable SSH Host 
	- Restart the server
	- From the shell on the host OS, copy the public SSH key to the server.  This
	  allows logging in without have to provide a password each time.

- After the above is completed, all the needed VMs running Ubuntu Server will be
  ready to have a Kubernetes Cluster created using kubeadm.


- Start the VM.  When prompted, select 	the downloaded ISO.

- Select the default settings until the Profile setup screen is shown.
	- Your Name: 				      Kubernetes
	- Your server's name:		  m-node1
	- Pick a username:			  m-node1
	- Choose a password:		  m-node1
	- Confirm your password:	m-node1

- On the SSH Setup step, select Install OpenSSH Server.

- Do not make any selections on the Featured Server Snaps.  Note that Docker is listed but
  but it is best installed after the server is created since its configuration must be changed.

- After the OS is installed, it will be rebooted.  Note that there will be an error saying that
  Failed unmounting /cdrom.  Just enter return and the server will start.

- Once the server is started, you will be presented with a login.  Enter the username and password
  above.  Note if you don't see a login and the boot logs has been written, just press return.

- After logging into the server, note the IP address for the server below and repeat the same
  steps for each of the remaining 3 nodes using the VM name for the server name, username,
  and password.


Node		IP Address
m-node1		192.168.56.4
w-node1		192.168.56.5
w-node2		192.168.56.6
w-node3		192.168.56.7

Next, execute the following commands (from the Host's shell) to copy the public SSH key generated
previously, to each of the servers.  When prompted with question about continuing, type "yes".

$ ssh-copy-id m-node1@192.168.56.4
$ ssh-copy-id w-node1@192.168.56.5
$ ssh-copy-id w-node2@192.168.56.6
$ ssh-copy-id w-node3@192.168.56.7


=============================================================================
== Installing Kubernetes 
=============================================================================

- The following outlines each step to property configure kubernetes on each
  server.  The steps completed are as follows:

  - Install Docker
  - Configure Docker to use systemd for cgroupdriver 
  - Remove Swap
  - Install: kubelet, kubeadm, and kubectl
  - Initialize Kubernetes Cluster on Master Node
  - Install Calico Pod Networking
  - Join each worker node to the cluster

- The above steps can be executed by using SSH to log into each node from
  the host operating system:

  $ ssh m-node1@192.168.56.4
	$ ssh w-node1@192.168.56.5
	$ ssh w-node2@192.168.56.6
	$ ssh w-node3@192.168.56.7

- Run as the root user:
 sudo -i 

-----------------------------------------------------------------------------
-- Install Docker (All Nodes)
-----------------------------------------------------------------------------

https://kubernetes.io/docs/setup/production-environment/container-runtimes/


apt-get update && apt-get install -y \
  apt-transport-https ca-certificates curl software-properties-common gnupg2


curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -

add-apt-repository \
  "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) \
  stable"


apt-get update && apt-get install -y \
  containerd.io=1.2.13-2 \
  docker-ce=5:19.03.11~3-0~ubuntu-$(lsb_release -cs) \
  docker-ce-cli=5:19.03.11~3-0~ubuntu-$(lsb_release -cs)

cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

systemctl daemon-reload
systemctl restart docker

sudo systemctl enable docker
sudo usermod -aG docker $USER

-----------------------------------------------------------------------------
-- Removing Swap (All Nodes)
-----------------------------------------------------------------------------

sudo swapoff -a

# Comment out the swap entry.

sudo nano /etc/fstab

# Save Changes

-----------------------------------------------------------------------------
-- Install Kubernetes Components (All Nodes)
-----------------------------------------------------------------------------

https://medium.com/@dcarrascal75/setting-a-kubernetes-cluster-on-ubuntu-virtualbox-the-lost-guide-73706e28bc5b


sudo sh -c "echo 'deb http://apt.kubernetes.io/ kubernetes-xenial main' >> /etc/apt/sources.list.d/kubernetes.list"
sudo sh -c "curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -"

sudo apt-get update

# Install components
sudo apt-get install -y kubelet kubeadm kubectl

# Hold version
sudo apt-mark hold kubelet kubeadm kubectl

sudo nano /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

https://medium.com/@kanrangsan/how-to-specify-internal-ip-for-kubernetes-worker-node-24790b2884fd

Add  --node-ip=192.168.56.4 to KUBELET_KUBECONFIG_ARGS
Add  --node-ip=192.168.56.5 to KUBELET_KUBECONFIG_ARGS
Add  --node-ip=192.168.56.6 to KUBELET_KUBECONFIG_ARGS
Add  --node-ip=192.168.56.7 to KUBELET_KUBECONFIG_ARGS

# Save changes

sudo systemctl daemon-reload
sudo systemctl restart docker


- Before continuing, restart each of the nodes by running the following:

shutdown --reboot now

form each of the (VMs shells).


=============================================================================
== Initialize Kubernetes Cluster / Calico Pod Networking (Master Node)
=============================================================================

- The following command will initialize the Kubernetes cluster on the master node.

- Make sure the value of the --apiserver-advertise-address argument is the IP address
  of the master node.

$ sudo kubeadm init --apiserver-advertise-address=192.168.57.27 --pod-network-cidr=10.244.0.0/16

- When the above command finishes, copy the output and place below.  This will be needed When
  jointing the worker nodes to the cluster:

-------------------------------------------------------------------------------------------------------------

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

sudo kubeadm join 192.168.56.4:6443 --token jm0qit.2912dzb1ld7f3zi9 \
    --discovery-token-ca-cert-hash sha256:c9d543214c2b614a88a39754015437c6c5070562bb915e2ef69b15ac3479f54c 


------------------------------------------------------------------------------------------------------------

- Next, run the following commands so that the kubectl will point to the REST service
  hosted by the master node.  The master node is where all the Kubernetes commands are
  executed.  However, it is much easier to also point kubectl running on the host computer
  so all kubernetes kubectl commands can be easily executed without having to log into one
  of the master nodes.  This will be done in an upcoming step.


- Run the following on the master node:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


- Next, run the following to view the current state of the cluster.

kubectl cluster-info
kubectl get nodes -o wide

- The following command will shows all pods running on the master node.
  The output of the command is listed.

kubectl get pods -n kube-system


NAME                              READY   STATUS    RESTARTS   AGE
coredns-f9fd979d6-9x8tf           0/1     Pending   0          7m7s
coredns-f9fd979d6-px2tm           0/1     Pending   0          7m7s
etcd-m-node1                      1/1     Running   0          7m16s
kube-apiserver-m-node1            1/1     Running   0          7m16s
kube-controller-manager-m-node1   1/1     Running   0          7m16s
kube-proxy-n9mb9                  1/1     Running   0          7m7s
kube-scheduler-m-node1            1/1     Running   0          7m16s


- The above output shoes that the two coredns pods have not been created and have pending states.

- Before these nodes will start, a Pod network implementation must be installed.

---------------------------------------------------------------------------------------------

Calico

https://docs.projectcalico.org/getting-started/kubernetes/quickstart

- Next, run the following two commands on the master node to install the pods providing an 
  implementation of a Pod network:
  
kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml
kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yaml

- After the above two commands complete, run the following to monitor the pods within
  the calico-system namespace.  Once all pods are started, issue a control-c to stop.

watch kubectl get pods -n calico-system

NAME                                      READY   STATUS    RESTARTS   AGE
calico-kube-controllers-6ddfb4769-b9ntw   1/1     Running   0          48s
calico-node-wc2q7                         1/1     Running   0          48s
calico-typha-548b85d659-schsj             1/1     Running   0          48s

---------------------------------------------------------------------------------------------

Flannel

https://github.com/coreos/flannel#flannel

- Next, run the following two commands on the master node to install the pods providing an 
  implementation of a Pod network:

kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

- Running the following command will show Flannel pod:

kubectl get pods -n kube-system

----------------------------------------------------------------------------------------------

- Next, check the status of all the Pods within the kube-system to make sure that all
  are running and ready:


NAME                              READY   STATUS    RESTARTS   AGE
coredns-f9fd979d6-9x8tf           1/1     Running   0          15m
coredns-f9fd979d6-px2tm           1/1     Running   0          15m
etcd-m-node1                      1/1     Running   0          15m
kube-apiserver-m-node1            1/1     Running   0          15m
kube-controller-manager-m-node1   1/1     Running   0          15m
kube-proxy-n9mb9                  1/1     Running   0          15m
kube-scheduler-m-node1            1/1     Running   0          15m


- The overall status of the cluster can be viewed by running the following command:

kubectl cluster-info

- The following is the results:

Kubernetes master is running at https://192.168.57.27:6443
KubeDNS is running at https://192.168.57.27:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.


- Next, the following command will list all nodes that are joined to the cluster and their status.  
  Since worker nodes have not yet been joined, only the master node will be listed.


kubectl get nodes 

NAME      STATUS   ROLES    AGE   VERSION
m-node1   Ready    master   19m   v1.19.2


=============================================================================
== Join Worker nodes to Kubernetes Cluster (All Worker Nodes)
=============================================================================

- The next step is to join all worker nodes to the cluster.  When initializing
  the cluster on the master node, the output contained the command to be executed
  to join a worker node to the cluster.

- SSH into each of the 3 worker nodes and run the following.  On the host computer,
  keep the SSH session open to the master node so the status of each worker node can
  be viewed.

- Run the following command on the master node SSH session:  

watch kubectl get nodes

- Now SSH into the each worker node and run the following command.  Move onto the next
  worker node once the watch command shows the node is joined and ready.


$ ssh w-node1@192.168.56.5
$ ssh w-node2@192.168.56.6
$ ssh w-node3@192.168.56.7


sudo kubeadm join 192.168.56.4:6443 --token jm0qit.2912dzb1ld7f3zi9 \
    --discovery-token-ca-cert-hash sha256:c9d543214c2b614a88a39754015437c6c5070562bb915e2ef69b15ac3479f54c 


- Once the output of the get-nodes commands shows the following, the cluster has been 
  correctly initialized and configured:

kubectl get nodes -o wide

NAME      STATUS   ROLES    AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
m-node1   Ready    master   32m     v1.19.2   192.168.56.4   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.13
w-node1   Ready    <none>   20m     v1.19.2   192.168.56.5   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.13
w-node2   Ready    <none>   4m22s   v1.19.2   192.168.56.6   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.13
w-node3   Ready    <none>   2m43s   v1.19.2   192.168.56.7   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   docker://19.3.13

=============================================================================
== POINTING HOST TO CLUSTER
=============================================================================
- Run the following command so that kubectl will point to the cluster When executed:

- SSH into the Master node and execute the following:

sudo cp /etc/kubernetes/admin.conf ~/
sudo chmod 777 admin.conf

- Execute the following command on Host.  This will direct the kubectl so all commands are
  sent to the master node that was just configured and to a local instance or another cluster.

scp m-node1@192.168.56.4:/home/m-node1/admin.conf ~/.kube/config
export KUBECONFIG=~/.kube/config



=============================================================================
== Installing Kubernetes Dashboard (Master Node)
=============================================================================

https://www.replex.io/blog/how-to-install-access-and-add-heapster-metrics-to-the-kubernetes-dashboard

- SSH into the master node and install the dashboard:
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml


- In another terminal window on the host computer, run the following.  This will start 
  a proxy the will delate to the master node as specified by the KUBECONFIG environment
  variable.

kubectl proxy 

- Open Web Browser and enter the following URL:
http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/.


- Next a authentication token needs to be configured.  SSH into the master node and execute the following:

kubectl create serviceaccount dashboard-admin-sa
kubectl create clusterrolebinding dashboard-admin-sa --clusterrole=cluster-admin --serviceaccount=default:dashboard-admin-sa

- Execute the following command to get the secrets:

kubectl get secrets

- Now obtain the access token associated with the secret:

kubectl describe secret dashboard-admin-sa-token-2hdj7


eyJhbGciOiJSUzI1NiIsImtpZCI6IjNyWkp6TjZ5cHJIbmZycVF1U3g3ckFHZmFVRnU5SFdBTjQ2aUVQZkZiSVUifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRhc2hib2FyZC1hZG1pbi1zYS10b2tlbi0yaGRqNyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tc2EiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwYzljMjM3Yi0zZWE1LTRiNzEtYWMxNi1lMDliMWNmNTA3ZDQiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVmYXVsdDpkYXNoYm9hcmQtYWRtaW4tc2EifQ.ycQ_pfvAA51cUt0JRqqRHSwK8giWIqn6jwBfAdnqRjKqzvgaFU1yAKmQA_UdAac2l9lbPzUb5AR4-OKjxKn1p3W3Q9NDLNYigiZ-yNH8O0eT8k5-c0s36qN56S0YvNA1xNx4bW7NaRT-9B9DGWb2uIbM_9dTlHtiyxPFyfMBHa4hs2t-Ow0OP5fcjlhUfNjIRcVkcwmeRI1HcjhtDmAWjiX21buNplImGI-VldAPxhWwubsOjLczptLYkSXbZh15O-wZWAWUtOxOxk11iYgDDmof3GOsjh094mpq1TXK3NZ45Awt7Jp1D25em4DhkeT_fdfo7HEMyv36SxAZlP3z_A


- Copy the above token into the input field of the dashboard.


=============================================================================
== Other resource
=============================================================================


https://unix.stackexchange.com/questions/36540/why-am-i-still-getting-a-password-prompt-with-ssh-with-public-key-authentication